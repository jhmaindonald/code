[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Code for ‘A Practical Guide . . .’",
    "section": "",
    "text": "Preface\nCode provided here is for the May 2024 text:\n\n“A Practical Guide to Data Analysis Using R – An Example-Based Approach”, by John H Maindonald, W John Braun, and Jeffrey L Andrews.\n\nCode that is shown in the text is filled out to include all code for graphs. In chapter 2 and later, the text includes code only for those graphs that are specifically targeted at the methodology under discussion.\nThe text has been available in digital format since early May 2024. It has been available in print in the UK from May 30, is due to be available in print in the USA in July, and in Australia and New Zealand in August 2024.\nFor details for the UK, see: A Practical Guide to Data Analysis Using R – Cambridge University Press UK",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "ch1.html",
    "href": "ch1.html",
    "title": "1  Chapter 1: Learning from data",
    "section": "",
    "text": "library(knitr)\nopts_chunk[['set']](fig.width=6, fig.height=6, comment=\" \",\n                    out.width=\"80%\", fig.align=\"center\", fig.show='hold',\n                    size=\"small\", ps=10, strip.white = TRUE, \n                    tidy.opts = list(replace.assign=FALSE))\n\n\n## xtras=TRUE    ## Set to TRUE to execute code 'extras'\nxtras &lt;- FALSE\nlibrary(knitr)\n## opts_chunk[['set']](results=\"asis\")\n## opts_chunk[['set']](eval=FALSE)   ## Set to TRUE to execute main part of code\nopts_chunk[['set']](eval=FALSE) \n\n\nPackages required (plus any dependencies)\nlatticeExtra (lattice is a dependency); DAAG; car; MASS; AICcmodavg; BayesFactor; boot; MPV; ggplot2; tidyr\nAdditionally, knitr and Hmisc are required in order to process the Rmd source file. The prettydoc package is by default used to format the html output.\n\n\nChapter summary\n\n\nA note on terminology — variables, factors, and more!\n\n\nSection 1.1: Questions, and data that may point to answers\n\nSubsection 1.1.1: A sample is a window into the wider population\n\n## For the sequence below, precede with set.seed(3676)\nset.seed(3696)\nsample(1:9384, 12, replace=FALSE)  # NB: `replace=FALSE` is the default\n\n\nchosen1200 &lt;- sample(1:19384, 1200, replace=FALSE)  \n\n\n## For the sequence below, precede with set.seed(366)\nset.seed(366)\nsplit(sample(seq(1:10)), rep(c(\"Control\",\"Treatment\"), 5))\n# sample(1:10) gives a random re-arrangement (permutation) of 1, 2, ..., 10\n\n\nCluster sampling\n\n\n*A note on with-replacement samples\n\nsample(1:10, replace=TRUE)\n## sample(1:10, replace=FALSE) returns a random permutation of 1,2,...10\n\n\n\n\nSubsection 1.1.2: Formulating the scientific question\n\nExample: a question about cuckoo eggs\n\nlibrary(latticeExtra)   # Lattice package will be loaded and attached also\ncuckoos &lt;- DAAG::cuckoos\n## Panel A: Dotplot without species means added\ndotplot(species ~ length, data=cuckoos)   ## `species ~ length` is a 'formula'\n## Panel B: Box and whisker plot\nbwplot(species ~ length, data=cuckoos)\n## The following shows Panel A, including species means & other tweaks\nav &lt;- with(cuckoos, aggregate(length, list(species=species), FUN=mean))\ndotplot(species ~ length, data=cuckoos, alpha=0.4, xlab=\"Length of egg (mm)\") +\n  as.layer(dotplot(species ~ x, pch=3, cex=1.4, col=\"black\", data=av))\n  # Use `+` to indicate that more (another 'layer') is to be added.\n  # With `alpha=0.4`, 40% is the point color with 60% background color\n  # `pch=3`: Plot character 3 is '+'; `cex=1.4`: Default char size X 1.4\n\n\n## Code\nsuppressPackageStartupMessages(library(latticeExtra, quietly=TRUE))\ncuckoos &lt;- DAAG::cuckoos\n## For tidier labels replace \".\", in several of the names, by a space\nspecnam &lt;- with(cuckoos, sub(pattern=\".\", replacement=\" \", \n                             levels(species), fixed=TRUE))\n# fixed=TRUE: \"interpret \".\" as \".\", not as a 'any single character'\"\ncuckoos &lt;- within(cuckoos, levels(species) &lt;- specnam)\n## Panel A: Dotplot: data frame cuckoos (DAAG)\nav &lt;- with(cuckoos, aggregate(length, list(species=species), FUN=mean))\ngphA &lt;- dotplot(species ~ length, data=cuckoos, alpha=0.4) +\n  as.layer(dotplot(species ~ x, pch=3, cex=1.4, col=\"black\", data=av))\n# alpha sets opacity. With alpha=0.4, 60% of the background shows through\n# Enter `print(plt1)` or `plot(plt1)` or simply `plt1` to display the graph\n## Panel B: Box plot\ngphB &lt;- bwplot(species ~ length, data=cuckoos)\nupdate(c(\"A: Dotplot\"=gphA, \"B: Boxplot\"=gphB), between=list(x=0.4),\n       xlab=\"Length of egg (mm)\") \n## latticeExtra::c() joins compatible plots together. \n##   See `?latticeExtra::c`\n\n\n\n\nSubsection 1.1.3: Planning for a statistical analysis\n\nUnderstand the data\n\n\nCausal inference\n\n\nWhat was measured? Is it the relevant measure?\n\n\nUse relevant prior information in the planning stages\n\n\nSubject area knowledge and judgments\n\n\nThe importance of clear communication\n\n\nData-based selection of comparisons\n\n\nModels must be fit for their intended use\n\n\n\nSubsection 1.1.4: Results that withstand thorough and informed challenge\n\n\nSubsection 1.1.5: Using graphs to make sense of data\n\nGraphical comparisons\n\n\n\nSubsection 1.1.6: Formal model-based comparison\n\noptions(width=70)\ncuckoos &lt;- DAAG::cuckoos\nav &lt;- with(cuckoos, aggregate(length, list(species=species), FUN=mean))\nsetNames(round(av[[\"x\"]],2), abbreviate(av[[\"species\"]],10))\n\n\nwith(cuckoos, scale(length[species==\"wren\"], scale=FALSE))[,1] \n\n\n\n\nSection 1.2: Graphical tools for data exploration\n\nSubsection 1.2.1: Displays of a single variable\n\nlibrary(latticeExtra, quietly=TRUE)\nfossum &lt;- subset(DAAG::possum, sex==\"f\")\nfemlen &lt;- DAAG::bounce(fossum[[\"totlngth\"]], d=0.1)\n## Panel A\nyaxpos &lt;- c(0,5,10,15,20)/(5*nrow(fossum))\nz &lt;- boxplot(list(val = femlen), plot = FALSE)\ngph1 &lt;- bwplot(~femlen, ylim=c(0.55,2.75), xlim=c(70,100), \n               scales=list(y=list(draw=FALSE)))+\n        latticeExtra::layer(panel.rug(x,pch=\"|\"))\nlegstat &lt;- data.frame(x=c(z$out,z$stats), y=c(1.08, rep(1.3,5)),\n  tx=c(\"Outlier?\", \"Smallest value\", \"lower quartile\", \"median\", \n       \"upper quartile\",  \"Largest value\"), \n  tx2= c(\"\", \"(outliers excepted)\",rep(\"\",3), \"(no outliers)\"))\ngphA &lt;- gph1+latticeExtra::layer(data=legstat,\n  panel.text(x=x,y=y,labels=tx,adj=c(0,0.4),srt=90, cex=0.85),\n  panel.text(x=x[c(2,6)]+0.75,y=c(1.125,1.38),labels=tx2[c(2,6)],\n             adj=c(0,0.4),srt=90, cex=0.85))\n## Panel B\ngph2 &lt;- densityplot(~femlen, ylim=c(0,0.108), xlim=c(70,100), \n          plot.points=TRUE, pch=\"|\",cex=1.75, ylab=c(\"\",\"      Density\"))\ngph3 &lt;- histogram(~femlen, ylim=c(0,0.108), type=\"density\", \n  scales=list(y=list(at=yaxpos, labels=c(0,5,10,15,20), col=\"gray40\")), \n  alpha=0.5, ylab=\"\", breaks=c(75,80,85,90,95,100), \n  col='transparent',border='gray40')\ngph4 &lt;- doubleYScale(gph2, gph3, use.style=FALSE, add.ylab2=FALSE)\ngphB &lt;- update(gph4, par.settings=list(fontsize = list(text=10, points=5)),\n  scales=list(tck=c(0.5,0.5)))\nupdate(c(\"B: Density curve, with histogram overlaid\"=gphB, \n         \"A: Boxplot, with annotation added\"=gphA, layout=c(1,2), y.same=F), \n       as.table=TRUE, between=list(y=1.4), \n       xlab=\"Total length of female possums (cm)\")\n\n\nfossum &lt;- subset(DAAG::possum, sex==\"f\")\ndensityplot(~totlngth, plot.points=TRUE, pch=\"|\", data=fossum) +\n  layer_(panel.histogram(x, type=\"density\", breaks=c(75,80,85,90,95,100)))\n\n\nComparing univariate displays across factor levels\n\nlibrary(latticeExtra, quietly=TRUE)\nfossum &lt;- subset(DAAG::possum, sex==\"f\")\nfemlen &lt;- DAAG::bounce(fossum[[\"totlngth\"]], d=0.1)\n## Panel A\nyaxpos &lt;- c(0,5,10,15,20)/(5*nrow(fossum))\nz &lt;- boxplot(list(val = femlen), plot = FALSE)\ngph1 &lt;- bwplot(~femlen, ylim=c(0.55,2.75), xlim=c(70,100), \n               scales=list(y=list(draw=FALSE)))+\n        latticeExtra::layer(panel.rug(x,pch=\"|\"))\nlegstat &lt;- data.frame(x=c(z$out,z$stats), y=c(1.08, rep(1.3,5)),\n  tx=c(\"Outlier?\", \"Smallest value\", \"lower quartile\", \"median\", \n       \"upper quartile\",  \"Largest value\"), \n  tx2= c(\"\", \"(outliers excepted)\",rep(\"\",3), \"(no outliers)\"))\ngphA &lt;- gph1+latticeExtra::layer(data=legstat,\n  panel.text(x=x,y=y,labels=tx,adj=c(0,0.4),srt=90, cex=0.85),\n  panel.text(x=x[c(2,6)]+0.75,y=c(1.125,1.38),labels=tx2[c(2,6)],\n             adj=c(0,0.4),srt=90, cex=0.85))\n## Panel B\ngph2 &lt;- densityplot(~femlen, ylim=c(0,0.108), xlim=c(70,100), \n          plot.points=TRUE, pch=\"|\",cex=1.75, ylab=c(\"\",\"      Density\"))\ngph3 &lt;- histogram(~femlen, ylim=c(0,0.108), type=\"density\", \n  scales=list(y=list(at=yaxpos, labels=c(0,5,10,15,20), col=\"gray40\")), \n  alpha=0.5, ylab=\"\", breaks=c(75,80,85,90,95,100), \n  col='transparent',border='gray40')\ngph4 &lt;- doubleYScale(gph2, gph3, use.style=FALSE, add.ylab2=FALSE)\ngphB &lt;- update(gph4, par.settings=list(fontsize = list(text=10, points=5)),\n  scales=list(tck=c(0.5,0.5)))\nupdate(c(\"B: Density curve, with histogram overlaid\"=gphB, \n         \"A: Boxplot, with annotation added\"=gphA, layout=c(1,2), y.same=F), \n       as.table=TRUE, between=list(y=1.4), \n       xlab=\"Total length of female possums (cm)\")\n\n\n## Create boxplot graph object --- Simplified code\ngph &lt;- bwplot(Pop~totlngth | sex, data=possum) \n## plot graph, with dotplot distribution of points below boxplots\ngph + latticeExtra::layer(panel.dotplot(x, unclass(y)-0.4)) \n\n\n\n\nSubsection 1.2.2: Patterns in univariate time series\n\nlayout(matrix(c(1,2)), heights=c(2.6,1.75))\nmeasles &lt;- DAAG::measles\n## Panel A:\npar(mgp=c(2.0,0.5,0))\nplot(log10(measles), xlab=\"\", ylim=log10 (c(1,5000*540)),\n     ylab=\" Deaths\", yaxt=\"n\", fg=\"gray\", adj=0.16)\nlondonpop &lt;-ts(c(1088, 1258, 1504, 1778, 2073, 2491, 2921, 3336, 3881,\n  4266, 4563, 4541, 4498, 4408), start=1801, end=1931, deltat=10)\npoints(log10(londonpop*500), pch=16, cex=.5)\nytiks1 &lt;- c(1, 10, 100, 1000)\naxis(2, at=log10(ytiks1), labels=paste(ytiks1), lwd=0, lwd.ticks=1)\nabline(h=log10(ytiks1), col = \"lightgray\", lwd=2)\npar(mgp=c(-2,-0.5,0))\nytiks2 &lt;- c(1000000, 5000000)  ## London population in thousands\nabline(h=log10(ytiks2*0.5), col = \"lightgray\", lwd=1.5)\nabline(v=seq(from=1650,to=1950,by=50), col = \"lightgray\", lwd = 1.5)\nmtext(side=2, line=0.5, \"Population\", adj=1, cex=1.15, las=3)\naxis(2, at=log10(ytiks2*0.6), labels=paste(ytiks2), tcl=0.3,\n     hadj=0, lwd=0, lwd.ticks=1)\nmtext(side=3, line=0.3, \"A (1629-1939)\", adj=0, cex=1.15)\n##\n## Panel B: window from 1840 to 1882\npar(mgp=c(2.0,0.5,0))\nplot(window(measles, start=1840, end=1882), xlab=\"\",\nylab=\"Deaths    Pop (1000s)\", ylim=c(0, 4200), fg=\"gray\")\npoints(window(londonpop, start=1840, end=1882), pch=16, cex=0.5)\nmtext(side=3, line=0.5, \"B (1841-1881)\", adj=0, cex=1.15)\n\n\n\nSubsection 1.2.3: Visualizing relationships between pairs of variables\n\n\nSubsection 1.2.4: Response lines (and/or curves)\n\npar(pty=\"s\")\nplot(distance.traveled ~ starting.point, data=DAAG::modelcars, fg=\"gray\",\nxlim=c(0,12.5), xaxs=\"i\", xlab = \"Distance up ramp (cm)\",\nylab=\"Distance traveled (cm)\")\n\n\n\nSubsection 1.2.5: Multiple variables and times\n\n## Apply function range to columns of data frame jobs (DAAG)\nsapply(DAAG::jobs, range)  ## NB: `BC` = British Columbia\n\n\n## Panel A: Basic plot; all series in a single panel; use log y-scale\nformRegions &lt;- Ontario+Quebec+BC+Alberta+Prairies+Atlantic ~ Date\nbasicGphA &lt;-\n  xyplot(formRegions, outer=FALSE, data=DAAG::jobs, type=\"l\", xlab=\"\", \n         ylab=\"Number of workers\", scales=list(y=list(log=\"e\")),\n         auto.key=list(space=\"right\", lines=TRUE, points=FALSE))\n  ## `outer=FALSE`: plot all columns in one panel\n## Panel B: Separate panels (`outer=TRUE`); sliced log scale\nbasicGphB &lt;-\n  xyplot(formRegions, data=DAAG::jobs, outer=TRUE, type=\"l\", layout=c(3,2), \n         xlab=\"\", ylab=\"Number of workers\",\n         scales=list(y=list(relation=\"sliced\", log=TRUE)))\n# Provinces are in order of number of workers in Dec96\n## Create improved x- and y-axis tick labels; will update to use\ndatelabpos &lt;- seq(from=95, by=0.5, length=5)\ndatelabs &lt;- format(seq(from=as.Date(\"1Jan1995\", format=\"%d%b%Y\"),\n                   by=\"6 month\", length=5), \"%b%y\")\n## Now create $y$-labels that have numbers, with log values underneath\nylabposA &lt;- exp(pretty(log(unlist(DAAG::jobs[,-7])), 5))\nylabelsA &lt;- paste(round(ylabposA),\"\\n(\", log(ylabposA), \")\", sep=\"\")\n## Repeat, now with 100 ticks, to cover all 6 slices of the scale\nylabposB &lt;- exp(pretty(log(unlist(DAAG::jobs[,-7])), 100))\nylabelsB &lt;- paste(round(ylabposB),\"\\n(\", log(ylabposB), \")\", sep=\"\")\ngphA &lt;- update(basicGphA, scales=list(x=list(at=datelabpos, labels=datelabs),\n              y=list(at=ylabposA, labels=ylabelsA)))\ngphB &lt;- update(basicGphB, xlab=\"\", between=list(x=0.25, y=0.25),\n               scales=list(x=list(at=datelabpos, labels=datelabs),\n               y=list(at=ylabposB, labels=ylabelsB)))\nlayout.list &lt;- list(layout.heights=list(top.padding=0,\n                    bottom.padding=0, sub=0, xlab=0), \n                    fontsize=list(text=8, points=5))\njobstheme &lt;- modifyList(ggplot2like(pch=1, lty=c(4:6,1:3),\n                                    col.line='black', cex=0.75),layout.list)\nprint(update(gphA, par.settings=jobstheme, axis=axis.grid,\n      main=list(\"A: Same vertical log scale\",y=0)),\n      position=c(0.1,0.615,0.9,1), newpage=TRUE)\nprint(update(gphB, par.settings=jobstheme, axis=axis.grid,\n      main=list(\"B: Sliced vertical log scale\",y=0)),\n      position=c(0,0,1,0.625), newpage=FALSE)\n\n\nplot(c(1230,1860), c(0, 10.5), axes=FALSE, bty=\"n\",\n     xlab=\"\", ylab=\"\", type=\"n\", log=\"x\")\nxpoints &lt;- c(1366, 1436, 1752, 1840)\naxis(1, at=xpoints, labels=FALSE, tck=0.01, lty=1, lwd=0, lwd.ticks=1)\nfor(i in 1:4){\n  axis(1, at=xpoints[i],\n       labels=substitute(italic(a), list(a=paste(xpoints[i]))),\n  line=-2.25, lty=0, cex=0.8, lwd=0, lwd.ticks=1)\n  lines(rep(xpoints[i],2), c(0, 0.15*par()[[\"cxy\"]][2]), lty=1)\n}\naxpos &lt;- 1250*cumprod(c(1, rep(1.2,2)))\naxis(1, at=c(axpos,1840), labels=F, lwd.ticks=0)\nlab &lt;- round(axpos)\naxis(1, at=axpos, labels=lab)\nlab2 &lt;- lapply(round(log2(xpoints),3), function(x)substitute(2^a, list(a=x)))\naxis(1, at=xpoints, labels=as.expression(lab2), line=-3.5, lwd=0)\nlabe &lt;- lapply(format(round(log(xpoints),3)), function(x)substitute(e^a, list(a=x)))\naxis(1, at=xpoints, labels=as.expression(labe), line=-5, lwd=0)\nlab10 &lt;- lapply(round(log10(xpoints),3), function(x)substitute(10^a, list(a=x)))\naxis(1, at=xpoints, labels=as.expression(lab10), line=-6.5, lwd=0)\npar(family=\"mono\", xpd=TRUE)\naxis(1, at=1220, labels=\"log=2\", line=-3.5, hadj=0, lwd=0)\naxis(1, at=1220, labels='log=\"e\"', line=-5, hadj=0, lwd=0)\naxis(1, at=1220, labels=\"log=10\", line=-6.5, hadj=0, lwd=0)  \nwid2 &lt;- strwidth(\"log=2\")\npar(family=\"sans\")\n\n\n\nSubsection 1.2.6: *Labeling technicalities\n\n\nSubsection 1.2.7: Graphical displays for categorical data\n\nstones &lt;- array(c(81,6,234,36,192,71,55,25), dim=c(2,2,2),\n                dimnames=list(Success=c(\"yes\",\"no\"),\n                Method=c(\"open\",\"ultrasound\"), Size=c(\"&lt;2cm\", \"&gt;=2cm\")))\nmargin12 &lt;- margin.table(stones, margin=1:2)\n\n\nbyMethod &lt;- 100*prop.table(margin12, margin=2)\npcGood &lt;- 100*prop.table(stones, margin=2:3)[\"yes\", , ]\ndimnam &lt;- dimnames(stones)\nnumOps &lt;- margin.table(stones, margin=2:3)\nopStats &lt;- data.frame(Good=c(pcGood[1,],pcGood[2,]),\n                      numOps=c(numOps[1,], numOps[2,]),\n                      opType=factor(rep(dimnam[[\"Method\"]],c(2,2))),\n                      Size=rep(dimnam[[\"Size\"]],2))\nxlim &lt;- range(opStats$Good)*c(0.65,1.015)\nylim &lt;- c(0, max(opStats$numOps)*1.15)\nplot(numOps~Good, data=opStats, type=\"h\", lwd=4, xlim=xlim, ylim=ylim,\n     fg=\"gray\",col=rep(c(\"blue\",\"red\"),rep(2,2)),\n     xlab=\"Success rate (%)\", ylab=\"Number of operations\")\n# with(opStats, text(numOps~Good, labels=Size,\n#                    col=rep(c('blue','red'),rep(2,2)),\n#                    offset=0.25,pos=3, cex=0.75))\nlabpos &lt;- lapply(split(opStats, opStats$Size), \n  function(x)apply(x[,1:2],2,function(z)c(z[1],mean(z),z[2])))\nsizeNam &lt;- names(labpos)\nlapply(labpos, function(x)lines(x[,'Good'],x[,'numOps']+c(0,35,0),\n  type=\"c\",col=\"gray\"))\ntxtmid &lt;- sapply(labpos, function(x)c(x[2,'Good'],x[2,'numOps']+35))\ntext(txtmid[1,]+c(-1.4,0.85),txtmid[2,],labels=sizeNam,col=\"gray40\",\n     pos=c(4,2), offset=0)\npar(xpd=TRUE)\ntext(byMethod[1,1:2],rep(par()$usr[4],2)+0.5*strheight(\"^\"), labels=c(\"^\",\"^\"),\n     col=c(\"blue\",\"red\"),cex=1.2,srt=180)\ntext(byMethod[1,], par()$usr[4]+1.4*strheight(\"A\"),\n     labels=paste(round(byMethod[1,],1)),cex=0.85)\ntext(byMethod[1,1:2]+c(3.5,-3.5), rep(par()$usr[4],2)+2.65*strheight(\"A\"),\nlabels=c(\"All open\",\"All ultrasound\"), pos=c(2,4))\npar(xpd=FALSE)\nabline(h=100*(0:2),col=\"lightgray\",lwd=0.5)\nabline(v=10*(5:9),col=\"lightgray\",lwd=0.5)\nlegend(\"topleft\", col=c('blue','red'),lty=c(1,1), lwd=1, cex=0.9,\n       y.intersp=0.75, legend=c(\"Open\",\"Ultrasound\"),bty=\"n\",\n       inset=c(-0.01,-0.01))\n\n\n\nSubsection 1.2.8: What to look for in plots\n\nOutliers\n\n\nAsymmetry of the distribution\n\n\nChanges in variability\n\n\nClustering\n\n\nNonlinearity\n\n\nTime trends in the data\n\n\n\n\nSection 1.3: Data Summary\n\nSubsection 1.3.1: Counts\n\n## Table of counts example: data frame nswpsid1 (DAAG)\n## Specify `useNA=\"ifany\"` to ensure that any NAs are tabulated\ntab &lt;- with(DAAG::nswpsid1, table(trt, nodeg, useNA=\"ifany\"))\ndimnames(tab) &lt;- list(trt=c(\"none\", \"training\"), educ = c(\"completed\", \"dropout\"))\ntab\n\n\nTabulation that accounts for frequencies or weights – the xtabs() function\n\ngph &lt;- lattice::bwplot(log(nassCDS$weight+1), xlab=\"Inverse sampling weights\",\n  scales=list(x=list(at=c(0,log(c(10^(0:5)+1))), labels=c(0,10^(0:5)))))\nupdate(gph, par.settings=DAAG::DAAGtheme(color=F, col.points='gray50'))\n\n\nsampNum &lt;- table(nassCDS$dead)\npopNum &lt;- as.vector(xtabs(weight ~ dead, data=nassCDS))\nrbind(Sample=sampNum, \"Total number\"=round(popNum,1))\n\n\nnassCDS &lt;- DAAG::nassCDS\nAtab &lt;- xtabs(weight ~ airbag + dead, data=nassCDS)/1000\n## Define a function that calculates Deaths per 1000\nDeadPer1000 &lt;- function(x)1000*x[2]/sum(x)\nAtabm &lt;- ftable(addmargins(Atab, margin=2, FUN=DeadPer1000))\nprint(Atabm, digits=2, method=\"compact\", big.mark=\",\")\n\n\nSAtab &lt;- xtabs(weight ~ seatbelt + airbag + dead, data=nassCDS)\n## SAtab &lt;- addmargins(SAtab, margin=3, FUN=list(Total=sum))  ## Gdet Totals\nSAtabf &lt;- ftable(addmargins(SAtab, margin=3, FUN=DeadPer1000), col.vars=3)\nprint(SAtabf, digits=2, method=\"compact\", big.mark=\",\")\n\n\nFSAtab &lt;- xtabs(weight ~ dvcat + seatbelt + airbag + dead, data=nassCDS)\nFSAtabf &lt;- ftable(addmargins(FSAtab, margin=4, FUN=DeadPer1000), col.vars=3:4)\nprint(FSAtabf, digits=1)\n\n\n\n\nSubsection 1.3.2: Summaries of information from data frames\n\n## Individual vine yields, with means by block and treatment overlaid\nkiwishade &lt;- DAAG::kiwishade\nkiwishade$block &lt;- factor(kiwishade$block, levels=c(\"west\",\"north\",\"east\"))\nkeyset &lt;- list(space=\"top\", columns=2,\ntext=list(c(\"Individual vine yields\", \"Plot means (4 vines)\")),\npoints=list(pch=c(1,3), cex=c(1,1.35), col=c(\"gray40\",\"black\")))\npanelfun &lt;- function(x,y,...){panel.dotplot(x,y, pch=1, ...)\nav &lt;- sapply(split(x,y),mean); ypos &lt;- unique(y)\nlpoints(ypos~av, pch=3, col=\"black\")}\ndotplot(shade~yield | block, data=kiwishade, col=\"gray40\", aspect=0.65,\n        panel=panelfun, key=keyset, layout=c(3,1))\n## Note that parameter settings were given both in the calls\n## to the panel functions and in the list supplied to key.\n\n\n## mean yield by block by shade: data frame kiwishade (DAAG)\nkiwimeans &lt;- with(DAAG::kiwishade, \n                  aggregate(yield, by=list(block, shade), mean))\nnames(kiwimeans) &lt;- c(\"block\",\"shade\",\"meanyield\")\nhead(kiwimeans, 4)  # First 4 rows\n\n\nThe benefits of data summary – dengue status example\n\n\n\nSubsection 1.3.3: Measures of variation\n\nCuckoo eggs example\n\noptions(width=72)\n## SD of length, by species: data frame cuckoos (DAAG)\nz &lt;- with(cuckoos, sapply(split(length,species), function(x)c(sd(x),length(x))))\nprint(setNames(paste0(round(z[1,],2),\" (\",z[2,],\")\"),\n               abbreviate(colnames(z),11)), quote=FALSE)\n\n\n\nDegrees of freedom\n\n\n\nSubsection 1.3.4: Inter-quartile range (IQR) and median absolute deviation (MAD)\n\n\nSubsection 1.3.5: A pooled standard deviation estimate\n\nElastic bands example\n&lt;&gt;= sapply(DAAG::two65, function(x) c(Mean=mean(x), sd=sd(x))) |&gt; round(2) @\n\n\n\nSubsection 1.3.6: Effect size\n\nsetNames(diff(c(ambient=244.1, heated=253.5))/c(sd=10.91), \"Effect size\")\n\nData are available in the data frame DAAG::two65.\n\nvignette('effectsize', package='effectsize')\n\n\n\nSubsection 1.3.7: Correlation\n\nset.seed(17)\nx1 &lt;- x2 &lt;- x3 &lt;- (11:30)/5\ny1 &lt;- x1 + rnorm(20, sd=0.5)\ny2 &lt;- 2 - 0.05 * x1 + 0.1 * ((x1 - 1.75))^4 + rnorm(20, sd=1.5)\ny3 &lt;- (x1 - 3.85)^2 + 0.015 + rnorm(20)\ntheta &lt;- ((2 * pi) * (1:20))/20\nx4 &lt;- 10 + 4 * cos(theta)\ny4 &lt;- 10 + 4 * sin(theta) + rnorm(20, sd=0.6)\nxy &lt;- data.frame(x = c(rep(x1, 3), x4), y = c(y1, y2, y3, y4),\n                gp = factor(rep(1:4, rep(20, 4))))\nxysplit &lt;- split(xy, xy$gp)\nrho &lt;- sapply(xysplit, function(z)with(z,cor(x,y, method=c(\"pearson\"))))\nrhoS &lt;- sapply(xysplit, function(z)with(z,cor(x,y, method=c(\"spearman\"))))\nrnam &lt;- as.list(setNames(round(c(rho,rhoS),2), paste0(\"r\",1:8)))\nstriplabs &lt;- bquote(expression(paste(r==.(r1), \"    \",r[s]==.(r5)), \n                               paste(r==.(r2), \"    \",r[s]==.(r6)),\n                               paste(r==.(r3), \"    \",r[s]==.(r7)), \n                               paste(r==.(r4), \"    \",r[s]==.(r8))), rnam)\nxyplot(y ~ x | gp, data=xy, layout=c(4,1), xlab=\"\", ylab=\"\", \n  strip=strip.custom(factor.levels=striplabs), aspect=1,\n  scales=list(relation='free', draw=FALSE), between=list(x=0.5,y=0)\n)\n\n\n\n\nSection 1.4: Distributions: quantifying uncertainty\n\nSubsection 1.4.1: Discrete distributions\n\n## dbinom(0:10, size=10, prob=0.15)\nsetNames(round(dbinom(0:10, size=10, prob=0.15), 3), 0:10)\n\n\npbinom(q=4, size=10, prob=0.15)\n\n\nqbinom(p = 0.70, size = 10, prob = 0.15)\n## Check that this lies between the two cumulative probabilities:\n## pbinom(q = 1:2, size=10, prob=0.15)\n\n\nrbinom(15, size=4, p=0.5)\n\n\n## dpois(x = 0:8, lambda = 3)\nsetNames(round(dpois(x = 0:8, lambda = 3),4), 0:8)\n## Probability of &gt; 8 raisins\n## 1-ppois(q = 8, lambda = 3)     ## Or, ppois(q=8, lambda=3, lower.tail=FALSE)\n\n\n1 - ppois(q = 8, lambda = 3)\nppois(q=8, lambda=3, lower.tail=FALSE)  ## Alternative\n1-sum(dpois(x = 0:8, lambda = 3))       ## Another alternative\n\n\nraisins &lt;- rpois(20, 3)\nraisins\n\n\nInitializing the random number generator\n\nset.seed(23286)  # Use to reproduce the sample below\nrbinom(15, size=1, p=0.5)\n\n\n\nMeans and variances\n\n\n\nSubsection 1.4.2: Continuous distributions\n\nz &lt;- seq(-3,3,length=101)\nplot(z, dnorm(z), type=\"l\", ylab=\"Normal density\",\n     yaxs=\"i\", bty=\"L\",  tcl=-0.3, fg=\"gray\",\n    xlab=\"Distance, in SDs, from mean\", cex.lab=0.9)\npolygon(c(z[z &lt;= 1.0],1.0),c(dnorm(z[z &lt;= 1.0]), dnorm(-3)), col=\"grey\")\nchh &lt;- par()$cxy[2]\narrows(-1.8, 0.32, -0.25, 0.2, length=0.07, xpd=T)\ncump &lt;- round(pnorm(1), 3)\ntext(-1.8, 0.32+0.75*chh, paste(\"pnorm(1)\\n\", \"=\", cump), xpd=T, cex=0.8)\n\n\npnormExs &lt;- c('pnorm(0)', 'pnorm(1)', 'pnorm(-1.96)', 'pnorm(1.96)',\n'pnorm(1.96, mean=2)', 'pnorm(1.96, sd=2)')\nProb &lt;- sapply(pnormExs, function(x)eval(parse(text=x)))\ndf &lt;- as.data.frame(Prob)\ndf$Prob &lt;- round(df$Prob,3)\nprint(df)\n\n\n## Plot the normal density, in the range -3 to 3\nz &lt;- pretty(c(-3,3), 30)   # Find ~30 equally spaced points\nht &lt;- dnorm(z)             # Equivalent to dnorm(z, mean=0, sd=1)\nplot(z, ht, type=\"l\", xlab=\"Normal variate\", ylab=\"Density\", yaxs=\"i\")\n# yaxs=\"i\" locates the axes at the limits of the data\n\n\nqnorm(.9)          # 90th percentile; mean=0 and SD=1\n\n\n## Additional examples:\nsetNames(qnorm(c(.5,.841,.975)), nm=c(.5,.841,.975))\nqnorm(c(.1,.2,.3))   # -1.282 -0.842 -0.524  (10th, 20th and 30th percentiles)\nqnorm(.1, mean=100, sd=10)  # 87.2 (10th percentile, mean=100, SD=10)\n\n\nDifferent ways to represent distributions\n\n\nGenerating simulated samples from the normal and other continuous distributions\n\noptions(digits=2)  # Suggest number of digits to display\nrnorm(10)          # 10 random values from the normal distribution\n\n\nmu &lt;- 10\nsigma &lt;- 1\nn &lt;- 1\nm &lt;- 50\nfour &lt;- 4\nnrep &lt;- 5\nseed &lt;- 21\ntotrows &lt;- 1\nif(is.null(totrows))\ntotrows &lt;- floor(sqrt(nrep))\ntotcols &lt;- ceiling(nrep/totrows)\nz &lt;- range(pretty(mu + (c(-3.4, 3.4) * sigma), 50))\nxy &lt;- data.frame(x=rep(0,nrep),y=rep(0,nrep),n=rep(n,nrep),\n                 mm=rep(m,nrep),four=rep(four,nrep))\nfac &lt;- factor(paste(\"Simulation\", 1:nrep),\n              lev &lt;- paste(\"Simulation\", 1:nrep))\nxlim&lt;-z\n## ylim&lt;-c(0,dnorm(0)*sqrt(n))\nylim &lt;- c(0,1)\nxy &lt;- split(xy,fac)\nxy&lt;-lapply(1:length(xy),function(i){c(as.list(xy[[i]]), list(xlim=xlim,\n           ylim=ylim))})\npanel.mean &lt;- function(data, mu = 10, sigma = 1, n2 = 1,\n                       mm = 100, nrows, ncols, ...)\n{\n  vline &lt;- function(x, y, lty = 1, col = 1)\n  lines(c(x, x), c(0, y), lty = lty, col = col)\n  n2&lt;-data$n[1]\n  mm&lt;-data$mm[1]\n  our&lt;-data$four[1]  ## Four characters in each unit interval of x\n  nmid &lt;- round(four * 4)\n  nn &lt;- array(0, 2 * nmid + 1)\n  #########################################\n  z &lt;- mu+seq(from=-3.4*sigma, to=3.4*sigma, length=mm)\n  atx&lt;-pretty(z)\n  qz &lt;- pnorm((z - mu)/sigma)\n  dz &lt;- dnorm((z - mu)/sigma)\n  chw &lt;- sigma/four\n  chh &lt;- strheight(\"O\")*0.75\n  htfac &lt;- (mm * chh)/four\n  if(nrows==1&&ncols==1)\n  lines(z, dz * htfac)\n  if(nrows==1)axis(1,at=atx, lwd=0, lwd.ticks=1)\n  y &lt;- rnorm(mm, mu, sigma/sqrt(n2))\n  pos &lt;- round((y - mu)/sigma * four)\n  for(i in 1:mm) {\n    nn[nmid + pos[i]] &lt;- nn[nmid + pos[i]] + 1\n    xpos &lt;- chw * pos[i]\n    text(mu + xpos, nn[nmid + pos[i]] * chh - chh/4, \"x\")\n  }\n}\nDAAG::panelplot(xy,panel=panel.mean,totrows=totrows,totcols=totcols,\n  oma=c(1.5, 0, rep(0.5,2)), fg='gray')\n\n\n## The following gives a conventional histogram representations:\nset.seed (21)        # Use to reproduce the data in the figure\ndf &lt;- data.frame(x=rnorm(250), gp=rep(1:5, rep(50,5)))\nlattice::histogram(~x|gp, data=df, layout=c(5,1))\n\n\nrunif(n = 20, min=0, max=1) # 20 numbers, uniform distn on (0, 1)\nrexp(n=10, rate=3)          # 10 numbers, exponential, mean 1/3.\n\n\n\n\nSubsection 1.4.3: Graphical checks for normality\n\ntab &lt;- t(as.matrix(DAAG::pair65))\nrbind(tab,\"heated-ambient\"=tab[1,]-tab[2,])\n\n\n## Normal quantile-quantile plot for heated-ambient differences,\n## compared with plots for random normal samples of the same size\nplt &lt;- with(DAAG::pair65, DAAG::qreference(heated-ambient, nrep=10, nrows=2))\nupdate(plt, scales=list(tck=0.4), xlab=\"\")\n\n\n\nSubsection 1.4.4: Population parameters and sample statistics\n\nThe sampling distribution of the mean\n\nlibrary(lattice)\n## Generate n sample values; skew population\nsampfun = function(n) exp(rnorm(n, mean = 0.5, sd = 0.3))\ngph &lt;- DAAG::sampdist(sampsize = c(3, 9, 30), seed = 23, nsamp = 1000,\n  FUN = mean, sampvals=sampfun, plot.type = \"density\")\nsamptheme &lt;- DAAG::DAAGtheme(color=FALSE)\nprint(update(gph, scales=list(tck=0.4),  layout = c(3,1),\n             par.settings=samptheme, main=list(\"A: Density curves\", cex=1.25)),\n      position=c(0,0.5,1,1), more=TRUE)\nsampfun = function(n) exp(rnorm(n, mean = 0.5, sd = 0.3))\ngph &lt;- DAAG::sampdist(sampsize = c(3, 9, 30), seed = 23, nsamp = 1000, \n                      FUN = mean, sampvals=sampfun, plot.type = \"qq\")\nprint(update(gph, scales=list(tck=0.4), layout = c(3,1),\n             par.settings=samptheme, \n             main=list(\"B: Normal quantile-quantile plots\", cex=1.25)),\n      position=c(0,0,1,0.5))\n\n\n\nThe standard error of the median\n\n\nSimulation in learning and research\n\n\n\nSubsection 1.4.5: The \\(t\\)-distribution\n\nx &lt;- seq(from=-4.2, to = 4.2, length.out = 50)\nylim &lt;- c(0, dnorm(0))\nylim[2] &lt;- ylim[2]+0.1*diff(ylim)\nh1 &lt;- dnorm(x)\nh3 &lt;- dt(x, 3)\nh8 &lt;- dt(x,8)\nplot(x, h1, type=\"l\", xlab = \"\", xaxs=\"i\", ylab = \"\", yaxs=\"i\",\nbty=\"L\", ylim=ylim, fg=\"gray\")\nmtext(side=3,line=0.5, \"A: Normal (t8 overlaid)\", adj=-0.2)\nlines(x, h8, col=\"grey60\")\nmtext(side=1, line=1.75, \"No. of SEMs from mean\")\nmtext(side=2, line=2.0, \"Probability density\")\nchh &lt;- par()$cxy[2]\ntopleft &lt;- par()$usr[c(1,4)] + c(0, 0.6*chh)\nlegend(topleft[1], topleft[2], col=c(\"black\",\"grey60\"),\nlty=c(1,1), legend=c(\"Normal\",\"t (8 d.f.)\"), bty=\"n\", cex=0.8)\nplot(x, h1, type=\"l\", xlab = \"\", xaxs=\"i\",\nylab = \"\", yaxs=\"i\", bty=\"L\", ylim=ylim, fg=\"gray\")\nmtext(side=3,line=0.5, \"B: Normal (t3 overlaid)\", adj=-0.2)\nlines(x, h3, col=\"grey60\")\nmtext(side=1, line=1.75, \"No. of SEMs from mean\")\n## mtext(side=2, line=2.0, \"Probability density\")\nlegend(topleft[1], topleft[2], col=c(\"black\",\"grey60\"),\nlty=c(1,1), legend=c(\"Normal\",\"t (3 d.f.)\"), bty=\"n\", cex=0.8)\n## Panels C and D\ncump &lt;- 0.975\nx &lt;- seq(from=-3.9, to = 3.9, length.out = 50)\nylim &lt;- c(0, dnorm(0))\nplotfun &lt;- function(cump, dfun = dnorm, qfun=qnorm,\nytxt = \"Probability density\",\ntxt1=\"qnorm\", txt2=\"\", ...)\n{\nh &lt;- dfun(x)\nplot(x, h, type=\"l\", xlab = \"\", xaxs=\"i\", xaxt=\"n\",\nylab = ytxt, yaxs=\"i\", bty=\"L\", ylim=ylim, fg=\"gray\",\n...)\naxis(1, at=c(-2, 0), cex=0.8, lwd=0, lwd.ticks=1)\naxis(1, at=c((-3):3), labels=F, lwd=0, lwd.ticks=1)\ntailp &lt;- 1-cump\nz &lt;- qfun(cump)\nztail &lt;- pretty(c(z,4),20)\nhtail &lt;- dfun(ztail)\npolygon(c(z,z,ztail,max(ztail)), c(0,dfun(z),htail,0), col=\"gray\")\ntext(0, 0.5*dfun(z)+0.08*dfun(0),\npaste(round(tailp, 3), \" + \", round(1-2*tailp,3),\n\"\\n= \", round(cump, 3), sep=\"\"), cex=0.8)\nlines(rep(z, 2), c(0, dfun(z)))\nlines(rep(-z, 2), c(0, dfun(z)), col=\"gray60\")\nchh &lt;- par()$cxy[2]\narrows(z, -1.5*chh,z,-0.1*chh, length=0.1, xpd=T)\ntext(z, -2.5*chh, paste(txt1, \"(\", cump, txt2, \")\", \"\\n= \",\nround(z,2), sep=\"\"), xpd=T)\nx1 &lt;- z + .3\ny1 &lt;- dfun(x1)*0.35\ny0 &lt;- dfun(0)*0.2\narrows(-2.75, y0, -x1, y1, length=0.1, col=\"gray60\")\narrows(2.75, y0, x1, y1, length=0.1)\ntext(-2.75, y0+0.5*chh, tailp, col=\"gray60\")\ntext(2.75, y0+0.5*chh, tailp)\n}\n## ytxt &lt;- \"t probability density (8 d.f.)\"\nplotfun(cump=cump, cex.lab=1.05)\nmtext(side=3, line=1.25, \"C: Normal distribution\", adj=-0.2)\nytxt &lt;- \"t probability density (8 d.f.)\"\nplotfun(cump=cump, dfun=function(x)dt(x, 8),\n        qfun=function(x)qt(x, 8),\n        ytxt=\"\", txt1=\"qt\", txt2=\", 8\", cex.lab=1.05)\nmtext(side=3, line=1.25, \"D: t distribution (8 df)\", adj=-0.2)\n\n\nqnorm(c(0.975,0.995), mean=0)    # normal distribution\nqt(c(0.975, 0.995), df=8)        # t-distribution with 8 d.f.\n\n\n\nSubsection 1.4.6: The likelihood, and maximum likelihood estimation\n\n\n\nSection 1.5: Simple forms of regression model\n\nSubsection 1.5.1: Line or curve?\n\nroller &lt;- DAAG::roller\nt(cbind(roller, \"depression/weight ratio\"=round(roller[,2]/roller[,1],2)))\n\n\nUsing models to predict\n\ny &lt;- DAAG::roller$depression\nx &lt;- DAAG::roller$weight\npretext &lt;- c(reg = \"A\", lo = \"B\")\nfor(curve in c(\"reg\", \"lo\")) {\n  plot(x, y, xlab = \"Roller weight (t)\", xlim=c(0,12.75), fg=\"gray\",\n       ylab = \"Depression in lawn (mm)\", type=\"n\")\n  points(x, y, cex=0.8, pch = 4)\n  mtext(side = 3, line = 0.25, pretext[curve], adj = 0)\n  topleft &lt;- par()$usr[c(1, 4)]\n  chw &lt;- strwidth(\"O\"); chh &lt;- strheight(\"O\")\n  points(topleft[1]+rep(0.75,2)*chw,topleft[2]-c(0.75,1.8)*chh,\n         pch=c(4,1), col=c(\"black\",\"gray40\"), cex=0.8)\n  text(topleft[1]+rep(1.2,2)*chw, topleft[2]-c(0.75,1.8)*chh,\n       c(\"Data values\", \"Fitted values\"),adj=0, cex=0.8)\n  if(curve==\"lo\")\n    text(topleft[1]+1.2*chw, topleft[2]-2.85*chh,\"(smooth)\", adj=0, cex=0.8)\n  if(curve[1] == \"reg\") {\n    u &lt;- lm(y ~ -1 + x)\n  abline(0, u$coef[1])\n  yhat &lt;- predict(u)\n}\nelse {\n  lawn.lm&lt;-lm(y~x+I(x^2))\n  yhat&lt;-predict(lawn.lm)\n  xnew&lt;-pretty(x,20)\n  b&lt;-lawn.lm$coef\n  ynew&lt;-b[1]+b[2]*xnew+b[3]*xnew^2\n  lines(xnew,ynew)\n}\nhere &lt;- y &lt; yhat\nyyhat &lt;- as.vector(rbind(y[here], yhat[here], rep(NA, sum(here))))\nxx &lt;- as.vector(rbind(x[here], x[here], rep(NA, sum(here))))\nlines(xx, yyhat, lty = 2, col=\"gray\")\nhere &lt;- y &gt; yhat\nyyhat &lt;- as.vector(rbind(y[here], yhat[here], rep(NA, sum(here))))\nxx &lt;- as.vector(rbind(x[here], x[here], rep(NA, sum(here))))\nlines(xx, yyhat, lty = 1, col=\"gray\")\nn &lt;- length(y)\nns &lt;- min((1:n)[y - yhat &gt;= 0.75*max(y - yhat)])\nypos &lt;- 0.5 * (y[ns] + yhat[ns])\nchw &lt;- par()$cxy[1]\ntext(x[ns] - 0.25*chw, ypos, \"+ve residual\", adj = 1,cex=0.75, col=\"gray30\")\npoints(x, yhat, pch = 1, col=\"gray40\")\nns &lt;- (1:n)[y - yhat == min(y - yhat)][1]\nypos &lt;- 0.5 * (y[ns] + yhat[ns])\ntext(x[ns] + 0.4*chw, ypos, \"-ve residual\", adj = 0,cex=0.75,col=\"gray30\")\n}\n\n\n\nWhich model is best — line or curve?\n\n\n\nSubsection 1.5.2: Fitting models – the model formula\n\n## Fit line - by default, this fits intercept & slope.\nroller.lm &lt;- lm(depression ~ weight, data=DAAG::roller)\n## Compare with the code used to plot the data\nplot(depression ~ weight, data=DAAG::roller)\n## Add the fitted line to the plot\nabline(roller.lm)\n\n\n## For a model that omits the intercept term, specify\nlm(depression ~ 0 + weight, data=roller)  # Or, if preferred, replace `0` by `-1`\n\n\nModel objects\n\nroller.lm &lt;- lm(depression ~ weight, data=DAAG::roller)\nnames(roller.lm)     # Get names of list elements\n\n\ncoef(roller.lm)           # Extract coefficients\nsummary(roller.lm)        # Extract model summary information\ncoef(summary(roller.lm))  # Extract coefficients and SEs\nfitted(roller.lm)         # Extract fitted values\npredict(roller.lm)        # Predictions for existing or new data, with SE\n                          # or confidence interval information if required.\nresid(roller.lm)          # Extract residuals\n\n\nroller.lm$coef            # An alternative is roller.lm[[\"coef\"]]\n\n\nprint(summary(roller.lm), digits=3)\n\n\n\nResidual plots\n\n## Normal quantile-quantile plot, plus 7 reference plots\nDAAG::qreference(residuals(roller.lm), nrep=8, nrows=2, xlab=\"\")\n\n\n\nSimulation of regression data\n\nroller.lm &lt;- lm(depression ~ weight, data=DAAG::roller)\nroller.sim &lt;- simulate(roller.lm, nsim=20)  # 20 simulations\n\n\nwith(DAAG::roller, matplot(weight, roller.sim, pch=1, ylim=range(depression)))\npoints(DAAG::roller, pch=16)\n\n\n\n\nSubsection 1.5.3: The model matrix in regression\n\nmodel.matrix(roller.lm)\n## Specify coef(roller.lm) to obtain the column multipliers.\n\n\nFrom straight line regression to multiple regression\n\nmouse.lm &lt;- lm(brainwt ~ lsize+bodywt, data=DAAG::litters)\ncoef(summary(mouse.lm))\n\n\n\n\n\nSection 1.6: Data-based judgments – frequentist, in a Bayesian world\n\nSubsection 1.6.1: Inference with known prior probabilities\n\n## `before` is the `prevalence` or `prior`. \nafter &lt;- function(prevalence, sens, spec){\n  prPos &lt;- sens*prevalence + (1-spec)*(1-prevalence)\n  sens*prevalence/prPos}\n## Compare posterior for a prior of 0.002 with those for 0.02 and 0.2\nsetNames(round(after(prevalence=c(0.002, 0.02, 0.2), sens=.8, spec=.95), 3),\n         c(\"Prevalence=0.002\", \"Prevalence=0.02\", \"Prevalence=0.2\"))\n\n\nRelating ‘incriminating’ evidence to the probability of guilt\n\n\n\nSubsection 1.6.2: Treatment differences that are on a continuous scale\n\n## Use pipe syntax, introduced in R 4.1.0\nsleep &lt;- with(datasets::sleep, extra[group==2] - extra[group==1])\nsleep |&gt; (function(x)c(mean = mean(x), SD = sd(x), n=length(x)))() |&gt; \n     (function(x)c(x, SEM=x['SD']/sqrt(x['n'])))() |&gt;\n     setNames(c(\"mean\",\"SD\",\"n\",\"SEM\")) -&gt; stats\n     print(stats, digits=3)\n\n\n## Sum of tail probabilities\n2*pt(1.580/0.389, 9, lower.tail=FALSE)  \n\n\n## 95% CI for mean of heated-ambient: data frame DAAG::pair65\nt.test(sleep, conf.level=0.95)\n\n\nAn hypothesis test\n\npt(4.06, 9, lower.tail=F)\n\n\n\nThe \\(p\\)-value probability relates to a decision process\n\n\n\nSubsection 1.6.3: Use of simulation with \\(p\\)-values\n\neff2stat &lt;- function(eff=c(.2,.4,.8,1.2), n=c(10,40), numreps=100,\n                     FUN=function(x,N)pt(sqrt(N)*mean(x)/sd(x), df=N-1, \n                                         lower.tail=FALSE)){\n  simStat &lt;- function(eff=c(.2,.4,.8,1.2), N=10, nrep=100, FUN){\n    num &lt;- N*nrep*length(eff)\n    array(rnorm(num, mean=eff), dim=c(length(eff),nrep,N)) |&gt;\n      apply(2:1, FUN, N=N) \n  }\n  mat &lt;- matrix(nrow=numreps*length(eff),ncol=length(n))\n  for(j in 1:length(n)) mat[,j] &lt;- \n    as.vector(simStat(eff, N=n[j], numreps, FUN=FUN))  ## length(eff)*numep\n  data.frame(effsize=rep(rep(eff, each=numreps), length(n)),\n             N=rep(n, each=numreps*length(eff)), stat=as.vector(mat))\n}\n\n\nset.seed(31)\ndf200 &lt;- eff2stat(eff=c(.2,.4,.8,1.2), n=c(10, 40), numreps=200)\nlabx &lt;- c(0.001,0.01,0.05,0.2,0.4,0.8)\ngph &lt;- bwplot(factor(effsize) ~ I(stat^0.25) | factor(N), data=df200, \n              layout=c(2,1), xlab=\"P-value\", ylab=\"Effect size\", \n              scales=list(x=list(at=labx^0.25, labels =labx)))\nupdate(gph+latticeExtra::layer(panel.abline(v=labx[1:3]^0.25, col='lightgray')),\n       strip=strip.custom(factor.levels=paste0(\"n=\",c(10,40))),\n       par.settings=DAAG::DAAGtheme(color=F, col.points=\"gray50\"))\n\n\neff10 &lt;- with(subset(df200, N==10&effsize==0.2), c(gt5pc=sum(stat&gt;0.05), lohi=fivenum(stat)[c(2,4)]))\neff40 &lt;- with(subset(df200, N==40&effsize==0.2), c(gt5pc=sum(stat&gt;0.05), lohi=fivenum(stat)[c(2,4)]))\n\n\n\nSubsection 1.6.4: Power — minimizing the chance of false positives\n\ntf1 &lt;- rbind('R=0.2'=c(0.8*50, 0.05*250),\n'R=1'=c(0.8*150, 0.05*150),\n'R=5'=c(0.8*200, 0.05*50))\ntf2 &lt;- rbind(c('0.8 x50', '0.05x250'),\nc('0.8x150', '0.05x150'),\nc('0.8x250', '0.05 x50'))\ntf &lt;- cbind(\"True positives\"=paste(tf2[,2],tf1[,2],sep=\"=\"),\n\"False positives\"=paste(tf2[,1],tf1[,1],sep=\"=\"))\nrownames(tf) &lt;- rownames(tf1)\nprint(tf, quote=FALSE)\n\n\nPower calculations – examples\n\npower.t.test(d=0.5, sig.level=0.05, type=\"one.sample\", power=0.8)\npwr1 &lt;- power.t.test(d=0.5, sig.level=0.005, type=\"one.sample\", power=0.8)\npwr2 &lt;- power.t.test(d=0.5, sig.level=0.005, type=\"two.sample\", power=0.8)\n## d=0.5, sig.level=0.005, One- and two-sample numbers \nc(\"One sample\"=pwr1$n, \"Two sample\"=pwr2$n)\n\n\neffsize &lt;- c(.05,.2,.4,.8,1.2); npairs &lt;- c(10,20,40)\npwr0.05 &lt;- matrix(nrow=length(effsize), ncol=length(npairs),\n               dimnames=list(paste0('ES=',effsize), paste0('n=',npairs)))\npwr0.005 &lt;- matrix(nrow=length(effsize), ncol=length(npairs),\n               dimnames=list(paste0(effsize), paste0('n=',npairs)))\nfor(i in 1:length(effsize)) for(j in 1:length(npairs)){\n    pwr0.05[i,j] &lt;- power.t.test(n=npairs[j],d=effsize[i],sig.level=0.05,\n                                 type='one.sample')$power\n    pwr0.005[i,j] &lt;- power.t.test(n=npairs[j],d=effsize[i],sig.level=0.005,\n                                  type='one.sample')$power}\ntab &lt;- cbind(round(pwr0.05,4), round(pwr0.005,4))\ntab[1:3,] &lt;- round(tab[1:3,],3)\ntab[5,3] &lt;- '~1.0000'\ntab[5,6] &lt;- '~1.0000'\n\n\nprint(tab[,1:3], quote=F)\n\n\nprint(tab[,4:6], quote=F)\n\n\neffsize &lt;- c(.05,.2,.4,.8,1.2); npairs &lt;- c(10,20,40)\npwr0.05 &lt;- matrix(nrow=length(effsize), ncol=length(npairs),\n               dimnames=list(paste0('ES=',effsize), paste0('n=',npairs)))\npwr0.005 &lt;- matrix(nrow=length(effsize), ncol=length(npairs),\n               dimnames=list(paste0(effsize), paste0('n=',npairs)))\nfor(i in 1:length(effsize)) for(j in 1:length(npairs)){\n    pwr0.05[i,j] &lt;- power.t.test(n=npairs[j],d=effsize[i],sig.level=0.05,\n                                 type='one.sample')$power\n    pwr0.005[i,j] &lt;- power.t.test(n=npairs[j],d=effsize[i],sig.level=0.005,\n                                  type='one.sample')$power}\ntab &lt;- cbind(round(pwr0.05,4), round(pwr0.005,4))\ntab[1:3,] &lt;- round(tab[1:3,],3)\ntab[5,3] &lt;- '~1.0000'\ntab[5,6] &lt;- '~1.0000'\n\n\n\nPositive Predictive Values\n\nR &lt;- pretty(0:3, 40)\npostOdds &lt;- outer(R/0.05,c(.8,.3,.08))\nPPV &lt;- as.data.frame(cbind(R,postOdds/(1+postOdds)))\nnames(PPV) &lt;- c(\"R\",\"p80\",\"p30\",\"p8\")\nkey &lt;- list(text = list(text=c(\"80% power\",\"30% power\", \"8% power\"), cex = 1.0),\n            x = .6, y = .25, color=F)\ngph &lt;- lattice::xyplot(p80+p30+p8~R, data=PPV, lwd=2, type=c(\"l\",\"g\"), \n  xlab=\"Pre-study odds R\", ylab=\"Post-study probability (PPV)\")\nupdate(gph, scales=list(tck=0.5), key=key) \n\n\n\n\nSubsection 1.6.5: The future for \\(p\\)-values\n\nHow small a \\(p\\)-value is needed?\n\n\n\nSubsection 1.6.6: Reporting results\n\nIs there an alternative that is more likely?\n\n\n\n\nSection 1.7: Information statistics and Bayesian methods with Bayes Factors\n\nSubsection 1.7.1: Information statistics – using likelihoods for model choice\n\n## Calculations using mouse brain weight data\nmouse.lm &lt;- lm(brainwt ~ lsize+bodywt, data=DAAG::litters)\nn &lt;- nrow(DAAG::litters)\nRSSlogLik &lt;- with(mouse.lm, n*(log(sum(residuals^2)/n)+1+log(2*pi)))\np &lt;- length(coef(mouse.lm))+1  # NB: p=4 (3 coefficients + 1 scale parameter)\nk &lt;- 2*n/(n-p-1)\nc(\"AICc\" = AICcmodavg::AICc(mouse.lm), fromlogL=k*p-2*logLik(mouse.lm)[1], \n  fromFit=k*p + RSSlogLik) |&gt; print(digits=4)\n\n\nThe sampling properties of the difference in AIC statistics\n\nsim0vs1 &lt;- function(mu=0, n=15, ntimes=200){\na0 &lt;- a1 &lt;- numeric(ntimes)\nfor(i in 1:ntimes){\n  y &lt;- rnorm(n, mean=mu, sd=1)\n  m0 &lt;- lm(y ~ 0); m1 &lt;- lm(y ~ 1) \n  a0[i] &lt;- AIC(m0); a1[i] &lt;- AIC(m1)\n}\ndata.frame(a0=a0, a1=a1, diff01=a0-a1, mu=rep(paste0(\"mu=\",mu)))\n}\nlibrary(latticeExtra)\nsim0 &lt;- sim0vs1(mu=0)\nsim0.5 &lt;- sim0vs1(mu=0.5)\nsimboth &lt;- rbind(sim0, sim0.5)\ncdiff &lt;- with(list(n=15, p=2), 2*(p+1)*p/(n-(p+1)-1))\nxyplot(diff01 ~ a0 | mu, data=simboth, xlab=\"AIC(m0)\", ylab=\"AIC(m0) - AIC(m1)\") + \n  latticeExtra::layer({panel.abline(h=0, col='red'); \n         panel.abline(h=cdiff, lwd=1.5, lty=3, col='red', alpha=0.5);\n         panel.abline(h=-2, lty=2, col='red')})\n\n\ntab &lt;- rbind(c(with(sim0, sum(diff01&gt;0))/200, with(sim0.5, sum(diff01&gt;0))/200),\n  c(with(sim0,sum(diff01&gt;-cdiff))/200, with(sim0.5, sum(diff01&gt;-cdiff))/200))\ndimnames(tab) &lt;- list(c(\"AIC: Proportion choosing m1\",\n                        \"AICc: Proportion choosing m1\"),\n                      c(\"True model is m0\", \"True model is m1\"))\ntab\n\n\n\n\nSubsection 1.7.2: Bayesian methods with Bayes Factors\n\n## Setting `scale=1/sqrt(2)` gives a mildly narrower distribution\nprint(c(\"pcauchy(1, scale=1)\"=pcauchy(1, scale=1), \n        \"   pcauchy(1, scale=1/sqrt(2))\"=pcauchy(1, scale=1/sqrt(2))),\n      quote=FALSE)\n\n\nThe Cauchy prior with different choices of scale parameter\n\nx &lt;- seq(from=-4.5, to=4.5, by=0.1)\ndensMed &lt;- dcauchy(x,scale=sqrt(2)/2)\ndensUltra &lt;- dcauchy(x, scale=sqrt(2))\ndenn &lt;- dnorm(x, sd=1)\nplot(x,densMed, type='l', mgp=c(2,0.5,0), xlab=\"\",\n     ylab=\"Prior density\", col=\"red\", fg='gray')\nmtext(side=1, line=2, expression(\"Effect size \"==phantom(0)*delta), cex=1.1)\nlines(x, denn, col=\"blue\", lty=2)\nlines(x, densUltra,col=2, lty=2)\nlegend(\"topleft\", title=\"Normal prior\",\n       y.intersp=0.8, lty=2, col=\"blue\", bty='n', cex=0.8,\n       legend=expression(bold('sd=1')))\nlegend(\"topright\", title=\"Cauchy priors\", y.intersp=0.8,\n       col=c('red', 'red'),lty=c(1,2), cex=0.8,\n         legend=c(expression(bold('medium')),\n         expression(bold('ultrawide'))),bty=\"n\")\nmtext(side=3, line=0.25, adj=0, cex=1.15,\n      expression(\"A: Alternative priors for \"*delta==frac(mu,sigma)))\n## Panel B\npairedDiffs &lt;- with(datasets::sleep, extra[group==2] - extra[group==1])\nttBF0 &lt;- BayesFactor::ttestBF(pairedDiffs)\nsimpost &lt;- BayesFactor::posterior(ttBF0, iterations=10000)\nplot(density(simpost[,'mu']), main=\"\", xlab=\"\", col=\"red\",\n     mgp=c(2,0.5,0), ylab=\"Posterior density\", fg='gray')\nmtext(side=1, line=2, expression(mu), cex=1.1)\nabline(v=mean(pairedDiffs), col=\"gray\")\nmtext(side=3, line=0.5, expression(\"B: Posterior density for \"*mu), adj=0, cex=1.15)\n\n\n## Calculate and plot density for default prior - Selected lines of code\nx &lt;- seq(from=-4.5, to=4.5, by=0.1)\ndensMed &lt;- dcauchy(x, scale=sqrt(2)/2)\nplot(x, densMed, type='l')\n## Panel B\npairedDiffs &lt;- with(datasets::sleep, extra[group==2] - extra[group==1])\nttBF0 &lt;- BayesFactor::ttestBF(pairedDiffs)\n## Sample from posterior, and show density plot for mu\nsimpost &lt;- BayesFactor::posterior(ttBF0, iterations=10000)\nplot(density(simpost[,'mu']))\n\n\n\nA thought experiment\n\ntval &lt;- setNames(qt(1-c(.05,.01,.005)/2, df=19), paste(c(.05,.01,.005)))\nbf01 &lt;- setNames(numeric(3), paste(c(.05,.01,.005)))\nfor(i in 1:3)bf01[i] &lt;- BayesFactor::ttest.tstat(tval[i],n1=20, simple=T)\n\n\npairedDiffs &lt;- with(datasets::sleep, extra[group==2] - extra[group==1])\nttBF0 &lt;- BayesFactor::ttestBF(pairedDiffs)\nttBFwide &lt;- BayesFactor::ttestBF(pairedDiffs, rscale=1)\nttBFultra &lt;- BayesFactor::ttestBF(pairedDiffs, rscale=sqrt(2))\nrscales &lt;- c(\"medium\"=sqrt(2)/2, \"wide\"=1, ultrawide=sqrt(2))\nBF3 &lt;- c(as.data.frame(ttBF0)[['bf']], as.data.frame(ttBFwide)[['bf']],\n         as.data.frame(ttBFultra)[['bf']])\nsetNames(round(BF3,2), c(\"medium\", \"wide\", \"ultrawide\"))\n\n\npval &lt;- t.test(pairedDiffs)[['p.value']]\n1/(-exp(1)*pval*log(pval))\n\n\n\nA null interval may make better sense\n\nmin45 &lt;- round(0.75/sd(pairedDiffs),2)   ## Use standardized units\nttBFint &lt;- BayesFactor::ttestBF(pairedDiffs, nullInterval=c(-min45,min45))\nround(as.data.frame(ttBFint)['bf'],3)\n\n\nbf01 &lt;- as.data.frame(ttBFint)[['bf']]\n\n\n\nThe effect of changing sample size\n\nt2bfInterval &lt;- function(t, n=10, rscale=\"medium\", mu=c(-.1,.1)){\n     null0 &lt;- BayesFactor::ttest.tstat(t=t, n1=n, nullInterval=mu,\n                                       rscale=rscale,simple=TRUE)\nalt0 &lt;- BayesFactor::ttest.tstat(t=t, n1=n, nullInterval=mu, rscale=rscale, \n                                 complement=TRUE, simple=TRUE)\nalt0/null0\n}\n##\n## Calculate Bayes factors\npval &lt;- c(0.05,0.01,0.001); nval &lt;- c(4,6,10,20,40,80,160)\nbfDF &lt;- expand.grid(p=pval, n=nval)\npcol &lt;- 1; ncol &lt;- 2; tcol &lt;- 3\nbfDF[,'t'] &lt;- apply(bfDF,1,function(x){qt(x[pcol]/2, df=x[ncol]-1,                                  lower.tail=FALSE)})\nother &lt;- apply(bfDF,1,function(x)\n    c(BayesFactor::ttest.tstat(t=x[tcol], n1=x[ncol], rscale=\"medium\",\n                               simple=TRUE),\n## Now specify a null interval\n    t2bfInterval(t=x[tcol], n=x[ncol], mu=c(-0.1,0.1),rscale=\"medium\")\n  ))\nbfDF &lt;- setNames(cbind(bfDF, t(other)),\n    c('p','n','t','bf','bfInterval'))\n\n\nplabpos &lt;- with(subset(bfDF, n==max(bfDF$n)), log((bf+bfInterval)/2))\ngphA1 &lt;- lattice::xyplot(log(bf)~log(n), groups=factor(p), data=bfDF,\n                        panel=function(x,y,...){\n                        lattice::panel.xyplot(x,y,type='b',...)})\nylabA &lt;- 10^((-3):6/2)\nscalesA &lt;- list(x=list(at=log(nval), labels=nval),\n                y=list(at=log(ylabA), labels=signif(ylabA,2)))\nkeyA &lt;- list(corner=c(0.99,0.98), lines=list(col=c(1,1), lty=1:2),\n             text=list(c('Point null at 0', \"null=(-0.1,0.1)\")))\nylim2 &lt;- log(c(min(bfDF[['bfInterval']])-0.05,150)) \ngphA2 &lt;- lattice::xyplot(log(bfInterval)~log(n), groups=factor(p), lty=2,\n  xlim=c(log(3.5),log(max(nval)*3.25)), ylim=ylim2, data=bfDF,\n  panel=function(x,y,...){\n    panel.xyplot(x,y,type='b',...)\n    panel.grid(h=-1,v=-1)\n    panel.text(rep(log(max(nval*0.975)),3), plabpos, \n      labels=c('p=0.05','0.01','0.001'), pos=4)\n  },\n  par.settings=DAAG::DAAGtheme(color=T),\n  main=\"A: Bayes factor vs sample size\", \n  xlab=\"Sample size\", ylab=\"Bayes factor\", scales=scalesA, key=keyA)\n## Panel B\nbfDF[['eff']] = bfDF[[\"t\"]]/sqrt(bfDF[['n']])\nylabB &lt;- 10^((-3):2/3)\nscalesB= list(x=list(at=log(nval), labels=nval),\n              y=list(at=log(ylabB), labels=signif(ylabB,2)))\nkeyB &lt;- list(corner=c(0.98,0.975), lines=list(lty=1:3), \n             points=list(pch=1:3), text=list(c('p=0.001','p=0.01','p=0.05')))\ngphB &lt;- xyplot(log(eff)~log(n), groups=log(p), data=bfDF, pch=1:3, lty=1:3, \n               type='b', xlab=\"Sample size\", ylab=\"Effect  size\",\n               par.settings=DAAG::DAAGtheme(color=T),\n  main=\"B: Effect size vs sample size\", key=keyB, scales=scalesB) +\n  latticeExtra::layer(panel.grid(h=-1,v=-1))\nplot(gphA2+latticeExtra::as.layer(gphA1), position=c(0, 0, 0.525, 1), more=T)\nplot(gphB, position=c(0.52, 0, 1, 1), par.settings=DAAG::DAAGtheme(color=T))\n\n\n\nDifferent statistics give different perspectives\n\nn1 &lt;- BayesFactor::ttest.tstat(qt(0.00001, df=40), n1=40, simple=T)\nn2 &lt;- BayesFactor::ttest.tstat(qt(0.000001, df=40), n1=40, simple=T)\n\n\nbf1 &lt;- BayesFactor::ttest.tstat(qt(0.00001, df=40), n1=40, simple=T)\nbf2 &lt;- BayesFactor::ttest.tstat(qt(0.000001, df=40), n1=40, simple=T)\nrbind(\"Bayes Factors\"=setNames(c(bf1,bf2), c(\"p=0.00001\",\"p=0.000001\")),\n  \"t-statistics\"=c(qt(0.00001, df=40), qt(0.000001, df=40))) \n\n\nknitr::kable(matrix(c(\"A bare mention\",\"Positive\",\"Strong\",\"Very strong\"), nrow=1),\n       col.names=c(\"1 -- 3\", \"3 -- 20\", \"20 -- 150\", \"&gt;150\"), align='c',\n      midrule='', vline=\"\")\n\n\n\n*Technical details of the family of priors used in the BayesFactor package\n\n\n\n\nSection 1.8: Resampling methods for SEs, tests and confidence intervals\n\nSubsection 1.8.1: The one-sample permutation test\n\ntab &lt;- t(as.matrix(DAAG::pair65))\nrbind(tab,\"heated-ambient\"=tab[1,]-tab[2,])\n\n\n\nSubsection 1.8.2: The two-sample permutation test\n\n## First of 3 curves; permutation distribution of difference in means\ntwo65 &lt;- DAAG::two65\nset.seed(47)        # Repeat curves shown here\nnsim &lt;- 2000; dsims &lt;- numeric(nsim)\nx &lt;- with(two65, c(ambient, heated))\nn &lt;- length(x); n2 &lt;- length(two65$heated)\ndbar &lt;- with(two65, mean(heated)-mean(ambient))\nfor(i in 1:nsim){\n  mn &lt;- sample(n,n2,replace=FALSE); dsims[i] &lt;- mean(x[mn]) - mean(x[-mn]) }\nplot(density(dsims), xlab=\"\", main=\"\", lwd=0.5, yaxs=\"i\", ylim=c(0,0.08), bty=\"n\")\nabline(v=c(dbar, -dbar), lty=3)\npval1 &lt;- (sum(dsims &gt;= abs(dbar)) + sum (dsims &lt;= -abs(dbar)))/nsim\nmtext(side=3,line=0.25,\n  text=expression(bar(italic(x))[2]-bar(italic(x))[1]), at=dbar)\nmtext(side=3,line=0.25,\n  text=expression(-(bar(italic(x))[2] - bar(italic(x))[1])), at=-dbar)\n## Second permutation density\nfor(i in 1:nsim){\nmn &lt;- sample(n,n2,replace=FALSE)\ndsims[i] &lt;- mean(x[mn]) - mean(x[-mn])\n}\npval2 &lt;- (sum(dsims &gt;= abs(dbar)) + sum (dsims &lt;= -abs(dbar)))/nsim\nlines(density(dsims),lty=2,lwd=1)\n## Third permutation density\nfor(i in 1:nsim){\nmn &lt;- sample(n,n2,replace=FALSE)\ndsims[i] &lt;- mean(x[mn]) - mean(x[-mn])\n}\npval3 &lt;- (sum(dsims &gt;= abs(dbar)) + sum (dsims &lt;= -abs(dbar)))/nsim\nlines(density(dsims),lty=3,lwd=1.25)\nbox(col=\"gray\")\nleg3 &lt;- paste(c(pval1,pval2,pval3))\nlegend(x=20, y=0.078, title=\"P-values are\", cex=1, xpd=TRUE,\n  bty=\"n\", lty=c(1,2,3), lwd=c(1,1,1,1.25), legend=leg3, y.intersp=0.8)\n\n\n\nSubsection 1.8.3: Estimating the standard error of the median: bootstrapping\n\n## Bootstrap estimate of median of wren length: data frame cuckoos\nwren &lt;- subset(DAAG::cuckoos, species==\"wren\")[, \"length\"]\nlibrary(boot)\n## First define median.fun(), with two required arguments:\n##         data specifies the data vector,\n##         indices selects vector elements for each resample\nmedian.fun &lt;- function(data, indices){median(data[indices])}\n## Call boot(), with statistic=median.fun, R = # of resamples\nset.seed(23)\n(wren.boot &lt;- boot(data = wren, statistic = median.fun, R = 4999))\n\n\n\nSubsection 1.8.4: Bootstrap estimates of confidence intervals\n\nBootstrap 95% confidence intervals for the median\n\n## Call the function boot.ci() , with boot.out=wren.boot\nboot.ci(boot.out=wren.boot, type=c(\"perc\",\"bca\"))\n\n\n\nThe correlation coefficient\n\n## Bootstrap estimate of 95% CI for `cor(chest, belly)`: `DAAG::possum`\ncorr.fun &lt;- function(data, indices) \n  with(data, cor(belly[indices], chest[indices]))\nset.seed(29)\ncorr.boot &lt;- boot(DAAG::possum, corr.fun, R=9999)\n\n\nlibrary(boot)\nboot.ci(boot.out = corr.boot, type = c(\"perc\", \"bca\"))\n\n\n\nThe bootstrap – parting comments\n\n\n\n\nSection 1.9: Organizing and managing work, and tools that can assist\n\nThe RStudio Integrated Development Environment\n\n\nSubsection 1.9.1: Reproducible reporting — the knitr package\n\n\n\nSection 1.10: The changing environment for data analysis\n\nSubsection 1.10.1: Models and machine learning\n\nThe limits of current machine learning systems\n\n\nTraps in big data analysis\n\n\nOf mice and machine learning — missing data values\n\n\nHumans are not good intuitive statisticians\n\n\n\nSubsection 1.10.2: Replicability is the definitive check\n\nTo what extent is published work replicable? What is the evidence?\n\n\nSome major replication studies\n\n\nReplicability in pre-clinical cancer research\n\n\nStudies where there may be strong social influences\n\n\nThe scientific study of scientific processes\n\n\nWould lowering the \\(p\\)-value threshold help?\n\n\nPeer review at the study planning stage\n\n\n\n\nSection 1.11: Further, or supplementary, reading\n\n\nExercises (1_12)\n1.4\n\nAnimals &lt;- MASS::Animals\nmanyMals &lt;- rbind(Animals, sqrt(Animals), Animals^0.1, log(Animals))\nmanyMals$transgp &lt;- rep(c(\"Untransformed\", \"Square root transform\",\n  \"Power transform, lambda=0.1\", \"log transform\"),\nrep(nrow(Animals),4))\nmanyMals$transgp &lt;- with(manyMals, factor(transgp, levels=unique(transgp)))\nlattice::xyplot(brain~body|transgp, data=manyMals,\n  scales=list(relation='free'), layout=c(2,2))\n\n1.5\n\nwith(Animals, c(cor(brain,body), cor(brain,body, method=\"spearman\")))\nwith(Animals, c(cor(log(brain),log(body)),\n  cor(log(brain),log(body), method=\"spearman\")))\n\n1.9\n\nusableDF &lt;- DAAG::cuckoohosts[c(1:6,8),]\nnr &lt;- nrow(usableDF)\nwith(usableDF, {\n  plot(c(clength, hlength), c(cbreadth, hbreadth), col=rep(1:2,c(nr,nr)))\n  for(i in 1:nr)lines(c(clength[i], hlength[i]), c(cbreadth[i], hbreadth[i]))\n  text(hlength, hbreadth, abbreviate(rownames(usableDF),8), pos=c(2,4,2,1,2,4,2))\n})\n\n1.10\n\n## Take a random sample of 100 values from the normal distribution\nx &lt;- rnorm(100, mean=3, sd=5)\n(xbar &lt;- mean(x))\n## Plot, against `xbar`, the sum of squared deviations from `xbar`\nlsfun &lt;- function(xbar) apply(outer(x, xbar, \"-\")^2, 2, sum)\ncurve(lsfun, from=xbar-0.01, to=xbar+0.01)\n\n\nboxplot(avs, meds, horizontal=T)\n\n1.15\n\nx &lt;- rpois(7, 78.3)\nmean(x); var(x)\n\n1.16\n\nnvals100 &lt;- rnorm(100)\nheavytail &lt;- rt(100, df = 4)\nveryheavytail &lt;- rt(100, df = 2)\nboxplot(nvals100, heavytail, veryheavytail, horizontal=TRUE)\n\n1.19\n\nboxdists &lt;- function(n=1000, times=10){\n  df &lt;- data.frame(normal=rnorm(n*times), t=rt(n*times, 7),\n  sampnum &lt;- rep(1:times, rep(n,times)))\n  lattice::bwplot(sampnum ~ normal+t, data=df, outer=TRUE, xlab=\"\", \n                  horizontal=T)\n}\n\n1.20\n\na &lt;- 1\nform &lt;- ~rchisq(1000,1)^a+rchisq(1000,25)^a+rchisq(1000,500)^a\nlattice::qqmath(form, scales=list(relation=\"free\"), outer=TRUE)\n\n1.21\n\ny &lt;- rnorm(51)\nydep &lt;- y1[-1] + y1[-51]\nacf(y)      # acf plots `autocorrelation function'(see Chapter 6)\nacf(ydep)\n\n1.24\n\nptFun &lt;- function(x,N)pt(sqrt(N)*mean(x)/sd(x), df=N-1, lower.tail=FALSE)\nsimStat &lt;- function(eff=.4, N=10, nrep=200, FUN)\n    array(rnorm(n=N*nrep*length(eff), mean=eff), dim=c(length(eff),nrep,N)) |&gt;\n      apply(2:1, FUN, N=N) \npval &lt;- simStat(eff=.4, N=10, nrep=200, FUN=ptFun)\n# Suggest a power transform that makes the distribution more symmetric\ncar::powerTransform(pval)   # See Subsection 2.5.6\nlabx &lt;- c(0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.25)\nbwplot(~I(pval^0.2), scales=list(x=list(at=labx^0.2, labels=paste(labx))),\n       xlab=expression(\"P-value (scale is \"*p^{0.2}*\")\") )\n\n1.24a\n\npvalDF &lt;- subset(df200, effsize==0.4 & N==10)$stat\nplot(sort(pval^0.2), sort(pvalDF^0.2))\nabline(0,1)\n\n1.24c\n\n## Estimated effect sizes: Set `FUN=effFun` in the call to `eff2stat()`\neffFun &lt;- function(x,N)mean(x)/sd(x)   \n  # Try: `labx &lt;- ((-1):6)/2`; `at = log(labx)`; `v = log(labx)  \n## NB also, Bayes Factors: Set `FUN=BFfun` in the call to `eff2stat()`\nBFfun &lt;- function(x,N)BayesFactor::ttest.tstat(sqrt(N)*mean(x)/sd(x),\n                                               n1=N, simple=T)\n  # A few very large Bayes Factors are likely to dominate the plots\n\n1.27\n\n(degC &lt;- setNames(c(21,30,38,46),paste('rep',1:4)) ) \n\n1.27a\n\nradonC &lt;- tidyr::pivot_longer(MPV::radon, names_to='key', \n                              cols=names(degC), values_to='percent')\nradonC$temp &lt;- degC[radonC$key]\nlattice::xyplot(percent ~ temp|factor(diameter), data = radonC)\n\n\nmatplot(scale(t(MPV::radon[,-1])), type=\"l\", ylab=\"scaled residuals\")  \n\n1.27d\n\nradon.res &lt;- aggregate(percent ~ diameter, data = radonC, FUN = scale, \n    scale = FALSE)\n\n1.30\n\ndiamonds &lt;- ggplot2::diamonds\nwith(diamonds, plot(carat, price, pch=16, cex=0.25))\nwith(diamonds, smoothScatter(carat, price))\n\n\nt2bfInterval &lt;- function(t, n=10, rscale=\"medium\", mu=c(-.1,.1)){\n     null0 &lt;- BayesFactor::ttest.tstat(t=t, n1=n, nullInterval=mu,\n                                       rscale=rscale,simple=TRUE)\nalt0 &lt;- BayesFactor::ttest.tstat(t=t, n1=n, nullInterval=mu, rscale=rscale, \n                                 complement=TRUE, simple=TRUE)\nalt0/null0\n}\n\n\npval &lt;- c(0.05,0.01,0.001); nval &lt;- c(10,40,160)\nbfDF &lt;- expand.grid(p=pval, n=nval)\npcol &lt;- 1; ncol &lt;- 2; tcol &lt;- 3\nbfDF[,'t'] &lt;- apply(bfDF,1,function(x){qt(x[pcol]/2, df=x[ncol]-1,                                  lower.tail=FALSE)})\nother &lt;- apply(bfDF,1,function(x)\n    c(BayesFactor::ttest.tstat(t=x[tcol], n1=x[ncol], rscale=\"medium\",\n                               simple=TRUE),\n      BayesFactor::ttest.tstat(t=x[tcol], n1=x[ncol], rscale=\"wide\",\n                               simple=TRUE),\n## Now specify a null interval\n    t2bfInterval(t=x[tcol], n=x[ncol], mu=c(-0.1,0.1),rscale=\"medium\"),\n    t2bfInterval(t=x[tcol], n=x[ncol], mu=c(-0.1,0.1),rscale=\"wide\")\n  ))\nbfDF &lt;- setNames(cbind(bfDF, t(other)),\n    c('p','n','t','bf','bfInterval'))\n\n\ndf &lt;- data.frame(d = with(datasets::sleep, extra[group==2] - extra[group==1]))\nlibrary(statsr)\nBayesFactor::ttestBF(df$d, rscale=1/sqrt(2))   # Or, `rscale=\"medium\"`\n  # `rscale=\"medium\"` is the default\nbayes_inference(d, type='ht', data=df, statistic='mean', method='t', rscale=1/sqrt(2),\n                alternative='twosided', null=0, prior_family = \"JZS\")\n  # Set `rscale=1/sqrt(2)` (default is 1.0) \n  # as for BayesFactor; gives same BF\n# Compare with `prior_family = \"JUI\"` (`\"JZS\"` is the default), \n# with (if not supplied) default settings\nbayes_inference(d, type='ht', data=df, statistic='mean', method='t',\n                alternative='twosided', null=0, prior_family = \"JUI\")\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/ch1.R\")\n}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 1: Learning from data</span>"
    ]
  },
  {
    "objectID": "ch2.html",
    "href": "ch2.html",
    "title": "2  Chapter 2: Generalizing from models",
    "section": "",
    "text": "Packages required (plus any dependencies)\nDAAG MASS qra investr HistData BHH2 xtable BayesFactor boot zoo boot MCMCpack,\nAdditionally, knitr and Hmisc are required in order to process the Rmd source file.\n\n\nSection 2.1 Model assumptions\n\nSubsection 2.1.1: Inferences are never assumption free\n\n\nSubsection 2.1.2: Has account been taken of all relevant effects?\n\n## Tabulate by Admit and Gender\nbyGender &lt;- 100*prop.table(margin.table(UCBAdmissions, margin=1:2), margin=2)\nround(byGender,1)\n\n\n## Admission rates, by department\npcAdmit &lt;- 100*prop.table(UCBAdmissions, margin=2:3)[\"Admitted\", , ]\nround(pcAdmit,1)\n\n\napplied &lt;- margin.table(UCBAdmissions, margin=2:3)\npcAdmit &lt;- 100*prop.table(UCBAdmissions, margin=2:3)[\"Admitted\", , ]\n      byGender &lt;- 100*prop.table(margin.table(UCBAdmissions,\n      margin=1:2), margin=2)\ndimnam &lt;- dimnames(UCBAdmissions)\nmfStats &lt;- data.frame(Admit=c(pcAdmit[1,],pcAdmit[2,]),\n  Applicants=c(applied[1,], applied[2,]),\n  mf=factor(rep(dimnam[['Gender']],c(6,6)),\n  levels=dimnam[['Gender']]), Department=rep(dimnam[[\"Dept\"]],2))\nxlim &lt;- c(0, max(mfStats$Admit)*1.025)\nylim &lt;- c(0, max(mfStats$Applicants)*1.075)\nplot(Applicants~Admit, data=mfStats, type=\"h\",lwd=2, xlim=xlim, ylim=ylim,\n     fg=\"gray\", cex.lab=1.2, col=rep(c(\"blue\",\"red\"),rep(6,2)),\n     xlab=\"UCB Admission rates (%), 1973\", ylab=\"Number of applicants\")\npcA &lt;- rbind(pcAdmit[1,], apply(pcAdmit,2, mean)+2, pcAdmit[2,], rep(NA,6))\npcA[2,3] &lt;- pcA[2,3]+1\nappA &lt;- rbind(applied[1,], apply(applied,2, mean)+80,\n              applied[2,], rep(NA,6))\ndeptNam &lt;- dimnam[[3]]\nfor(j in 1: ncol(appA)) lines(pcA[,j], appA[,j], col=\"gray\", lwd=0.8)\npoints(pcA[2,],appA[2,], pch=16, cex=1.1, col=\"white\")\ntext(pcA[2,],appA[2,],deptNam, cex=0.85)\n##\npar(xpd=TRUE)\ntext(byGender[1,1:2], rep(par()$usr[4],2)+0.5*strheight(\"^\"),\n     labels=c(\"^\",\"^\"), col=c(\"blue\",\"red\"),cex=1.2,srt=180)\ntext(byGender[1,], par()$usr[4]+1.4*strheight(\"A\"),\n     labels=paste(round(byGender[1,],1)),cex=0.85)\ntext(byGender[1,1:2]+c(-3.5,3.5), rep(par()$usr[4],2)+2.65*strheight(\"A\"),\n     labels=c(\"All males\",\"All females\"), pos=c(4,2), cex=1.2)\npar(xpd=FALSE)\nabline(h=200*(0:4),col=\"lightgray\",lty=\"dotted\")\nabline(v=20*(0:4),col=\"lightgray\",lty=\"dotted\")\nlegend(\"topleft\", col=c('blue','red'),lty=c(1,1), lwd=0.75, cex=0.9,\n       y.intersp=0.65, legend=c(\"Males\",\"Females\"),bty=\"n\")\n\n\n## Calculate totals, by department, of males & females applying\nmargin.table(UCBAdmissions, margin=2:3)\n\n\n\nSubsection 2.1.3: The limitations of models\n\n\nSubsection 2.1.4: Use the methodology that best suits the task in hand?\n\n\n\nSection 2.2: t-statistics, binomial proportions, and correlations\n\nSubsection 2.2.1: One- and two-sample t-tests\n\n\nSubsection 2.2.2: A two-sample comparison\n\nstats2 &lt;- sapply(DAAG::two65,\n                 function(x) c(av=mean(x), sd=sd(x), n=length(x)))\npooledsd &lt;- sqrt( sum(stats2['n',]*stats2['sd',]^2)/sum(stats2['n',]-1) )\nstats2 &lt;- setNames(c(as.vector(stats2), pooledsd),\n                   c('av1','sd1','n1','av2','sd2','n2','pooledsd'))\nprint(stats2, digits=4)\n\n\nwith(DAAG::two65, t.test(heated, ambient, var.equal=TRUE))\n\n\nWhen is pairing helpful?\n\ntitl &lt;- paste(\"Second versus first member, for each pair.  The first\",\n\"\\npanel is for the elastic band data. The second (from\",\n\"\\nDarwin) is for plants of the species Reseda lutea\")\noldpar &lt;- par(pty=\"s\")\non.exit(par(oldpar))\nDAAG::onesamp(dset = DAAG::pair65, x = \"ambient\", y = \"heated\",\n  xlab = \"Amount of stretch (ambient)\",\n  ylab = \"Amount of stretch (heated)\", fg='gray')\n## Data set mignonette holds the Darwin (1877) data on Reseda lutea.\n## Data were in 5 pots, holding 5,5,5,5,4 pairs of plants respectively.\nDAAG::onesamp(dset = DAAG::mignonette, x = \"self\", y = \"cross\",\n  xlab = \"Height of self-fertilised plant\", ylab =\n  \"Height of cross-fertilised plant\", dubious = 0, cex=0.7, fg='gray')\n\n\n\nWhat if the standard deviations are unequal?\n\n\n\nSubsection 2.2.3: The normal approximation to the binomial\n\n\nSubsection 2.2.4: The Pearson or product–moment correlation\n\n## Pearson correlation between `body` and `brain`: Animals\nAnimals &lt;- MASS::Animals\nrho &lt;- with(Animals, cor(body, brain))\n## Pearson correlation, after log transformation\nrhoLogged &lt;- with(log(Animals), cor(body, brain))\n## Spearman rank correlation\nrhoSpearman &lt;- with(Animals, cor(body, brain, method=\"spearman\"))\nc(Pearson=round(rho,2), \" Pearson:log values\"=round(rhoLogged,2),\n  Spearman=round(rhoSpearman,2))\n\n\n\n\nSection 2.3 Extra-binomial and extra-Poisson variation\n\nmaleDF &lt;- data.frame(number=0:12, freq=unname(qra::malesINfirst12[[\"freq\"]]))\nN &lt;- sum(maleDF$freq)\npihat &lt;- with(maleDF, weighted.mean(number, freq))/12\nprobBin &lt;- dbinom(0:12, size=12, prob=pihat)\nrbind(Frequency=setNames(maleDF$freq, nm=0:12),\n      binomialFit=setNames(probBin*N, nm=0:12),\n      rawResiduals = maleDF$freq-probBin*N,\n      SDbinomial=sqrt(probBin*(1-probBin)*N)) |&gt;\n  formatC(digits=2, format=\"fg\") |&gt; print(digits=2, quote=F, right=T)\n\n\nset.seed(29)\nrqres.plot(doBI, plot.type='all', type=\"QQ\", main=\"\"); box(col='white')\nmtext(side=3, line=0.5, \"A: Binomial model, Q-Q\", adj=0, cex=1.25)\nrqres.plot(doBI, plot.type='all', type=\"wp\", main=\"\"); box(col='white')\n## Plots C, D, E, F: Set object name; set`type=\"wp\" (C, E, F), or`\"QQ\"` (D)\nmtext(side=3, line=0.5, \"B: Binomial, worm plot 1\", adj=-0.05, cex=1.25)\nrqres.plot(doBI, plot.type='all', type=\"wp\", main=\"\"); box(col='white')\nmtext(side=3, line=0.5, \"C: Binomial, worm plot 2\", adj=-0.05, cex=1.25)\nrqres.plot(doBB, plot.type='all', type=\"QQ\", main=\"\", ylab=''); box(col='white')\nmtext(side=3, line=0.5, \"D: BB model, Q-Q\", adj=0, cex=1.25)\nrqres.plot(doBB, plot.type='all', type=\"wp\", main=\"\", ylab=''); box(col='white')\nmtext(side=3, line=0.5, \"E: BB, worm plot 1\", adj=0, cex=1.25)\nrqres.plot(doBB, plot.type='all', type=\"wp\", main=\"\", ylab=''); box(col='white')\nmtext(side=3, line=0.5, \"F: BB, worm plot 2\", adj=0, cex=1.25)\n\n\naicStat &lt;- AIC(doBI, doBB)\nrownames(aicStat) &lt;-\n  c(doBI=\"Binomial\", doBB=\"Betabinomial\")[rownames(aicStat)]\naicStat$dAIC &lt;- with(aicStat, round(AIC-AIC[1],1))\naicStat\n\n\n## Numbers of accidents in three months, with Poisson fit\nmachinists &lt;- data.frame(number=0:8, freq=c(296, 74, 26, 8, 4, 4, 1, 0, 1))\nN &lt;- sum(machinists[['freq']])\nlambda &lt;- with(machinists, weighted.mean(number, freq))\nfitPoisson &lt;- dpois(0:8, lambda)*sum(machinists[['freq']])\nrbind(Frequency=with(machinists, setNames(freq, number)),\n      poissonFit=fitPoisson) |&gt;\n  formatC(digits=2, format=\"fg\") |&gt; print(quote=F, digits=2, right=T)\n\n\nset.seed(23)\nrqres.plot(doPO, plot.type='all', type=\"QQ\", main=\"\"); box(col='white')\n## Repeat, changing the argument, for remaining plots\nmtext(side=3, line=0.5, \"A: Poisson, Q-Q plot\", adj=0, cex=1.25)\nrqres.plot(doPO, plot.type='all', type=\"wp\", main=\"\", ylab=''); box(col='white')\nmtext(side=3, line=0.5, \"B: Poisson, worm plot\", adj=0, cex=1.25)\nrqres.plot(doNBI, plot.type='all', type=\"wp\", main=\"\", ylab='')\nmtext(side=3, line=0.5, \"C: NBI, worm plot\", adj=0, cex=1.25); box(col='white')\n\n\n\nSubsection 2.3.2: *Technical details – extra-binomial or extra-Poisson variation\n\nsigma &lt;- exp(coef(doBB, \"sigma\"))\ncat(\"Phi =\", (1+12*sigma)/(1+sigma))\n\n\nmu &lt;- exp(coef(doNBI, \"mu\"))\nsigma &lt;- exp(coef(doNBI, \"sigma\"))\ncat(\"Phi =\", (1+sigma*mu))\n\n\n\nSection 2.4 Contingency tables\n\n## 'Untreated' rows (no training) from psid3, 'treated' rows from nswdemo\nnswpsid3 &lt;- rbind(DAAG::psid3, subset(DAAG::nswdemo, trt==1))\ndegTAB &lt;- with(nswpsid3, table(trt,nodeg))\n# Code 'Yes' if completed high school; 'No' if dropout\ndimnames(degTAB) &lt;- list(trt=c(\"PSID3_males\",\"NSW_male_trainees\"),\n                         deg =c(\"Yes\",\"No\"))\ndegTAB\n\n\n# To agree with hand calculation below, specify correct=FALSE\nchisq.test(degTAB, correct=FALSE)\n\n\nThe mechanics of the chi-squared test\n\n\nAn example where a chi-squared test may not be valid\n\n## Engine man data\nengineman &lt;- matrix(c(5,3,17,85), 2,2)\nchisq.test(engineman)\n\n\n\nRare and endangered plant species\n\nfisher.test(engineman)\n\n\n## Enter the data thus:\nrareplants &lt;- matrix(c(37,190,94,23, 59,23,10,141, 28,15,58,16), ncol=3,\n  byrow=TRUE, dimnames=list(c(\"CC\",\"CR\",\"RC\",\"RR\"), c(\"D\",\"W\",\"WD\")))\n\n\n(x2 &lt;- chisq.test(rareplants))\n\n\n\nExamination of departures from a consistent overall row pattern\n\n## Expected values\nx2$expected\n\n\noptions(digits=2)\n## Standardized residuals\nresiduals(x2)\n\n\n\nInterpretation issues\n\n\n\nSection 2.5 Issues for Regression with a single explanatory variable\n\nSubsection 2.5.1: Iron slag example — check residuals with care!\n\nleg &lt;- c(\"A: Fitted line\", \"B: Residuals from line\", \"C: Variance check\")\nord &lt;- order(DAAG::ironslag[[\"magnetic\"]])\nironslag &lt;- DAAG::ironslag[ord,]\nslagAlpha.lm &lt;- lm(chemical~magnetic, data=ironslag)\nresval &lt;- residuals(slagAlpha.lm)\nfitchem &lt;- fitted(slagAlpha.lm)\nsqrtabs2 &lt;- sqrt(abs(resval))\nplot(chemical~magnetic, xlab = \"Magnetic\", ylab = \"Chemical\",\n     pch = 1, data=ironslag, fg=\"gray\")\nlines(fitchem~ironslag[[\"magnetic\"]])\nmtext(side = 3, line = 0.25, leg[1], adj=-0.1, cex=0.925)\nscatter.smooth(resval~ironslag[[\"magnetic\"]], lpars=list(col=\"red\"), span=0.8,\n               xlab = \"Magnetic\", ylab = \"Residual\", fg=\"gray\")\nmtext(side = 3, line = 0.25, leg[2], adj = -0.1, cex=0.925)\nscatter.smooth(sqrtabs2 ~ fitchem, lpars=list(col=\"red\"), span=0.8,\nxlab = \"Predicted chemical\", fg=\"gray\",\nylab = expression(sqrt(abs(residual))))\nmtext(side = 3, line = 0.25, leg[3], adj = -0.1, cex=0.8)\n## Diagnostics from fit using loess()\nleg2 &lt;- c(\"D: Smooth, using loess()\",\n          \"E: Residuals from smooth\",\n          \"F: Variance check\")\nslag.loess &lt;- loess(chemical~magnetic, data=ironslag, span=0.8)\nresval2 &lt;- slag.loess[[\"residuals\"]]\nfitchem2 &lt;- slag.loess[[\"fitted\"]]\nsqrtabs2 &lt;- sqrt(abs(resval2))\nplot(chemical~magnetic, xlab = \"Magnetic\", ylab = \"Chemical\",\npch = 1, data=ironslag, fg=\"gray\")\nlines(fitchem2 ~ ironslag[[\"magnetic\"]], col=\"red\")\nmtext(side = 3, line = 0.25, leg2[1], adj=-0.1, cex=0.925)\nscatter.smooth(resval2~ironslag[[\"magnetic\"]], span=0.8,\nlpars=list(col=\"red\"),\nxlab = \"Magnetic\", ylab = \"Residual\", fg=\"gray\")\nmtext(side = 3, line = 0.25, leg2[2], adj = -0.1, cex=0.925)\nscatter.smooth(sqrtabs2 ~ fitchem2, lpars=list(col=\"red\"),\nspan=0.8, xlab = \"Predicted chemical\", fg=\"gray\",\nylab = expression(sqrt(abs(residual))))\nmtext(side = 3, line = 0.25, leg2[3], adj = -0.1, cex=0.925)\n\n\n\nSubsection 2.5.2: The analysis of variance table\n\nroller.lm &lt;- lm(depression ~ weight, data=DAAG::roller)\nanova(roller.lm)\n\n\n\nSubsection 2.5.3: Outliers, influence, and robust regression\n\nsoftbacks &lt;- DAAG::softbacks\nx &lt;- softbacks[,\"volume\"]\ny &lt;- softbacks[,\"weight\"]\nu &lt;- lm(y ~ x)\nyhat &lt;- predict(u)\nres &lt;- resid(u)\nr &lt;- with(softbacks, cor(x, y))\nxlim &lt;- with(softbacks, range(volume))\nxlim[2] &lt;- xlim[2]+diff(xlim)*0.08\nplot(y ~ x, xlab = \"Volume (cc)\", xlim=xlim,\ndata=softbacks, ylab = \"Weight (g)\", pch = 4,\nylim = range(c(y, yhat)), cex.lab=0.9, fg=\"gray\")\nabline(u$coef[1], u$coef[2], lty = 1)\nbottomright &lt;- par()$usr[c(2, 3)]\nchw &lt;- par()$cxy[1]\nchh &lt;- par()$cxy[2]\nz &lt;- summary(u)$coef\nbtxt &lt;- c(paste(\"a =\", format(round(z[1, 1], 1)),\n\"  SE =\", format(round(z[1, 2], 1))),\npaste(\"b =\", format(round(z[2, 1], 2)),\n\"  SE =\", format(round(z[2, 2], 2))))\nlegend(bottomright[1],  bottomright[2],\nlegend=btxt, xjust=1, yjust=0, cex=0.8, bty=\"n\")\n\n\nsoftbacks.lm &lt;- lm(weight ~ volume, data=DAAG::softbacks)\nprint(coef(summary(softbacks.lm)), digits=3)\n\n\nplot(softbacks.lm, fg=\"gray\",\ncaption = c(\"A: Residuals vs Fitted\", \"B: Normal Q-Q\",\n\"C: Scale-Location\", \"\", \"D: Resids vs Leverage\"))\n\n\nRobust regression\n\n\n\nSubsection 2.5.4: Standard errors and confidence intervals\n\nConfidence intervals and tests for the slope\n\nSEb &lt;- coef(summary(roller.lm))[2, 2]\ncoef(roller.lm)[2] + qt(c(0.025,.975), 8)*SEb\n\n\n\nSEs and confidence intervals for predicted values\n\n## Code to obtain fitted values and standard errors (SE, then SE.OBS)\nfit.with.se &lt;- predict(roller.lm, se.fit=TRUE)\nfit.with.se$se.fit                                            # SE\nsqrt(fit.with.se[[\"se.fit\"]]^2+fit.with.se$residual.scale^2)  # SE.OBS\n\n\npredict(roller.lm, interval=\"confidence\", level=0.95)\npredict(roller.lm, interval=\"prediction\", level=0.95)  # CI for a new observation\n\n\n## Depression vs weight, with 95\\% pointwise bounds for both\n## the fitted line and predicted values\ninvestr::plotFit(roller.lm, interval=\"both\", col.conf=\"red\", fg=\"gray\")\nmtext(side=3,line=0.75, \"A: Lawn roller data\", cex=1.2, adj=-0.25)\n## Male child vs father height, Galton's data\ngaltonMales &lt;- subset(HistData::GaltonFamilies, gender==\"male\")\ngalton.lm &lt;- lm(childHeight~father, data=galtonMales)\ninvestr::plotFit(galton.lm, interval=\"both\", col.conf=\"red\", hide=FALSE,\n                 col=adjustcolor('black',alpha=0.5), fg=\"gray\")\nmtext(side=3,line=0.75, \"B: Son vs father heights\", cex=1.2, adj=-0.25)\n\n\n\nImplications for design\n\npanelci&lt;-function(data,...)\n{\nnrows&lt;-list(...)$nrows\nncols&lt;-list(...)$ncols\nif(ncols==1)axis(2, lwd=0, lwd.ticks=1)\nif(ncols==1)axis(1, lwd=0, lwd.ticks=1) else\naxis(3, lwd=0, lwd.ticks=1)\nx&lt;-data$stretch; y&lt;-data$distance\nu &lt;- lm(y ~ x)\nupred &lt;- predict(u, interval=\"confidence\")\nci &lt;- data.frame(fit=upred[,\"fit\"],lower=upred[,\"lwr\"], upper=upred[,\"upr\"])\nord&lt;-order(x)\nlines(x[ord], ci[[\"fit\"]][ord], lty=1, lwd=2)\nlines(lowess(x[ord], ci[[\"upper\"]][ord]), lty=2, lwd=2, col=\"grey\")\nlines(lowess(x[ord], ci[[\"lower\"]][ord]), lty=2, lwd=2, col=\"grey\")\n}\nelastic1 &lt;- DAAG::elastic1\nelastic2 &lt;- DAAG::elastic2\nxy&lt;-rbind(elastic2,elastic1)\nnam &lt;- c(\"Range of stretch 30-65 mm\",\"Range of stretch 42-54 mm\")\ntrial&lt;-rep(nam, c(dim(elastic2)[1],dim(elastic1)[1]))\nxlim&lt;-range(elastic2$stretch)\nylim&lt;-range(elastic2$distance)\nxy&lt;-split(xy,trial)\nxy&lt;-lapply(1:length(xy),function(i){c(as.list(xy[[i]]), list(xlim=xlim,\nylim=ylim))})\nnames(xy) &lt;- nam\nDAAG::panelplot(xy,panel=panelci,totrows=1,totcols=2,\n                par.strip.text=list(cex=.9), oma=c(4,4,2.5,2), fg='gray')\nmtext(side = 2, line = 3.35, \"Distance moved (cm)\", cex=1.1, las=0)\nmtext(side=1,line=3,\"Amount of stretch (mm)\", cex=1.1)\n\n\n\n\nSubsection 2.5.5: There are two regression lines!\n\n## There are two regression lines!\npair65 &lt;- DAAG::pair65\nbothregs &lt;- function(x=pair65[, \"ambient\"], y=pair65[, \"heated\"],\n  xlab=\"Stretch (band at ambient)\", ylab = \"Stretch (heated band)\", pch=16){\n    plot(y ~ x, xlab = xlab, ylab = ylab, pch = pch, fg=\"gray\")\n    topleft &lt;- par()$usr[c(1, 4)] + c(0.5, -0.5) * par()$cxy\n    text(topleft[1], topleft[2], paste(\"r =\", round(cor(x, y), 2)), adj = 0)\n    u1 &lt;- lm(y ~ x)\n    abline(u1$coef[1], u1$coef[2])\n    u2 &lt;- lm(x ~ y)\n    abline( - coef(u2)[1]/coef(u2)[2], 1/coef(u2)[2], lty = 2)\n}\nbothregs()\nmtext(side = 3, line = 0.5, \"A\", adj = 0)\nbothregs(x=trees[, \"Girth\"], y=trees[, \"Height\"],\n         xlab=\"Girth (in)\", ylab &lt;- \"Height (ft)\", pch=16)\nmtext(side = 3, line = 0.5, \"B\", adj = 0)\n\n\nAn alternative to a regression line\n\n\n\nSubsection 2.5.6: Logarithmic and Power Transformations\n\n## Logarithmic and Power Transformations\nDAAG::powerplot(expr=\"sqrt(x)\", xlab=\"\")\nDAAG::powerplot(expr=\"x^0.25\", xlab=\"\", ylab=\"\")\nDAAG::powerplot(expr=\"log(x)\", xlab=\"\", ylab=\"\")\nDAAG::powerplot(expr=\"x^2\")\nDAAG::powerplot(expr=\"x^4\", ylab=\"\")\nDAAG::powerplot(expr=\"exp(x)\", ylab=\"\")\n\n\nGeneral power transformations — Box-Cox and Yeo-Johnson\n\n\n\nSubsection 2.5.7: General forms of nonlinear response\n\n\nSubsection 2.5.8: Size and shape data – allometric growth\n\n## Heart weight versus body weight, for 30 Cape fur seals.\ng2.12 &lt;- function()\n{\ncfseal &lt;- DAAG::cfseal\nx &lt;- log(cfseal[,\"weight\"])\ny &lt;- log(cfseal[, \"heart\"])\nylim &lt;- log(c(82.5,1100))\nxlim &lt;- log(c(17,180))\nylab &lt;- \"Heart weight (g, log scale)\"\nxlab &lt;- \"Body weight (kg, log scale)\"\nxtik &lt;- c(20,40,80,160)\nytik &lt;- c(100,200,400,800)\nplot(x, y, xlab = xlab, ylab = ylab, axes = F, xlim =\nxlim, ylim = ylim, pch = 16, cex=0.85, fg=\"gray\", cex.lab=1.1)\naxis(1, at = log(xtik), labels = paste(xtik), lwd=0, lwd.ticks=1)\naxis(2, at = log(ytik), labels = paste(ytik), lwd=0, lwd.ticks=1)\nbox(col=\"gray\")\nform1 &lt;- formula(y ~ x)\nu &lt;- lm(form1, data = cfseal)\nabline(u$coef[1], u$coef[2])\nusum &lt;- summary(u)$coef\noptions(digits=3)\nprint(usum)\ncwh &lt;- par()$cxy\neqn &lt;- paste(\"log y =\", round(usum[1, 1], 2), \" [\",\nround(usum[1, 2], 2), \"] +\", round(usum[2, 1], 3),\n\" [\", round(usum[2, 2], 3), \"] log x\")\nmtext(side=3, line=1.15, eqn, adj = 0.4, cex = 0.8)\nmtext(side=3, line=0.25, \"(Values in square brackets are SEs)\", adj = 0.4, cex = 0.8)\n}\ng2.12()\n\n\nThe allometric growth equation\n\noptions(scipen=4)\ncfseal.lm &lt;- lm(log(heart) ~ log(weight), data=DAAG::cfseal)\nprint(coef(summary(cfseal.lm)), digits=4)\n\n\n\n\n\nSection 2.6 Empirical assessment of predictive accuracy\n\nSubsection 2.6.1: The training/test approach, and cross-validation\n\nCross-validation – a tutorial example\n\nhouseprices &lt;- DAAG::houseprices\ndf &lt;- DAAG::CVlm(houseprices, form.lm = formula(sale.price ~ area),m=3,printit=F,plotit=FALSE)\npanelfun &lt;- function(x,y,subscripts,groups, ...){\n  lattice::panel.superpose(x,y,subscripts,groups, ...)\n  lattice::panel.superpose(x,df[[\"cvpred\"]],subscripts,groups,type=\"b\", cex=0.5, ...)\n}\ngph &lt;- lattice::xyplot(sale.price ~ area, groups=fold, data=df, pch=1:3, panel=panelfun)\nparset &lt;- DAAG::DAAGtheme(color=T, lty=1:3, pch=1:3, lwd=2)\nkeylist &lt;- list(lines=TRUE, columns=3, between.columns=1.5, between=1, cex=0.85)\nupdate(gph, par.settings=parset, auto.key=keylist)\n\n\nset.seed(29)        # Generate results shown\nrand &lt;- sample(rep(1:3, length=15))\n## sample() randomly permutes the vector of values 1:3\nfor(i in 1:3) cat(paste0(i,\":\"), (1:15)[rand == i],\"\\n\")\n\n\nhouseprices &lt;- DAAG::houseprices\nrow.names(houseprices) &lt;- (1:nrow(houseprices))\nDAAG::CVlm(houseprices, form.lm = formula(sale.price ~ area), plotit=FALSE)\n\n\n## Estimate of sigma^2 from regression output\nhouseprices &lt;- DAAG::houseprices\nhouseprices.lm &lt;- lm(sale.price ~ area, houseprices)\nsummary(houseprices.lm)[[\"sigma\"]]^2\n\n\n\n\nSubsection 2.6.2: Bootstrapping in regression\n\nhouseprices &lt;- DAAG::houseprices\nhouseprices.lm &lt;- lm(sale.price ~ area, houseprices)\nprint(coef(summary(houseprices.lm)),digits=2)\n\n\nhouseprices.fn &lt;-\n  function (houseprices, index,\n            statfun=function(obj)coef(obj)[2]){\n            house.resample &lt;- houseprices[index, ]\n            house.lm &lt;- lm(sale.price ~ area, data=house.resample)\n            statfun(house.lm)    # slope estimate for resampled data\n            }\n\n\nset.seed(1028)     # use to replicate the exact results below\nlibrary(boot)      # ensure that the boot package is loaded\n## requires the data frame houseprices (DAAG)\n(houseprices.boot &lt;- boot(houseprices, R=999, statistic=houseprices.fn))\n\n\nstatfun1200 &lt;- function(obj)predict(obj, newdata=data.frame(area=1200))\nprice1200.boot &lt;- boot(houseprices, R=999, statistic=houseprices.fn,\nstatfun=statfun1200)\nboot.ci(price1200.boot, type=\"perc\") # \"basic\" is an alternative to \"perc\"\n\n\nset.seed(1111)\nlibrary(boot)\npar(las=0)\nhouseprices2.fn&lt;-function (houseprices,index){\nhouse.resample&lt;-houseprices[index,]\nhouse.lm&lt;-lm(sale.price~area,data=house.resample)\nhouseprices$sale.price-predict(house.lm,houseprices)\n# resampled prediction errors\n}\nhouseprices &lt;- DAAG::houseprices\nn&lt;-nrow(houseprices)\nR &lt;- 199    ## Will obtain 199 estimates of prediction error\nhouseprices.lm&lt;-lm(sale.price~area,data=houseprices)\nhouseprices2.boot&lt;-boot(houseprices, R=R, statistic=houseprices2.fn)\nhouse.fac&lt;-factor(rep(1:n,rep(R,n)))\nplot(house.fac,as.vector(houseprices2.boot$t),\n     ylab=\"\", xlab=\"House\", fg=\"gray\")\nmtext(side=2, line=2, \"Prediction Errors\")\nmtext(side = 3, line = 0.5, \"A\", adj = 0)\nboot.se &lt;- apply(houseprices2.boot$t,2,sd)\nmodel.se &lt;- predict.lm(houseprices.lm,se.fit=T)$se.fit\nplot(boot.se/model.se, ylab=\"\", xlab=\"House\",pch=16, fg=\"gray\")\nmtext(side=2, line=2.0, \"Ratio of SEs\\nBootstrap to Model-Based\", cex=0.9)\nmtext(side = 3, line = 0.5, \"B\", adj = 0)\nabline(1,0)\n\n\nCommentary\n\n\n\n\nSection 2.7 One- and two-way comparisons\n\nSubsection 2.7.1: One-way comparisons\n\ntomato &lt;- data.frame(Weight = c(1.5, 1.9, 1.3, 1.5, 2.4, 1.5,   # Water\n                                1.5, 1.2, 1.2, 2.1, 2.9, 1.6,   # Nutrient\n                                1.9, 1.6, 0.8, 1.15, 0.9, 1.6), # Nutrient+24D\n  trt = factor(rep(c(\"Water\", \"Nutrient\", \"Nutrient+24D\"), c(6, 6, 6))))\n## Make `Water` the first level of trt.  In aov or lm calculations, it is\n## then taken as the baseline or reference level.\ntomato$trt &lt;- relevel(tomato$trt, ref=\"Water\")\n\n\n## A: Weights of tomato plants (g)\nlibrary(lattice, quietly=TRUE)\ngph &lt;- stripplot(trt~Weight, aspect=0.35, scale=list(tck=0.6), data=tomato)\nupdate(gph, scales=list(tck=0.4), cex=0.9, col=\"black\", xlab=\"\",\n       main=list('A: Weights of tomato plants (g)', y=0, cex=1.1))\n\n\n## B: Summarize comparison between LSD and Tukey's HSD graphically\ntomato.aov &lt;- aov(Weight ~ trt, data=tomato)\nDAAG::onewayPlot(obj=tomato.aov)\ntitle(main=\"B: LSD, compared with Tukey HSD\", adj=0.1, outer=T,\n      line=-1.0, font.main=1, cex.main=1.25)\n\n\nBHH2::anovaPlot(tomato.aov)\n\n\nThe analysis of variance table\n\n## Do analysis of variance calculations\nanova(tomato.aov)\n\n\n\nOther multiple comparison tests\n\n\n\nSubsection 2.7.2: Regression versus qualitative comparisons – issues of power\n\ngph &lt;- DAAG::simulateLinear(alpha=0.6, seed=17, aspect='iso')\nupdate(gph, par.settings=DAAG::DAAGtheme(color=FALSE, alpha=0.4))\n\n\n\nSubsection 2.7.3: *Severe multiplicity — the false discovery rate\n\n*Microarrays and alternatives — technical note\n\n\nThe false discovery rate (FDR)\n\ncoralPval &lt;- DAAG::coralPval\npcrit &lt;- c(0.05, 0.02, 0.01, 0.001)\nunder &lt;- sapply(pcrit, function(x)sum(coralPval&lt;=x))\n\n\nexpected &lt;- pcrit*length(coralPval)\n\n\nfdrtab &lt;- data.frame(Threshold=pcrit, Expected=expected,\nDiscoveries=under, FDR=round(expected/under, 4))\nprint(xtable::xtable(fdrtab), include.rownames=FALSE, hline.after=FALSE)\n\n\nfdr &lt;- p.adjust(coralPval, method=\"BH\")\n\n\nfdrcrit &lt;- c(0.05, 0.04, 0.02, 0.01)\nunder &lt;- sapply(fdrcrit, function(x)sum(coralPval&lt;=x))\nsetNames(under, paste(fdrcrit))\n\n\n\n\nSubsection 2.7.4: Data with a two-way structure, i.e., two factors\n\npar(fig=c(0.525,1,0,1), mgp=c(1.5,0.4,0))\nlev &lt;- c(\"F10\", \"NH4Cl\", \"NH4NO3\", \"F10 +ANU843\",\n         \"NH4Cl +ANU843\", \"NH4NO3 +ANU843\")\nrice &lt;- within(DAAG::rice, trt &lt;- factor(trt, levels=lev))\nwith(rice, interaction.plot(fert, variety, ShootDryMass, fg=\"gray\",\n     legend = FALSE, xlab=\"Fertiliser\", cex.lab=0.95, mex=0.65))\nxleg &lt;- par()$usr[2]\nyleg &lt;- par()$usr[4] - 0.72 * diff(par()$usr[3:4])\nleginfo &lt;- legend(xleg, yleg, bty = \"n\", legend = levels(rice$variety),\n                  col = 1, lty = 2:1, lwd=1, xjust = 1, cex = 0.8,\n                  y.intersp=0.8)$rect\ntext(leginfo$left + 0.5 * leginfo$w, leginfo$top, \"  variety\",\n      adj = 0.5, cex = 0.8)\nmtext(side=3, line=0.65, cex=0.9, adj=-0.15, \"B\")\ngph &lt;- dotplot(trt ~ ShootDryMass, pch=1, cex=0.9, las=2,\n               xlab=\"Shoot dry mass (g)\", data=rice,\n               panel=function(x,y,...){panel.dotplot(x,y,...)\n                 av &lt;- sapply(split(x,y),mean);\n                 ypos &lt;- unique(y)\n                 lpoints(ypos~av, pch=3, col=\"gray40\", cex=1.25)},\n               main=list(\"A\", cex=0.88, just=\"left\", x=0.1, y=-0.7, font=1))\npars &lt;-  DAAG::DAAGtheme(fontsize=list(text=9, points=6), color=FALSE)\nprint(update(gph, scales=list(tck=0.5), par.settings=pars, aspect=0.9),\n      position=c(-0.065,0.0,0.6,1), newpage=FALSE)\n\n\n\nSubsection 2.7.5: Presentation issues\n\n\n\nSection 2.8 Data with a nested variation structure\n\nSubsection 2.8.1: Degrees of freedom considerations\n\n\nSubsection 2.8.2: General multi-way analysis of variance designs\n\n\n\nSection 2.9 Bayesian estimation – further commentary and approaches\n\nSubsection 2.9.1: Bayesian estimation with normal priors and likelihood\n\n\nSubsection 2.9.2: Further comments on Bayes Factors\n\nThe Sellke calibration upper limit\n\n\nA note on the Bayesian Information Criterion\n\npval &lt;- c(.05,.01,.001); np &lt;- length(pval)\nNval &lt;- c(4,6,10,20,40,80,160); nlen &lt;- length(Nval)\n## Difference in BIC statistics, interpreted as Bayes factor\nt2BFbic &lt;- function(p,N){t &lt;- qt(p/2, df=N-1, lower.tail=FALSE)\n                         exp((N*log(1+t^2/(N-1))-log(N))/2)}\nbicVal &lt;- outer(pval, Nval, t2BFbic)\n## Bayes factor, calculated using BayesFactor::ttest.tstat()\nt2BF &lt;- function(p, N){t &lt;- qt(p/2, df=N-1, lower.tail=FALSE)\n          BayesFactor::ttest.tstat(t=t, n1=N, simple=TRUE, rscale = \"medium\")}\nBFval &lt;- matrix(nrow=np, ncol=nlen)\nfor(i in 1:np)for(j in 1:nlen) BFval[i,j] &lt;- t2BF(pval[i], Nval[j])\ncfVal &lt;- rbind(BFval, bicVal)[c(1,4,2,5,3,6),]\ndimnames(cfVal) &lt;- list(\n  paste(rep(pval,rep(2,np)), rep(c(\"- from ttest.tstat\", \"- from BIC\"),np)),\n        paste0(c(\"n=\",rep(\"\",nlen-1)),Nval))\nround(cfVal,1)\n\n\n\n\nSubsection 2.9.3: Bayesian regression estimation using the MCMCpack package\n\nsuppressPackageStartupMessages(library(MCMCpack))\nroller.mcmc &lt;- MCMCregress(depression ~ weight, data=DAAG::roller)\nsummary(roller.mcmc)\n\n\nmat &lt;- matrix(c(1:6), byrow=TRUE, ncol=2)\nlayout(mat, widths=rep(c(2,1.1),3), heights=rep(0.9,8))\n  # NB: widths & heights are relative\nplot(roller.mcmc, auto.layout=FALSE, ask=FALSE, col=\"gray\", fg=\"gray\")\n\n\n\n\nSection 2.10: Recap\n\nRegress \\(y\\) on \\(x\\), or \\(x\\) on~\\(y\\)?\n\n\n\nSection 2.11: Further reading\n\n\nExercises (2.12)\n2.2\n\n## UCBAdmissions is in the datasets package\n## For each combination of margins 1 and 2, calculate the sum\nUCBtotal &lt;- apply(UCBAdmissions, c(1,2), sum)\n\n2.2b\n\napply(UCBAdmissions, 3, function(x)(x[1,1]*x[2,2])/(x[1,2]*x[2,1]))\n\n2.3\n\ntabA &lt;- array(c(30,30,10,10,15,5,30,10), dim=c(2,2,2))\ntabB &lt;- array(c(30,30,20,10,10,5,20,25), dim=c(2,2,2))\n\n2.5\n\nz.transform &lt;- function(r) .5*log((1+r)/(1-r))\nz.inverse &lt;- function(z) (exp(2*z)-1)/(exp(2*z)+1)\n  possum.fun &lt;- function(data, indices) {\n    chest &lt;- data$chest[indices]\n    belly &lt;- data$belly[indices]\n    z.transform(cor(belly, chest))}\npossum.boot &lt;- boot::boot(DAAG::possum, possum.fun, R=999)\nz.inverse(boot.ci(possum.boot, type=\"perc\")$percent[4:5])\n # The 4th and 5th elements of the percent list element\n # hold the interval endpoints. See ?boot.ci\n\n2.11\n\nwith(pressure, MASS::boxcox(pressure ~ I(1/(temperature+273))))\n\n2.14\n\n\"funRel\" &lt;-\nfunction(x=leafshape$logpet, y=leafshape$loglen, scale=c(1,1)){\n  ## Find principal components rotation; see Subsection 9.1.2\n  ## Here (unlike 9.1.2) the interest is in the final component\n  xy.prc &lt;- prcomp(cbind(x,y), scale=scale)\n  b &lt;- xy.prc$rotation[,2]/scale\n  c(bxy = -b[1]/b[2])     # slope of functional equation line\n}\n## Try the following:\nleafshape &lt;- DAAG::leafshape\nfunRel(scale=c(1,1))    # Take x and y errors as equally important\n  # Note that all lines pass through (mean(x), mean(y))\n\n2.15\n\nP &lt;- rbind(\n    c(1 , 0 , 0 , 0 , 0 , 0),\n    c(.5, 0 , .5, 0 , 0 , 0),\n    c(0 , .5, 0 , .5, 0 , 0),\n    c(0 , 0 , .5, 0 , .5, 0),\n    c(0 , 0 , 0 , .5, 0 , .5),\n    c(0 , 0 , 0 , 0 , 0 , 1))\ndimnames(P) &lt;- list(0:5,0:5)\nP\n\n\nMarkov &lt;- function(N=15, initial.value=1, transition=P, stopval=NULL)\n  {X &lt;- numeric(N)\n   X[1] &lt;- initial.value + 1  # States 0:(n-1); subscripts 1:n\n   n &lt;- nrow(transition)\n   for (i in 2:N){\n    X[i] &lt;- sample(1:n, size=1, prob=transition[X[i-1], ])\n    if(length(stopval)&gt;0)if(X[i] %in% (stopval+1)){X &lt;- X[1:i]; break}}\n  X - 1\n}\n # Set `stopval=c(0,5)` to stop when  the player's fortune is $0 or $5\n\n2.16\n\nPb &lt;- rbind(\n  Sun = c(Sun=0.6, Cloud=0.2, Rain=0.2),\n  Cloud= c(0.2, 0.4, 0.4),\n  Rain= c(0.4, 0.3, 0.3))\nPb\n\n2.16b\n\nplotmarkov &lt;-\n  function(n=1000, width=101, start=0, transition=Pb, npanels=5){\n    xc2 &lt;- Markov(n, initial.value=start, transition)\n    mav0 &lt;- zoo::rollmean(as.integer(xc2==0), k=width)\n    mav1 &lt;- zoo::rollmean(as.integer(xc2==1), k=width)\n    npanel &lt;- cut(1:length(mav0), breaks=seq(from=1, to=length(mav0),\n                  length=npanels+1), include.lowest=TRUE)\n    df &lt;- data.frame(av0=mav0, av1=mav1, x=1:length(mav0), gp=npanel)\n    print(xyplot(av0+av1 ~ x | gp, data=df, layout=c(1,npanels), type=\"l\",\n          par.strip.text=list(cex=0.65), auto.key=list(columns=2),\n          scales=list(x=list(relation=\"free\"))))\n}\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/ch2.R\")\n}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Generalizing from models</span>"
    ]
  },
  {
    "objectID": "ch3.html",
    "href": "ch3.html",
    "title": "3  Chapter 3: Multiple linear regression",
    "section": "",
    "text": "Packages required (plus any dependencies)\nDAAG car MASS AICcmodavg leaps BayesFactor splines\nAdditionally, Hmisc and knitr are required in order to process the Rmd source file.\n\n\nSection 3.1 Basic ideas: the allbacks book weight data\n\nallbacks &lt;- DAAG::allbacks  # Place the data in the workspace\nallbacks.lm &lt;- lm(weight ~ volume+area, data=allbacks)\nprint(coef(summary(allbacks.lm)), digits=2)\n\n\nxlim &lt;- range(allbacks$volume)\nxlim &lt;- xlim+c(-.075,.075)*diff(xlim)\n## Plot of weight vs volume: data frame allbacks (DAAG)\nplot(weight ~ volume, data=allbacks, pch=c(16,1)[unclass(cover)],\nlwd=1.25, xlim=xlim, fg=\"gray\")\n## unclass(cover) gives the integer codes that identify levels\n## As text() does not accept the parameter data, use with()\n## to specify the data frame.\nwith(allbacks, text(weight ~ volume, labels=paste(1:15), cex=0.75, offset=0.35,\npos=c(2,4)[unclass(cover)]))\nlegend(x='topleft', pch=c(16,1), legend=c(\"hardback  \",\"softback\"),\nhoriz=T, bty=\"n\", xjust=0.5, x.intersp=0.75, )\n\n\n## Correlations between estimates -- model with intercept\nround(summary(allbacks.lm, corr=TRUE)$correlation, 3)\n\n\nout &lt;- capture.output(summary(allbacks.lm,digits=2))\ncat(out[15:17], sep='\\n')\n\n\n## 5% critical value; t-statistic with 12 d.f.\nqt(0.975, 12)\n\n\ncat(out[5:7], sep='\\n')\n\n\nSubsection 3.1.1: A sequential analysis of variance table\n\nanova(allbacks.lm)\n\n\nOmission of the intercept term\n\n## Show rows 1, 7, 8 and 15 only\nmodel.matrix(allbacks.lm)[c(1,7,8,15), ]\n## NB, also, code that returns the data frame used\nmodel.frame(allbacks.lm)\n\n\nallbacks.lm0 &lt;- lm(weight ~ -1+volume+area, data=allbacks)\nprint(coef(summary(allbacks.lm0)), digits=2)\n\n\n## Correlations between estimates -- no intercept\nprint(round(summary(allbacks.lm0, corr=TRUE)$correlation, 3))\n\n\n\n\nSubsection 3.1.2: Diagnostic plots\n\nallbacks.lm0 &lt;- lm(weight ~ -1+volume+area, data=allbacks)\nplot(allbacks.lm0, caption=c('A: Resids vs Fitted', 'B: Normal Q-Q',\n     'C: Scale-Location', '', 'D: Resids vs Leverage'), cex.caption=0.85,\n     fg='gray')\n\n\n## To show all plots in the one row, precede with\npar(mfrow=c(1,4))      # Follow with par(mfrow=c(1,1))\n\n\n## The following has the default captions\nplot(allbacks.lm0)\n\n\nallbacks.lm13 &lt;- lm(weight ~ -1+volume+area, data=allbacks[-13, ])\nprint(coef(summary(allbacks.lm13)), digits=2)\n\n\n\n\nSection 3.2 The interpretation of model coefficients\n\nSubsection 3.2.1: Times for Northern Irish hill races\n\noldpar &lt;- par(fg='gray20',col.axis='gray20',lwd=0.5,col.lab='gray20')\nnihr &lt;- within(DAAG::nihills, {mph &lt;- dist/time; gradient &lt;- climb/dist})\nnihr &lt;- nihr[, c(\"time\", \"dist\", \"climb\", \"gradient\", \"mph\")]\nvarLabs &lt;- c(\"\\ntime\\n(hours)\",\"\\ndist\\n(miles)\",\"\\nclimb\\n(feet)\",\n             \"\\ngradient\\n(ft/mi)\", \"\\nmph\\n(mph)\")\nsmoothPars &lt;- list(col.smooth='red', lty.smooth=2, lwd.smooth=0.5, spread=0)\ncar::spm(nihr, cex.labels=1.2, regLine=FALSE, col='blue',\n         oma=c(1.95,3,4, 3), gap=.25, var.labels=varLabs, smooth=smoothPars)\ntitle(main=\"A: Untransformed scales:\", outer=TRUE,\nadj=0, line=-1.0, cex.main=1, font.main=1)\n## Panel B: Repeat with log(nihills) in place of nihills,\n## and with variable labels suitably modified.\nvarLabs &lt;- c(\"\\ntime\\n(log h)\",\"\\ndist\\n(log miles)\", \"\\nclimb\\n(log feet)\",\n             \"\\ngradient\\n(log ft/mi)\", \"\\nmph\\n(log mph)\")\ncar::spm(log(nihr), regLine=FALSE, col=\"blue\", oma=c(1.95,2.5,4, 2.5),\n         gap=.25, var.labels=varLabs, smooth=smoothPars)\ntitle(\"B: Logarithmic scales\", outer=TRUE,\n      adj=0, line=-1.0, cex.main=1, font.main=1)\npar(oldpar)\n\n\nWhat is special about logarithmic transformations?\n\n\n\nSubsection 3.2.2: An equation that predicts dist/time\n\n##  Hold climb constant at mean on logarithmic scale\nmphClimb.lm &lt;- lm(mph ~ log(dist)+log(climb), data = nihr)\n## Hold `gradient=climb/dist` constant at mean on logarithmic scale\nmphGradient.lm &lt;- lm(mph ~ log(dist)+log(gradient), data = nihr)\navRate &lt;- mean(nihr$mph)\nbClimb &lt;- coef(mphClimb.lm)\nconstCl &lt;- c(bClimb[1]+bClimb[3]*mean(log(nihr$climb)), bClimb[2])\nbGradient &lt;- coef(mphGradient.lm)\nconstSl &lt;- c(bGradient[1]+bGradient[3]*mean((log(nihr$climb/nihr$dist))),\n             bGradient[2])\n# Use `dist` and `climb` as explanatory variables\ncoef(mphClimb.lm)\n# Use `dist` and `gradient` as explanatory variables\ncoef(mphGradient.lm)\n\n\nopar &lt;- par(mfrow=c(1,2), mgp=c(2.25,0.5,0), mar=c(3.6,4.1,2.1,1.6))\nlineCols &lt;- c(\"red\", adjustcolor(\"magenta\",0.4))\nyaxlab&lt;-substitute(paste(\"Minutes per mile (Add \", ym, \")\"), list(ym=round(avRate,2)))\ncar::crPlots(mphClimb.lm, terms = . ~ log(dist), xaxt='n',\n             xlab=\"Distance\", col.lines=lineCols,  ylab=yaxlab)\naxis(2, at=4:7, labels=paste(4:7))\nlabx &lt;- c(4,8,16,32)\naxis(1, at=log(2^(2:5)), labels=paste(2^(2:5)))\nbox(col=\"white\")\nmtext(\"A: Hold climb constant at mean value\", adj=0,\n      line=0.8, at=0.6, cex=1.15)\ncar::crPlots(mphGradient.lm, terms = . ~log(dist), xaxt='n',\n             xlab=\"Distance\", col.lines=lineCols, ylab=yaxlab)\naxis(1, at=log(2^(2:5)), labels=paste(2^(2:5)))\naxis(2, at=4:7, labels=paste(4:7))\nbox(col=\"white\")\nmtext(\"B: Hold log(gradient) constant at mean\", adj=0, line=0.8, at=0.6, cex=1.15)\npar(opar)\n\n\nsummary(mphClimb.lm, corr=T)$correlation[\"log(dist)\", \"log(climb)\"]\nsummary(mphGradient.lm, corr=T)$correlation[\"log(dist)\", \"log(gradient)\"]\n\n\n## Show the plots, with default captions\nplot(mphClimb.lm, fg='gray')\n\n\nplot(mphGradient.lm, caption=c('A: Resids vs Fitted', 'B: Normal Q-Q',\n'C: Scale-Location', '', 'D: Resids vs Leverage'),\ncex.caption=0.85, fg='gray')\n\n\n\nSubsection 3.2.3: Equations that predict log(time)\n\nlognihr &lt;- setNames(log(nihr), paste0(\"log\", names(nihr)))\ntimeClimb.lm &lt;- lm(logtime ~ logdist + logclimb, data = lognihr)\n\n\nprint(coef(summary(timeClimb.lm)), digits=2)\n\n\ntimeGradient.lm &lt;- lm(logtime ~ logdist + loggradient, data=lognihr)\nprint(coef(summary(timeGradient.lm)), digits=3)\n\n\n\nSubsection 3.2.4: Book dimensions — the oddbooks dataset\n\noldpar &lt;- par(fg='gray40',col.axis='gray20',lwd=0.5,col.lab='gray20')\n## Code for Panel A\noddbooks &lt;- DAAG::oddbooks\npairs(log(oddbooks), lower.panel=panel.smooth, upper.panel=panel.smooth,\n      labels=c(\"log(thick)\", \"log(breadth)\", \"log(height)\", \"log(weight)\"),\n      gap=0.25, oma=c(1.95,1.95,4, 1.95), col='blue')\ntitle(main=\"A: Columns from log(oddbooks)\",\n      outer=TRUE, adj=0, line=-1.0, cex.main=1.1, font.main=1)\n## Panel B\noddothers &lt;-\n  with(oddbooks, data.frame(density = weight/(breadth*height*thick),\narea = breadth*height, thick=thick, weight=weight))\npairs(log(oddothers), lower.panel=panel.smooth, upper.panel=panel.smooth,\nlabels=c(\"log(density)\", \"log(area)\", \"log(thick)\", \"log(weight)\"),\ngap=0.5, oma=c(1.95,1.95,4, 1.95), col='blue')\ntitle(\"B: Add density & area; omit breadth & height\",\nouter=TRUE, adj=0, line=-1.0, cex.main=1.1, font.main=1)\npar(oldpar)\n\n\nlob3.lm &lt;- lm(log(weight) ~ log(thick)+log(breadth)+log(height),\n              data=oddbooks)\n# coef(summary(lob3.lm))\n\n\nlob2.lm &lt;- lm(log(weight) ~ log(thick)+log(breadth), data=oddbooks)\ncoef(summary(lob2.lm))\n\n\nlob0.lm &lt;- lm(log(weight) ~ 1, data=oddbooks)\nadd1(lob0.lm, scope=~log(breadth) + log(thick) + log(height))\nlob1.lm &lt;- update(lob0.lm, formula=. ~ .+log(breadth))\n\n\nround(rbind(\"lob1.lm\"=predict(lob1.lm), \"lob2.lm\"=predict(lob2.lm),\n            \"lob3.lm\"=predict(lob3.lm)),2)\n\n\noddbooks &lt;- within(oddbooks, density &lt;- weight/(thick*breadth*height))\nlm(log(weight) ~ log(density), data=oddbooks) |&gt; summary() |&gt; coef() |&gt;\n  round(3)\n\n\n## Code that the reader may care to try\nlm(log(weight) ~ log(thick)+log(breadth)+log(height)+log(density),\n   data=oddbooks) |&gt; summary() |&gt; coef() |&gt; round(3)\n\n\n\nSubsection 3.2.5: Mouse brain weight example\n\noldpar &lt;- par(fg='gray40',col.axis='gray20',lwd=0.5,col.lab='gray20')\nlitters &lt;- DAAG::litters\npairs(litters, labels=c(\"lsize\\n\\n(litter size)\", \"bodywt\\n\\n(Body Weight)\",\n                        \"brainwt\\n\\n(Brain Weight)\"), gap=0.5, fg='gray',\n                        col=\"blue\", oma=rep(1.95,4))\npar(oldpar)\n\n\n## Regression of brainwt on lsize\nsummary(lm(brainwt ~ lsize, data = litters), digits=3)$coef\n## Regression of brainwt on lsize and bodywt\nsummary(lm(brainwt ~ lsize + bodywt, data = litters), digits=3)$coef\n\n\n\nSubsection 3.2.6: Issues for causal interpretation\n\nEffects of lifestyle on health\n\n\nThe studies mostly agree. But what do they say?\n\n\nAdjusting for confounders\n\n\n\n\nSection 3.3 Choosing the model, and checking it out\n\nEffects of lifestyle on health\n\n\nThe studies mostly agree. But what do they say?\n\n\nAdjusting for confounders\n\noddbooks.lm &lt;- lm((weight) ~ log(thick)+log(height)+log(breadth),\ndata=DAAG::oddbooks)\nyterms &lt;- predict(oddbooks.lm, type=\"terms\")\n\n\n\nSubsection 3.3.3: A more formal approach to the choice of transformation\n\n## Use car::powerTransform\nnihr &lt;- within(DAAG::nihills, {mph &lt;- dist/time; gradient &lt;- climb/dist})\nsummary(car::powerTransform(nihr[, c(\"dist\", \"gradient\")]), digits=3)\n\n\nform &lt;- mph ~ log(dist) + log(gradient)\nsummary(car::powerTransform(form, data=nihr))\n\n\nThe use of transformations — further comments\n\n\n\nSubsection 3.3.4: Accuracy estimates, fitted values and new observations\n\nlognihr &lt;- log(DAAG::nihills)\nnames(lognihr) &lt;- paste0(\"log\", names(lognihr))\ntimeClimb.lm &lt;- lm(logtime  ~ logdist + logclimb, data = lognihr)\n## Coverage intervals; use exp() to undo the log transformation\ncitimes &lt;- exp(predict(timeClimb.lm, interval=\"confidence\"))\n## Prediction intervals, i.e., for new observations\npitimes &lt;- exp(predict(timeClimb.lm, newdata=lognihr, interval=\"prediction\"))\n## fit ci:lwr ci:pwr pi:lwr pi:upr\nci_then_pi &lt;- cbind(citimes, pitimes[,2:3])\ncolnames(ci_then_pi) &lt;- paste0(c(\"\", rep(c(\"ci-\",\"pi-\"), c(2,2))),\n                               colnames(ci_then_pi))\n## First 4 rows\nprint(ci_then_pi[1:4,], digits=2)\n\n\ntimeClimb2.lm &lt;- update(timeClimb.lm, formula = . ~ . + I(logdist^2))\ng3.10 &lt;-\nfunction(model1=timeClimb.lm, model2=timeClimb2.lm)\n{\n## Panel A\ncitimes &lt;- predict(model1, interval=\"confidence\")\nord &lt;- order(citimes[,\"fit\"])\ncitimes &lt;- citimes[ord,]\nhat &lt;- citimes[,\"fit\"]\npitimes &lt;- predict(model1, newdata=lognihr, interval=\"prediction\")[ord,]\nlogobs &lt;- log(nihr[ord,\"time\"])\nxtiks &lt;- pretty(exp(hat))\nylim &lt;- range(c(pitimes[,\"lwr\"], pitimes[,\"upr\"], logobs)-rep(hat,3))\nlogytiks &lt;- pretty(ylim,5)\nytiks &lt;- round(exp(logytiks),2)\nxlim &lt;- range(hat)\nplot(hat, citimes[,\"lwr\"]-hat, type=\"n\", xlab = \"Time (fitted)\",\nylab = \"Difference from fit\",\nxlim=xlim, ylim = ylim, xaxt=\"n\", yaxt=\"n\", fg=\"gray\")\nmtext(side=3, line=0.75, adj=0, at=-2.0, \"A: CIs and PIs: Mean, prediction\")\nmtext(side=4, line=1.25, \"exp(Difference from fit)\", las=0)\naxis(1, at=log(xtiks), labels=paste(xtiks), lwd=0, lwd.ticks=1)\naxis(2, at=logytiks, las=1, lwd=0, lwd.ticks=1)\naxis(4, at=logytiks, labels=paste(ytiks), las=0, lwd=0, lwd.ticks=1)\npoints(hat, logobs-hat, pch=16, cex=0.65)\nlines(hat, citimes[,\"lwr\"]-hat, col = \"red\")\nlines(hat, citimes[,\"upr\"]-hat, col = \"red\")\nlines(hat, pitimes[,\"lwr\"]-hat, col = \"black\")\nlines(hat, pitimes[,\"upr\"]-hat, col = \"black\")\n## Panel B\ncitimes2 &lt;- predict(model2, interval=\"confidence\")[ord,]\nplot(hat, citimes[,\"lwr\"]-hat, type=\"n\", xlab = \"Time (fitted)\",\nylab = \"Difference from fit\",\nxlim=xlim, ylim = ylim, xaxt=\"n\", yaxt=\"n\", fg=\"gray\")\nmtext(side=3, line=0.75, adj=0, at=-2.0,\n\"B: CIs for fit, compare two models\")\nmtext(side=4, line=1.25, \"exp(Difference from fit)\", las=0)\naxis(1, at=log(xtiks), labels=paste(xtiks), lwd=0, lwd.ticks=1)\naxis(2, at=logytiks,las=1, lwd=0, lwd.ticks=1)\naxis(4, at=logytiks, labels=paste(ytiks), las=0,, lwd=0, lwd.ticks=1)\npoints(hat, logobs-hat, pch=16, cex=0.65)\nlines(hat, citimes[,\"lwr\"]-hat, col = \"red\")\nlines(hat, citimes[,\"upr\"]-hat, col = \"red\")\nhat2 &lt;- citimes2[,\"fit\"]\nlines(hat, citimes2[,\"lwr\"]-hat2, col = \"blue\", lty=2, lwd=1.5)\nlines(hat, citimes2[,\"upr\"]-hat2, col = \"blue\", lty=2, lwd=1.5)\n}\n\n\ntimeClimb2.lm &lt;- update(timeClimb.lm, formula = . ~ . + I(logdist^2))\n\n\n\nSubsection 3.3.5: Choosing the model — deaths from Atlantic hurricanes\n\noldpar &lt;- par(fg='gray20',col.axis='gray20',lwd=0.5,col.lab='gray20')\nhurric &lt;- DAAG::hurricNamed[,c(\"LF.PressureMB\", \"BaseDam2014\", \"deaths\")]\nthurric &lt;- car::powerTransform(hurric, family=\"yjPower\")\ntransY &lt;- car::yjPower(hurric, coef(thurric, round=TRUE))\nsmoothPars &lt;- list(col.smooth='red', lty.smooth=2, lwd.smooth=1, spread=0)\ncar::spm(transY, lwd=0.5, regLine=FALSE, oma=rep(2.5,4), gap=0.5,\n         col=\"blue\", smooth=smoothPars, cex.labels=1)\npar(oldpar)\n\n\nmodelform &lt;- deaths ~ log(BaseDam2014) + LF.PressureMB\npowerT &lt;- car::powerTransform(modelform, data=as.data.frame(hurric),\n                              family=\"yjPower\")\nsummary(powerT, digits=3)\n\n\ndeathP &lt;- with(hurric, car::yjPower(deaths, lambda=-0.2))\npower.lm &lt;- MASS::rlm(deathP ~ log(BaseDam2014) + LF.PressureMB, data=hurric)\nprint(coef(summary(power.lm)),digits=2)\n\n\n## Use (deaths+1)^(-0.2) as outcome variable\nplot(power.lm, cex.caption=0.85, fg=\"gray\",\n  caption=list('A: Resids vs Fitted', 'B: Normal Q-Q', 'C: Scale-Location', '',\n               'D: Resids vs Leverage'))\n\n\n\nSubsection 3.3.6: Strategies for fitting models — suggested steps\n\nDiagnostic checks\n\n\n\n\nSection 3.4 Robust regression, outliers, and influence\n\nSubsection 3.4.1: Making outliers obvious — robust regression\n\nhills2000 &lt;- DAAG::hills2000[,c(\"dist\", \"climb\", \"time\")]\nvarLabels &lt;- c(\"\\ndist\\n(log miles)\", \"\\nclimb\\n(log feet)\", \"\\ntime\\n(log hours)\")\nsmoothPars &lt;- list(col.smooth='red', lty.smooth=2, lwd.smooth=1, spread=0)\nhills2000 &lt;- DAAG::hills2000[,c(\"dist\", \"climb\", \"time\")]\nvarLabels &lt;- c(\"\\ndist\\n(log miles)\", \"\\nclimb\\n(log feet)\", \"\\ntime\\n(log hours)\")\ncar::spm(log(hills2000), smooth=smoothPars,  regLine=FALSE, cex.labels=1.5,\nvar.labels = varLabels, lwd=0.5, gap=0.5, oma=c(1.95,1.95,1.95,1.95))\n\n\n## Panel A\nlhills2k.lm &lt;- lm(log(time) ~ log(climb) + log(dist), data = hills2000)\nplot(lhills2k.lm, caption=\"\", which=1, fg=\"gray\", col=adjustcolor(\"black\", alpha=0.8))\nmtext(side=3, line=0.75, \"A: Least squares (lm) fit\", adj=0, cex=1.1)\n## Panel B\nlhills2k.lqs &lt;- MASS::lqs(log(time) ~ log(climb) + log(dist), data = hills2000)\nreres &lt;- residuals(lhills2k.lqs)\nrefit &lt;- fitted(lhills2k.lqs)\nbig3 &lt;- which(abs(reres) &gt;= sort(abs(reres), decreasing=TRUE)[3])\nplot(reres ~ refit, xlab=\"Fitted values (resistant fit)\",\nylab=\"Residuals (resistant fit)\", col=adjustcolor(\"black\", alpha=0.8), fg=\"gray\")\nlines(lowess(reres ~ refit), col=2)\ntext(reres[big3] ~ refit[big3], labels=rownames(hills2000)[big3],\npos=4-2*(refit[big3] &gt; mean(refit)), cex=0.8)\nmtext(side=3, line=0.75, \"B: Resistant (lqs) fit\", adj=0, cex=1.1)\n\n\n## Show only the 2nd diognostic plot, i.e., a normal Q-Q plot\n## plot(lhills2k.lm, which=2)\n\n\nOutliers, influential or not, should be taken seriously\n\n\n\nSubsection 3.4.2: Leverage, influence, and Cook’s distance\n\n\\(^*\\) Leverage and the hat matrix — technical details\n\nround(unname(hatvalues(timeClimb.lm)),2)\n\n\n\nInfluential points and Cook’s distance\n\n\nDynamic graphics\n\n## Residuals versus leverages\nnihills &lt;- DAAG::nihills\ntimeClimb.lm &lt;- lm(log(time)  ~ log(dist) + log(climb), data = nihills)\nplot(timeClimb.lm, which=5, add.smooth=FALSE, ps=9, sub.caption=\"\",\n     cex.caption=1.1, fg=\"gray\")\n  ## The points can alternatively be plotted using\n  ## plot(hatvalues(model.matrix(timeClimb.lm)), residuals(timeClimb.lm))\n\n\n## Residuals versus leverages\nplot(timeClimb.lm, which=5, add.smooth=FALSE)\n## The points can alternatively be plotted using\n## plot(hatvalues(model.matrix(timeClimb.lm)), residuals(timeClimb.lm))\n\n\n## This code is designed to be evaluated separately from other chunks\nwith(nihills, scatter3d(x=log(dist), y=log(climb), z=log(time), grid=FALSE,\n                        point.col=\"black\", surface.col=\"gray60\",\n                        surface.alpha=0.2, axis.scales=FALSE))\nwith(nihills, Identify3d(x=log(dist), y=log(climb), z=log(time),\n                labels=row.names(DAAG::nihills), minlength=8), offset=0.05)\n## To rotate display, hold down the left mouse button and move the mouse.\n## To put labels on points, right-click and drag a box around them, perhaps\n## repeatedly.  Create an empty box to exit from point identification mode.\n\n\n\nInfluence on the regression coefficients\n\n## Residuals versus leverages\nnihills &lt;- DAAG::nihills\ntimeClimb.lm &lt;- lm(log(time)  ~ log(dist) + log(climb), data = nihills)\nplot(timeClimb.lm, which=5, add.smooth=FALSE, ps=9, sub.caption=\"\",\n     cex.caption=1.1, fg=\"gray\")\n  ## The points can alternatively be plotted using\n  ## plot(hatvalues(model.matrix(timeClimb.lm)), residuals(timeClimb.lm))\n\n\n\n*Additional diagnostic plots\n\n## As an indication of what is available, try\ncar::influencePlot(allbacks.lm)\n\n\n\n\n\nSection 3.5 Assessment and comparison of regression models\n\nSubsection 3.5.1: *AIC, AICc, BIC, and Bayes Factors for normal theory regression models\n\n## Calculations using mouse brain weight data\nmouse.lm &lt;- lm(brainwt ~ lsize+bodywt, data=DAAG::litters)\nmouse0.lm &lt;- update(mouse.lm, formula = . ~ . - lsize)\n\n\naicc &lt;- sapply(list(mouse0.lm, mouse.lm), AICcmodavg::AICc)\ninfstats &lt;- cbind(AIC(mouse0.lm, mouse.lm), AICc=aicc,\n                  BIC=BIC(mouse0.lm, mouse.lm)[,-1])\nprint(rbind(infstats, \"Difference\"=apply(infstats,2,diff)), digits=3)\n\n\nlibrary(lattice)\ndf &lt;- data.frame(n=5:35, AIC=rep(2,31), BIC=log(5:35))\ncfAICc &lt;- function(n,p,d) 2*(p+d)*n/(n-(p+d)-1) - 2*p*n/(n-p-1)\ndf &lt;- cbind(df, AICc12=cfAICc(5:35,1,1), AICc34=cfAICc(5:35,3,1))\nlabs &lt;- sort(c(2^(0:6),2^(0:6)*1.5))\nxyplot(AICc12+AICc34+AIC+BIC ~ n, data=df, type='l', auto.key=list(columns=4),\n       scales=list(y=list(log=T, at=labs, labels=paste(labs))),\n par.settings=simpleTheme(lty=c(1,1:3), lwd=2, col=rep(c('gray','black'), c(1,3))))\n\n\nThe functions drop1() and add1()\n\n## Obtain AIC or BIC using `drop1()` or `add1()`\nn &lt;- nrow(DAAG::litters)\ndrop1(mouse.lm, scope=~lsize)              # AIC, with/without `lsize`\ndrop1(mouse.lm, scope=~lsize, k=log(n))   # BIC, w/wo `lsize`\nadd1(mouse0.lm, scope=~bodywt+lsize)     # AIC, w/wo `lsize`, alternative\n\n\n\nThe use of Bayesfactor::lmBF to compare the two models\n\nsuppressPackageStartupMessages(library(BayesFactor))\nbf1 &lt;- lmBF(brainwt ~ bodywt, data=DAAG::litters)\nbf2 &lt;- lmBF(brainwt ~ bodywt+lsize, data=DAAG::litters)\nbf2/bf1\n\n\n## Relative support statistics\nsetNames(exp(-apply(infstats[,-1],2,diff)/2), c(\"AIC\",\"AICc\",\"BIC\"))\n\n\n\n\nSubsection 3.5.2: Using anova() to compare models — the ihills data\n\nlognihr &lt;- log(DAAG::nihills)\nlognihr &lt;- setNames(log(nihr), paste0(\"log\", names(nihr)))\ntimeClimb.lm &lt;- lm(logtime ~ logdist + logclimb, data = lognihr)\ntimeClimb2.lm &lt;- update(timeClimb.lm, formula = . ~ . + I(logdist^2))\nprint(anova(timeClimb.lm, timeClimb2.lm, test=\"F\"), digits=4)\n\n\nprint(anova(timeClimb.lm, timeClimb2.lm, test=\"Cp\"), digits=3)\n## Compare with the AICc difference\nsapply(list(timeClimb.lm, timeClimb2.lm), AICcmodavg::AICc)\n\n\nform1 &lt;- update(formula(timeClimb.lm), ~ . + I(logdist^2) + logdist:logclimb)\naddcheck &lt;- add1(timeClimb.lm, scope=form1, test=\"F\")\nprint(addcheck, digits=4)\n\n\n\nSubsection 3.5.3: Training/test approaches, and cross-validation\n\n## Check how well timeClimb.lm model predicts for hills2000 data\ntimeClimb.lm &lt;- lm(logtime  ~ logdist + logclimb, data = lognihr)\nlogscot &lt;- log(subset(DAAG::hills2000,\n               !row.names(DAAG::hills2000)==\"Caerketton\"))\nnames(logscot) &lt;- paste0(\"log\", names(hills2000))\nscotpred &lt;- predict(timeClimb.lm, newdata=logscot, se=TRUE)\ntrainVar &lt;- summary(timeClimb.lm)[[\"sigma\"]]^2\ntrainDF &lt;- summary(timeClimb.lm)[[\"df\"]][2]\nmspe &lt;- mean((logscot[,'logtime']-scotpred[['fit']])^2)\nmspeDF &lt;- nrow(logscot)\n\n\npf(mspe/trainVar, mspeDF, trainDF, lower.tail=FALSE)\n\n\nscot.lm &lt;- lm(logtime ~ logdist+logclimb, data=logscot)\nsignif(summary(scot.lm)[['sigma']]^2, 4)\n\n\n\nSubsection 3.5.4: Further points and issues\n\nPatterns in the diagnostic plots – are they more than hints?\n{r 3_18, eval=F|\n\n\nWhat is the scatter about the fitted response\n\n\nModel selection and tuning risks\n\n\nGeneralization to new contexts requires a random sample of contexts\n\n\nWhat happens if we do not transform the hillrace data?\n\n\nAre “errors in x” an issue?\n\n\n\n\nSection 3.6 Problems with many explanatory variables\n\nSubsection 3.6.1: Variable selection issues\n\nVariable selection – a simulation with random data\n\ny &lt;- rnorm(100)\n## Generate a 100 by 40 matrix of random normal data\nxx &lt;- matrix(rnorm(4000), ncol = 40)\ndimnames(xx)&lt;- list(NULL, paste(\"X\",1:40, sep=\"\"))\n\n\n## ## Find the best fitting model. (The 'leaps' package must be installed.)\nxx.subsets &lt;- leaps::regsubsets(xx, y, method = \"exhaustive\", nvmax = 3, nbest = 1)\nsubvar &lt;- summary(xx.subsets)$which[3,-1]\nbest3.lm &lt;- lm(y ~ -1+xx[, subvar])\nprint(summary(best3.lm, corr = FALSE))\n\n\n## DAAG::bestsetNoise(m=100, n=40)\nbest3 &lt;- capture.output(DAAG::bestsetNoise(m=100, n=40))\ncat(best3[9:14], sep='\\n')\n\n\n\nThe extent of selection effects – a detailed simulation:\n\noldpar &lt;- par(fg='gray20',col.axis='gray20',lwd=0.5,col.lab='gray20')\nset.seed(41)\nlibrary(splines)\nDAAG::bsnVaryNvar(nvmax=3, nvar = 3:35, xlab=\"\")\nmtext(side=1, line=1.75, \"Number selected from\")\n\n\n\nCross-validation that accounts for the variable selection process\n\n\n*Regularization approaches\n\n\n\nSubsection 3.6.2: Multicollinearity\n\nAn example – compositional data\n\ndata(Coxite, package=\"compositions\")  # Places Coxite in the workspace\n  # NB: Proceed thus because `Coxite` is not exported from `compositions`\ncoxite &lt;- as.data.frame(Coxite)\n\n\noldpar &lt;- par(fg='gray20',col.axis='gray20',lwd=0.5,col.lab='gray20', tcl=-0.25)\npanel.cor &lt;- function(x, y, digits = 3, prefix = \"\", cex.cor=0.8, ...)\n{\nold.par &lt;- par(usr = c(0, 1, 0, 1)); on.exit(par(old.par))\nr &lt;- abs(cor(x, y))\ntxt &lt;- format(c(r, 0.123456789), digits = digits)[1]\ntxt &lt;- paste0(prefix, txt)\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * sqrt(r))\n}\npairs(coxite, gap=0.4, col=adjustcolor(\"blue\", alpha=0.9), upper.panel=panel.cor)\npar(oldpar)\n\n\ncoxiteAll.lm &lt;- lm(porosity ~ A+B+C+D+E+depth, data=coxite)\nprint(coef(summary(coxiteAll.lm)), digits=2)\n\n\ncoxiteAll.lm &lt;- lm(porosity ~ A+B+C+D+E+depth, data=coxite)\ncoxite.hat &lt;- predict(coxiteAll.lm, interval=\"confidence\")\nhat &lt;- coxite.hat[,\"fit\"]\nplot(porosity ~ hat, data=coxite, fg=\"gray\", type=\"n\", xlab=\"Fitted values\",\nylab=\"Fitted values, with 95% CIs\\n(Points are observed porosities)\",\ntcl=-0.35)\nwith(coxite, points(porosity ~ hat, cex=0.75, col=\"gray45\"))\nlines(hat, hat, lwd=0.75)\nord &lt;- order(hat)\nsebar &lt;- function(x, y1, y2, eps=0.15, lwd=0.75){\nlines(rep(x,2), c(y1,y2), lwd=lwd)\nlines(c(x-eps,x+eps), rep(y1,2), lwd=lwd)\nlines(c(x-eps,x+eps), rep(y2,2), lwd=lwd)\n}\nq &lt;- ord[round(quantile(1:length(hat), (1:9)/10))]\nfor(i in q)sebar(hat[i], coxite.hat[i,\"lwr\"], coxite.hat[i,\"upr\"])\ncoxiteAll.lm &lt;- lm(porosity ~ A+B+C+D+E+depth, data=coxite)\ncoxite.hat &lt;- predict(coxiteAll.lm, interval=\"confidence\")\nhat &lt;- coxite.hat[,\"fit\"]\n\n\n## Pointwise confidence bounds can be obtained thus:\nhat &lt;- predict(coxiteAll.lm, interval=\"confidence\", level=0.95)\n\n\n\n\nSubsection 3.6.3: The variance inflation factor (VIF)\n\nprint(DAAG::vif(lm(porosity ~ A+B+C+D+depth, data=coxite)), digits=2)\n\n\nb &lt;- leaps::regsubsets(porosity ~ ., data=coxite, nvmax=4, method='exhaustive')\n## The calculation fails for nvmax=5\ninOut &lt;- summary(b)[[\"which\"]]\n## Extract and print the coefficents for the four regressions\ndimnam &lt;- list(rep(\"\",4),c(\"Intercept\", colnames(coxite)[-7]))\ncmat &lt;- matrix(nrow=4, ncol=7, dimnames=dimnam)\nfor(i in 1:4)cmat[i,inOut[i,]] &lt;- signif(coef(b,id=1:4)[[i]],3)\noutMat &lt;- cbind(cmat,\"  \"=rep(NA,4),\nas.matrix(as.data.frame(summary(b)[c(\"adjr2\", \"cp\", \"bic\")])))\nprint(signif(outMat,3),na.print=\"\")\n\n\nBC.lm &lt;- lm(porosity ~ B+C, data=coxite)\nprint(signif(coef(summary(BC.lm)), digits=3))\ncar::vif(BC.lm)\n\n\n## Diagnostic plots can be checked thus:\nplot(BC.lm, eval=xtras)\n\n\nNumbers that do not quite add up\n\ncoxiteR &lt;- coxite\ncoxiteR[, 1:5] &lt;- round(coxiteR[, 1:5])\ncoxiteR.lm &lt;- lm(porosity ~ ., data=coxiteR)\nprint(coef(summary(coxiteR.lm)), digits=2)\nprint(DAAG::vif(lm(porosity ~ .-E, data=coxiteR)), digits=2)\n\n\n\nRemedies for multicollinearity\n\n\n\n\nSection 3.7 Errors in x\n\nMeasurement of dietary intake\n\n\nSimulations of the effect of measurement error\n\ngph &lt;- DAAG::errorsINx(gpdiff=0, plotit=FALSE, timesSDx=(1:4)/2,\n                       layout=c(5,1), print.summary=FALSE)[[\"gph\"]]\nparset &lt;- DAAG::DAAGtheme(color=FALSE, alpha=0.6, lwd=2,\n                          col.points=c(\"gray50\",\"black\"),\n                          col.line=c(\"gray50\",\"black\"), lty=1:2)\nupdate(gph, par.settings=parset)\n\n\n\n*Two explanatory variables\n\n\nTwo explanatory variables, one measured without error – a simulation\n\ngph &lt;- DAAG::errorsINx(gpdiff=1.5, timesSDx=(1:2)*0.8, layout=c(3,1),\nprint.summary=FALSE, plotit=FALSE)[[\"gph\"]]\nparset &lt;- DAAG::DAAGtheme(color=FALSE, alpha=0.6, lwd=2,\n                          col.points=c(\"gray50\",\"black\"),\n                          col.line=c(\"gray50\",\"black\"), lty=1:2)\nupdate(gph, par.settings=parset)\n\n\n\nAn arbitrary number of variables\n\n\n*The classical error model versus the Berkson error model\n\n\nUsing missing value approaches to address measurement error\n\n\n\nSection 3.8 Multiple regression models – additional points\n\ncoef(lm(area ~ volume + weight, data=allbacks))\nb &lt;- as.vector(coef(lm(weight ~ volume + area, data=allbacks)))\nc(\"_Intercept_\"=-b[1]/b[3], volume=-b[2]/b[3], weight=1/b[3])\n\n\nUnintended correlations\n\n\nSubsection 3.8.2: Missing explanatory variables\n\ngaba &lt;- DAAG::gaba\ngabalong &lt;- stack(gaba[\"30\", -match('min', colnames(gaba))])\ngabalong$sex &lt;- factor(rep(c(\"male\", \"female\",\"all\"), rep(2,3)),\nlevels=c(\"female\",\"male\",\"all\"))\ngabalong$treatment &lt;- factor(rep(c(\"Baclofen\",\"No baclofen\"), 3),\nlevels=c(\"No baclofen\",\"Baclofen\"))\ngph &lt;- lattice::stripplot(sex~values, groups=treatment, data=gabalong,\npanel=function(x,y,...){\nlattice::panel.stripplot(x,y,...)\nlattice::ltext(x,y,paste(c(3,9,15,7,22,12)), pos=1, cex=0.8)\n}, auto.key=list(space=\"right\", points=TRUE, cex=0.8))\nbw9 &lt;- list(fontsize=list(text=9, points=5),\ncex=c(1.5,1.5), pch=c(1,16))\nupdate(gph, par.settings=parset,\nxlab=list(\"Average reduction: 30 min vs 0 min\", cex=1.0),\nscales=list(cex=1.0, tck=0.35))\n\n\nStrategies\n\n\n\nSubsection 3.8.3: Added variable plots\n\nyONx.lm &lt;- lm(logtime ~ logclimb, data=lognihr)\ne_yONx &lt;- resid(yONx.lm)\nprint(coef(yONx.lm), digits=4)\n\n\nzONx.lm &lt;- lm(logdist ~ logclimb, data=lognihr)\ne_zONx &lt;- resid(zONx.lm)\nprint(coef(yONx.lm), digits=4)\n\n\ney_xONez_x.lm &lt;- lm(e_yONx ~ 0+e_zONx)\ne_yONxz &lt;- resid(ey_xONez_x.lm)\nprint(coef(ey_xONez_x.lm), digits=4)\n\n\noldpar &lt;- par(fg='gray')\n## Code for added variable plots\nlogtime.lm &lt;- lm(logtime ~ logclimb+logdist, data=lognihr)\ncar::avPlots(logtime.lm, lwd=1, terms=\"logdist\", fg=\"gray\")\nmtext(side=3, line=0.5, \"A: Added var: 'logdist'\", col=\"black\", adj=0, cex=1.15)\ncar::avPlots(logtime.lm, lwd=1, terms=\"logclimb\", fg=\"gray\")\nmtext(side=3, line=0.5, \"B: Added var: 'logclimb'\", col=\"black\", adj=0, cex=1.15)\npar(oldpar)\n\n\n## One call to show both plots\ncar::avPlots(timeClimb.lm, terms=~.)\n\n\n## Alternative code for first plot\nplot(e_yONx ~ e_zONx)\n\n\nplot(yONx.lm, which=1, caption=\"\", fg=\"gray\")\nmtext(side=3, line=0.5, \"A: From 'logtime' on 'logclimb'\", adj=0, cex=0.85)\nplot(zONx.lm, which=1, caption=\"\", fg=\"gray\")\nmtext(side=3, line=0.5, \"B: From 'logdist' on 'logclimb'\", adj=0, cex=0.85)\nplot(ey_xONez_x.lm, which=1, caption=\"\", fg=\"gray\")\nmtext(side=3, line=0.5, \"C: From AVP\", adj=-0, cex=0.85)\n\n\nAlternatives to Added Variable Plots\n\n\n*Algebraic details\n\nab1 &lt;- coef(yONx.lm)\nab2 &lt;- coef(zONx.lm)\nb2 &lt;- coef(ey_xONez_x.lm)\nb1 &lt;- ab1[2] - b2*ab2[2]\na &lt;- ab1[1] - b2*ab2[1]\n\n\ncoef(lm(logtime ~ logclimb + logdist, data=lognihr))\n\n\n\n\nSubsection 3.8.4: Nonlinear methods – an alternative to transformation?\n\nnihr$climb.mi &lt;- nihr$climb/5280\nnihr.nls0 &lt;- nls(time ~ (dist^alpha)*(climb.mi^beta), start =\n                    c(alpha = 0.68, beta = 0.465), data = nihr)\n## plot(residuals(nihr.nls0) ~ log(predict(nihr.nls0)))\n\n\nsignif(coef(summary(nihr.nls0)),3)\n\n\nnihr.nls &lt;- nls(time ~ gamma + delta1*dist^alpha + delta2*climb.mi^beta,\nstart=c(gamma = .045, delta1 = .09, alpha = 1,\ndelta2=.9, beta = 1.65), data=nihr)\n## plot(residuals(nihr.nls) ~ log(predict(nihr.nls)))\n\n\nsignif(coef(summary(nihr.nls)),3)\n\n\n\n\nSection 3.9: Recap\n\n\nSection 3.10: Further reading\n\n\nExercises (3.11)\n3.1\n\n## ## Set up factor that identifies the `have' cities\ncities &lt;- DAAG::cities\ncities$have &lt;- with(cities, factor(REGION %in% c(\"ON\",\"WEST\"),\n                                   labels=c(\"Have-not\",\"Have\")))\n\n\ngphA &lt;- lattice::xyplot(POP1996~POP1992, groups=have, data=cities,\n                auto.key=list(columns=2))\ngphB&lt;-lattice::xyplot(log(POP1996)~log(POP1992), groups=have, data=cities,\n                auto.key=list(columns=2))\nprint(gphA, split=c(1,1,2,1), more=TRUE)\nprint(gphB, split=c(2,1,2,1))\n\n\ncities.lm1 &lt;- lm(POP1996 ~ have+POP1992, data=cities)\ncities.lm2 &lt;- lm(log(POP1996) ~ have+log(POP1992), data=cities)\n\n3.8a\n\nnihills.lm &lt;- lm(time ~ dist+climb, data=DAAG::nihills)\nnihillsX.lm &lt;- lm(time ~ dist+climb+dist:climb, data=DAAG::nihills)\nanova(nihills.lm, nihillsX.lm)   # Use `anova()` to make the comparison\ncoef(summary(nihillsX.lm))       # Check coefficient for interaction term\ndrop1(nihillsX.lm)\n\n3.11\n\nlog(time) ~ log(dist) + log(climb)    ## lm model\ntime ~ alpha*dist + beta*I(climb^2)   ## nls model\n\n3.13\n\nx1 &lt;- runif(10)            # predictor which will be missing\nx2 &lt;- rbinom(10, 1, 1-x1)\n  ## observed predictor, depends on missing predictor\ny &lt;- 5*x1 + x2 + rnorm(10,sd=.1)  # simulated model; coef of x2 is positive\ny.lm &lt;- lm(y ~ factor(x2)) # model fitted to observed data\ncoef(y.lm)\ny.lm2 &lt;- lm(y ~ x1 + factor(x2))   # correct model\ncoef(y.lm2)\n\n3.16\n\nbomData &lt;- DAAG::bomregions2021\nnraw.lqs &lt;- MASS::lqs(northRain ~ SOI + CO2, data=bomData)\nnorth.lqs &lt;- MASS::lqs(I(northRain^(1/3)) ~ SOI + CO2, data=bomData)\nplot(residuals(nraw.lqs) ~ Year, data=bomData)\nplot(residuals(north.lqs) ~ Year, data=bomData)\n\n3.17f\n\nsocpsych &lt;- subset(DAAG::repPsych, Discipline=='Social')\nwith(socpsych, scatter.smooth(T_r.R~T_r.O))\nabline(v=.5)\n\n\nsoc.rlm &lt;- MASS::rlm(T_r.R~T_r.O, data=subset(socpsych, T_r.O&lt;=0.5))\n## Look at summary statistics\ntermplot(soc.rlm, partial.resid=T, se=T)\n\n\nplot(soc.rlm)\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/ch3.R\")\n}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "ch4.html",
    "href": "ch4.html",
    "title": "4  Chapter 4: Exploiting the linear model framework",
    "section": "",
    "text": "Packages required (with dependencies)\nDAAG effects mgcv splines scam MASS latticeExtra car WDI AICcmodavg ggplot2 kableExtra qgam patchwork\nAdditionally, Hmisc and knitr are required in order to process the Rmd source file.\nNote the use of the ‘patchwork’ package to make it easy to place two ggplot2 plots side by side.\n\nHmisc::knitrSet(basename=\"exploit\", lang='markdown', fig.path=\"figs/g\", w=7, h=7)\noldopt &lt;- options(digits=4, width=70, scipen=999)\nlibrary(knitr)\n## knitr::render_listings()\nopts_chunk[['set']](cache.path='cache-', out.width=\"80%\", fig.align=\"center\", \n                    fig.show='hold', formatR.arrow=FALSE, ps=10, \n                    strip.white = TRUE, comment=NA, width=70, \n                    tidy.opts = list(replace.assign=FALSE))\n\n\n\nSection 4.1 Levels of a factor – using indicator variables\n\nSubsection 4.1.1: Example – sugar weight\n\nsugar &lt;- DAAG::sugar  # Copy dataset 'sugar' into the workspace\n## Ensure that \"Control\" is the first level\nsugar[[\"trt\"]] &lt;- relevel(sugar[[\"trt\"]], ref=\"Control\")\noptions()[[\"contrasts\"]]  # Check the default factor contrasts\n## If your output does not agree with the above, then enter\n## options(contrasts=c(\"contr.treatment\", \"contr.poly\"))\n\n\nsugar.aov &lt;- aov(weight ~ trt, data=sugar)\n## To display the model matrix, enter: model.matrix(sugar.aov)\n## Note the use of summary.lm(), not summary() or summary.aov()\nround(signif(coef(summary.lm(sugar.aov)), 3), 4)\n\n\nsem &lt;- summary.lm(sugar.aov)$sigma/sqrt(3)  # 3 results/trt\n# Alternatively, sem &lt;- 6.33/sqrt(2)\nqtukey(p=.95, nmeans=4, df=8) * sem\n\n\n\nSubsection 4.1.2: Different choices for the model matrix when there are factors\n\ncontrasts(sugar$trt) &lt;- 'contr.sum'\nsugarSum.aov &lt;- aov(weight ~ trt, data = sugar)\nround(signif(coef(summary.lm(sugarSum.aov)), 3),4)\n\n\ndummy.coef(sugarSum.aov)\n\n\nFactor contrasts – further details\n\ncontrasts(sugar$trt) &lt;- \"contr.sum\"\n\n\nfish &lt;- factor(1:3, labels=c(\"Trout\",\"Cod\",\"Perch\"))\n\n\ncontr.treatment(fish)\n# Base is \"Trout\"\n\n\ncontr.SAS(fish)\n# Base is \"Perch\"\n\n\ncontr.sum(fish)\n# Base is mean of levels\n\n\n\n*Tests for main effects in the presence of interactions?\n\n\n\n\nSection 4.2 Block designs and balanced incomplete block designs\n\nSubsection 4.2.1: Analysis of the rice data, allowing for block effects\n\nrice &lt;- DAAG::rice\nricebl.aov &lt;- aov(ShootDryMass ~ Block + variety * fert, data=rice)\nprint(summary(ricebl.aov), digits=3)\n\n\nround(signif(coef(summary.lm(ricebl.aov)), 3), 5)\nwith(summary.lm(ricebl.aov),\ncat(\"Residual standard error: \", sigma, \"on\", df[2], \"degrees of freedom\"))\n\n## AOV calculations, ignoring block effects\nrice.aov &lt;- aov(ShootDryMass ~ variety * fert, data=rice)\nsummary.lm(rice.aov)$sigma\n\nricebl.aov &lt;- aov(ShootDryMass ~ factor(Block) + variety * fert, data=rice)\n\n\nmodel.tables(ricebl.aov, type=\"means\", se=TRUE, cterms=\"variety:fert\")\n\n\n\nSubsection 4.2.2: A balanced incomplete block design\n\nappletaste &lt;- DAAG::appletaste\nwith(appletaste, table(product, panelist))\n\n\nsapply(appletaste, is.factor)  # panelist & product are factors\nappletaste.aov &lt;- aov(aftertaste ~ product + panelist, data=appletaste)\nsummary(appletaste.aov)\n\n\nas.data.frame(effects::Effect(\"product\", appletaste.aov, confidence.level=0.95))\n\n\n## NB that 'product' was first term in the model formula\n## Thus, the 1st 4 coefficients have the information required\ncoef(summary.lm(appletaste.aov))[1:4, ]\n\n\n\n\nSection 4.3 Fitting multiple lines\n\n## Fit various models to columns of data frame leaftemp (DAAG)\nleaftemp &lt;- DAAG::leaftemp\nleaf.lm1 &lt;- lm(tempDiff ~ 1 , data = leaftemp)\nleaf.lm2 &lt;- lm(tempDiff ~ vapPress, data = leaftemp)\nleaf.lm3 &lt;- lm(tempDiff ~ CO2level + vapPress, data = leaftemp)\nleaf.lm4 &lt;- lm(tempDiff ~ CO2level + vapPress +\n               vapPress:CO2level, data = leaftemp)\n\n\nanova(leaf.lm1, leaf.lm2, leaf.lm3, leaf.lm4)\n\n\nprint(coef(summary(leaf.lm3)), digits=2)\n\n\n\nSection 4.4 Methods for fitting smooth curves\n\nSubsection 4.4.1: Polynomial Regression\n\nseedrates &lt;- DAAG::seedrates\nform2 &lt;- grain ~ rate + I(rate^2)\n# Without the wrapper function I(), rate^2 would be interpreted\n# as the model formula term rate:rate, and hence as rate.\nquad.lm2 &lt;- lm(form2, data = seedrates)\n## Alternative, using gam()\n## quad.gam &lt;- mgcv::gam(form2, data = seedrates)\n\n\nsuppressPackageStartupMessages(library(ggplot2))\n\n\n## Use ggplot2 functions to plot points, line, curve, & 95% CIs\n## library(ggplot2)\ngph &lt;- ggplot(DAAG::seedrates, aes(rate,grain))+\n  geom_point(aes(size=3), color='magenta')+xlim(c(25,185))\ncolors &lt;- c(\"Linear\"=\"blue\", \"Quadratic\"=\"red\")\nggdat &lt;- ggplot_build(gph+geom_smooth(aes(rate,grain,color=\"Linear\"),\n  method=lm, formula=y~poly(x,2),fullrange=TRUE))$data[[2]]\ngph1 &lt;- gph+geom_smooth(aes(color=\"Linear\"), method=lm, formula=y~x, fullrange=TRUE, fill='dodgerblue')\ngph1 +  geom_line(data = ggdat, aes(x = x, y = y, color=\"Quadratic\"),\n                  linewidth=0.75)+\n  geom_ribbon(data=ggdat, aes(x=x,y=y, ymin=ymin, ymax=ymax,\n                              color=\"Quadratic\"), linewidth=0.75,\n  fill=NA, linetype=2, outline.type='both', show.legend=FALSE) +\n  scale_color_manual(values=colors, aesthetics = \"color\")+\n  theme(legend.position=c(.8,.78)) +\n  coord_cartesian(expand=FALSE) + xlab(\"Seeding rate (kg/ha)\") +\n    ylab(\"Grains per head\") + labs(color=\"Model\") +\n  guides(size='none',\n         color = guide_legend(override.aes = list(fill=\"transparent\") ) )\n## detach(\"package:ggplot2\")\n\n\nquad.lm2 &lt;- lm(grain ~ rate + I(rate^2), data = DAAG::seedrates)\nprint(coef(summary(quad.lm2)), digits=2)\ncat(\"\\nCorrelation matrix\\n\")\nprint(summary(quad.lm2, corr=TRUE)$correlation, digits=2)\n\n\n*An alternative formulation using orthogonal polynomials\n\nseedratesP.lm2 &lt;- lm(grain ~ poly(rate,2), data = seedrates)\nprint(coef(summary(seedratesP.lm2)), digits=2)\n\n\n## Alternative, using mgcv::gam()\nseedratesP.gam &lt;- mgcv::gam(grain ~ poly(rate,2), data = seedrates)\n\n\nlogseed.lm &lt;- lm(log(grain) ~ log(rate), data=DAAG::seedrates)\ncoef(summary(logseed.lm))\n\n\n## Use ggplot2 functions to plot points, line, curve, & 95% CIs\n## library(ggplot2)\ngph &lt;- ggplot(DAAG::seedrates, aes(rate,grain)) +\n  geom_point(size=3, color=\"magenta\")+xlim(c(25,185))\ncolors &lt;- c(\"Loglinear\"=\"gray40\", \"Quadratic\"=\"red\")\nggdat &lt;- ggplot_build(gph+geom_smooth(method=lm, formula=y~poly(x,2),\n                                      fullrange=TRUE))$data[[2]]\nggln &lt;- ggplot_build(gph+geom_smooth(method=lm,\n                        formula=log(y)~log(x),fullrange=TRUE))$data[[2]]\n## Assign to gphA rather than (as in text) plotting at this point\ngphA &lt;- gph +  geom_line(data = ggdat, aes(x = x, y = y, color=\"Quadratic\"),\n                 linewidth=0.75) +\ngeom_ribbon(data=ggdat, aes(x=x,y=y, ymin=ymin, ymax=ymax, color=\"Quadratic\"),\n            linewidth=0.75, fill=NA, linetype=2, outline.type='both',\n            show.legend=FALSE) +\ngeom_line(data = ggln, aes(x = x, y = exp(y), color=\"Loglinear\"),\n          linewidth = 0.75) +\ngeom_ribbon(data=ggln, aes(x=x,y=exp(y), ymin=exp(ymin), ymax=exp(ymax),\n            color=\"Loglinear\"), fill=NA, linewidth=0.75, linetype=3,\n            outline.type='both', show.legend=FALSE)+\n  scale_color_manual(values=colors, aesthetics = \"color\")+\n  coord_cartesian(expand=FALSE) +\n  xlab(\"Seeding rate (kg/ha)\") + ylab(\"Grains per head\") +\n  labs(title=\"A: Loglinear fit vs quadratic fit\", color=\"Model\") +\n  guides(size='none',\n         color = guide_legend(override.aes = list(fill=\"transparent\") ) ) +\n  theme(legend.position=c(.8,.78))\ndf &lt;- data.frame(rate=rep(DAAG::seedrates$rate,2), res=c(resid(logseed.lm),\n  log(DAAG::seedrates$grain)-log(fitted(quad.lm2))),\n  Model=rep(c(\"Loglinear\",\"Quadratic\"),rep(nrow(DAAG::seedrates),2)))\n## Assign to gphB rather than (as in text) plotting at this point\ngphB &lt;- ggplot(df, aes(x=rate, y=res, shape=Model,color=Model))+\ngeom_point(size=2.5) + scale_color_manual(values=colors) +\nxlab(\"Seeding rate (kg/ha)\") + ylab(\"Residuals on log scale\") +\nlabs(title=\"B: Residuals\") +\n  guides(size='none',\n         color = guide_legend(override.aes = list(fill=\"transparent\") ) ) +\n  theme(legend.position=c(.8,.78))\n## Now take advantage of the magic of the 'patchwork' package\nlibrary(patchwork)\ngphA+gphB\n## detach(\"package:ggplot2\")\n\n\naic &lt;- AIC(quad.lm2, logseed.lm)\naic[\"logseed.lm\",2] &lt;- aic[\"logseed.lm\",2] + sum(2*log(seedrates$grain))\nround(aic,1)\n\n\nseedrates&lt;-DAAG::seedrates\nquad.lm2 &lt;- lm(grain ~ poly(rate,degree=2), data=seedrates)\nns.lm2 &lt;- lm(grain ~ splines::ns(rate,df=2), data=seedrates)\ntps.gam2 &lt;- mgcv::gam(grain ~ s(rate, k=3, fx=T), data=seedrates)\n\n\nmflist &lt;- lapply(list(quad=quad.lm2, nsplines=ns.lm2, tps=tps.gam2), model.matrix)\nmftab &lt;- with(mflist, cbind(quad, nsplines, tps))\ncolnames(mftab) &lt;- c(\"(Int)\", \"poly2.1\", \"poly2.2\", \"(Int)\", \"ns2.1\", \"ns2.2\", \"(Int)\", \"s3.1\", \"s3.2\")\nlibrary(kableExtra)\nlinesep = c('', '', '', '\\\\addlinespace')\nkbl(mftab, booktabs=TRUE, format='latex', toprule=FALSE,\nformat.args=list(justify=\"right\", width=8)) |&gt;\nkable_styling(latex_options = c(\"scale_down\",latex_options = \"hold_position\"), position='center') |&gt;\nadd_header_above(c('poly(rate,2)' = 3, 'splines::ns(rate,df=2)' = 3, 's(rate, k=3, fx=T)' = 3),\nalign='c', monospace=rep(T,3))|&gt;\nadd_header_above(c('lm: grain~' = 3, 'lm: grain~'=3, 'gam: grain~'=3),\n                 align='c', monospace=rep(T,3), line=F)\n\n\n\nGAM models versus models fitted using lm()\n\n\nAlternative fits – what is the best choice?\n\n## Load required packages\nsuppressPackageStartupMessages(library(splines))\nsuppressPackageStartupMessages(library(mgcv))\n\n\nohms.tp &lt;- gam(kohms~s(juice, bs=\"tp\"), data=fruitohms)\nohms.cs &lt;- gam(kohms~s(juice, bs=\"cs\"), data=fruitohms)\nrange(fitted(ohms.tp)-fitted(ohms.cs))\n\n\nsummary(ohms.tp)\n\n\nsummary(ohms.tpBIC)\n\n\n\n\nSubsection 4.4.3: The contributions of basis curves to the fit\n\n\nSubsection 4.4.4: Checks on the fitted model\n\n## Printed output from `gam.check(ohms.tpBIC)`\ncat(out, sep=\"\\n\")\n\n\n\nSubsection 4.3.5: Monotone curves\n\nohms.scam &lt;- scam::scam(kohms ~ s(juice,bs=\"mpd\"), data=fruitohms)\nsummary(ohms.scam)\n\n\nAIC(ohms.scam, ohms.tp)\n\n\nBIC(ohms.scam, ohms.tp)\n\n\n\nSubsection 4.4.6: Different smooths for different levels of a factor\n\nwhiteside &lt;- MASS::whiteside\ngas.gam &lt;- gam(Gas ~ Insul+s(Temp, by=Insul), data=whiteside)\n\n\nsummary(gas.gam)\n\n\nBox.test(resid(gas.gam)[whiteside$Insul=='Before'], lag=1)\nBox.test(resid(gas.gam)[whiteside$Insul=='After'], lag=1)\n\n\n\nSubsection 4.4.7: The remarkable reach of mgcv and related packages\n\nDepartures from independence assumptions\n\n\n\nSubsection 4.4.8: Multiple spline smoothing terms — dewpoint data\n\n## GAM model -- `dewpoint` data\ndewpoint &lt;- DAAG::dewpoint\nds.gam &lt;- gam(dewpt ~ s(mintemp) + s(maxtemp), data=dewpoint)\nplot(ds.gam, resid=TRUE, pch=\".\", se=2, cex=2, fg=\"gray\")\n\n\nUsing residuals as a check for non-additive effects\n\nlibrary(lattice)\n## Residuals vs maxtemp, for different mintemp ranges\nmintempRange &lt;- equal.count(dewpoint$mintemp, number=3)\nds.xy &lt;- xyplot(residuals(ds.gam) ~ maxtemp|mintempRange, data=dewpoint,\n                layout=c(3,1), scales=list(tck=0.5), aspect=1, cex=0.65,\n                par.strip.text=list(cex=0.75), type=c(\"p\",\"smooth\"),\n                xlab=\"Maximum temperature\", ylab=\"Residual\")\nds.xy\n\n\n\n*A smooth surface\n\n## Fit surface\nds.tp &lt;- gam(dewpt ~ s(mintemp, maxtemp), data=DAAG::dewpoint)\nvis.gam(ds.tp, plot.type=\"contour\")   # gives a contour plot of the\n# fitted regression surface\nvis.gam(ds.gam, plot.type=\"contour\")  # cf, model with 2 smooth terms\n\n\n\n\nSubsection 4.4.9: Atlantic hurricanes that made landfall in the US\n\nhurricNamed &lt;- DAAG::hurricNamed\nhurricS.gam &lt;- gam(car::yjPower(deaths, lambda=-0.2) ~\n  s(log(BaseDam2014)) + s(LF.PressureMB),\n  data=hurricNamed, method=\"ML\")\nanova(hurricS.gam)\n\n\nplot(hurricS.gam, resid=TRUE, pch=16, cex=0.5, select=1, fg=\"gray\")\nmtext(side=3, line=1, \"A: Term in log(BaseDam2014)\", cex=1.0, adj=0, at=-3.75)\nplot(hurricS.gam, resid=TRUE, pch=16, cex=0.5, select=2, fg=\"gray\")\nmtext(side=3, line=1, \"B: Term in LF.PressureMB\", cex=1.0, adj=0, at=878)\nqqnorm(resid(hurricS.gam), main=\"\", fg=\"gray\")\nmtext(side=3, line=1, \"C: Q-Q plot of residuals\", cex=1.0, adj=0, at=-4.25)\n\n\nAn explanatory variable with an overly long-tailed distribution\n\nhurricSlog1.gam &lt;- gam(log(deaths+1) ~ s(log(BaseDam2014)), data=hurricNamed)\nhurricSlog2.gam &lt;- gam(log(deaths+1) ~ s(BaseDam2014), data=hurricNamed)\n\n\nplot(hurricSlog1.gam, resid=TRUE, pch=16, cex=0.5, adj=0, fg=\"gray\")\nmtext(side=3, \"A: Use log(BaseDam2014)\", cex=1.4, adj=0, line=1, at=-3.15)\nplot(hurricSlog2.gam, resid=TRUE, pch=16, cex=0.5, fg=\"gray\")\nmtext(side=3, \"B: Use BaseDam2014\", cex=1.4, adj=0, line=1, at=-28500)\n\n\n\n\nSubsection 4.4.10: Other smoothing methods\n\n\n\nSection 4.5 Quantile regression\n\n## If necessary, install the 'WDI' package & download data\nif(!file.exists(\"wdi.RData\")){\n  if(!is.element(\"WDI\", installed.packages()[,1]) )install.packages(\"WDI\")\ninds &lt;- c('SP.DYN.TFRT.IN','SP.DYN.LE00.IN', 'SP.POP.TOTL')\nindnams &lt;- c(\"FertilityRate\", \"LifeExpectancy\", \"population\")\nwdi2020 &lt;- WDI::WDI(country=\"all\", indicator=inds, start=2020, end=2020,\n                    extra=TRUE)\nwdi2020 &lt;- na.omit(droplevels(subset(wdi2020, !region %in% \"Aggregates\")))\nwdi &lt;- setNames(wdi2020[order(wdi2020[, inds[1]]),inds], indnams)\nsave(wdi, file=\"wdi.RData\")\n}\n\n\n2020 World Bank data on fertility and life expectancy\n\nload(\"wdi.RData\")  # Needs `wdi.RData` in working directory; see footnote\nlibrary(qgam)\nwdi[, \"ppop\"] &lt;- with(wdi, population/sum(population))\nwdi[,\"logFert\"] &lt;- log(wdi[,\"FertilityRate\"])\nform &lt;- LifeExpectancy ~ s(logFert)\n## Panel A model\nfit.qgam &lt;- qgam(form, data=wdi, qu=.5)\n## Panel B: Multiple (10%, 90% quantiles; unweighted, then weighted\nfit19.mqgam &lt;- mqgam(form, data=wdi, qu=c(.1,.9))\nwtd19.mqgam &lt;- mqgam(form, data=wdi, qu=c(.1,.9),\n                      argGam=list(weights=wdi[[\"ppop\"]]))\n\n\nhat50 &lt;- cbind(LifeExpectancy=wdi[, \"LifeExpectancy\"], logFert=wdi[,\"logFert\"],\n                as.data.frame(predict(fit.qgam, se=T)))\nhat50 &lt;- within(hat50, {lo &lt;- fit-2*se.fit; hi &lt;- fit+2*se.fit})\nhat19 &lt;- as.data.frame(matrix(nrow=nrow(wdi), ncol=4))\nfor(i in 1:2){hat19[[i]] &lt;- qdo(fit19.mqgam, c(.1,.9)[i], predict)\n              hat19[[i+2]] &lt;- qdo(wtd19.mqgam, c(.1,.9)[i], predict) }\n  ## NB, can replace `predict` by `plot`, or `summary`\ncolnames(hat19) &lt;- c(paste0(rep(c('q','qwt'),c(2,2)), rep(c('10','90'),2)))\nhat19 &lt;- cbind(hat19, logFert=wdi[,\"logFert\"])\n\n\n## Panel A: Fit with SE limits, 50% quantile\ngphA &lt;- xyplot(lo+fit+hi~logFert, data=hat50, lty=c(2,1,2),lwd=1.5,type='l') +\n  latticeExtra::as.layer(xyplot(LifeExpectancy~logFert,\n                                data=hat50, pch='.', cex=2))\n## Panel B: Multiple quantiles; unweighted and weighted fits\ngph19 &lt;- xyplot(q10+q90+qwt10+qwt90 ~ logFert, type=\"l\",\n                data=hat19, lty=rep(1:2,c(2,2)),lwd=1.5)\ngphB &lt;- xyplot(LifeExpectancy ~ logFert, data=wdi) + as.layer(gph19)\nupdate(c(\"A: 50% curve, 2 SE limits\"=gphA, \"B: 0.1, 0.9 quantiles\"=gphB,\n         x.same=T, y.same=T), between=list(x=0.5),\n       xlab=\"Fertility Rate\", ylab=\"Life Expectancy\",\n       scales=list(x=list(at=log(2^((0:5)/2)), labels=round(2^((0:5)/2),1)),\n                   alternating=F),\n       par.settings=DAAG::DAAGtheme(color=F, col='gray50', cex=2, pch='.'))\n\n\n## Plots for the individual quantiles can be obtained thus:\n## ## Panel A\nplot(fit.qgam, shift=mean(predict(fit.qgam)))\n## Panel B, 10% quantile\nfitm10 &lt;- qdo(fit19.mqgam, qu=0.1)\nplot(fitm10, resid=T, shift=mean(predict(fitm10)),\n     ylim=range(wdi$LifeExpectancy), cex=2)\nwfitm10 &lt;- qdo(wtd19.mqgam, qu=0.1)\nplot(wfitm10, resid=T, shift=mean(predict(wfitm10)),\n     ylim=range(wdi$LifeExpectancy), cex=2)\n\n\n\n\nSection 4.6: Further reading and remarks\n\n\nExercises (4.7)\n4.2\n\nroller.lm &lt;- lm(depression~weight, data=DAAG::roller)\nroller.lm2 &lt;- lm(depression~weight+I(weight^2), data=DAAG::roller)\n\n4.4\n\ntoycars &lt;- DAAG::toycars\nlattice::xyplot(distance ~ angle, groups=factor(car), type=c('p','r'),\n                data=toycars, auto.key=list(columns=3))\n\n4.4a\n\nparLines.lm &lt;- lm(distance ~ 0+factor(car)+angle, data=toycars)\nsepLines.lm &lt;- lm(distance ~ factor(car)/angle, data=toycars)\n\n4.4b\n\nsepPol3.lm &lt;- lm(distance ~ factor(car)/angle+poly(angle,3)[,2:3], data=toycars)\n\n4.4c\n\nsapply(list(parLines.lm, sepLines.lm, sepPol3.lm), AICcmodavg::AICc)\n\n4.4e\n\nsetNames(sapply(list(parLines.lm, sepLines.lm, sepPol3.lm),\n  function(x)summary(x)$adj.r.squared), c(\"parLines\",\"sepLines\",\"sepPol3\"))\n\n4,7\n\nseedrates.lm &lt;- lm(grain ~ rate + I(rate^2), data=seedrates)\nseedrates.pol &lt;- lm(grain ~ poly(rate,2), data=seedrates)\n\n4.10a\n\ngeo.gam &lt;- gam(thickness ~ s(distance), data=DAAG::geophones)\n\n4.11\n\nplot(DAAG::geophones$distance, acf(resid(geo.gam), lag.max=55)$acf)\nBox.test(resid(geo.gam), lag=10)\nBox.test(resid(geo.gam), lag=20)\nBox.test(resid(geo.gam), lag=20, type=\"Ljung\")\n\n4.15\n\nlibrary(mgcv)\nxy &lt;- data.frame(x=1:200, y=arima.sim(list(ar=0.75), n=200))\ndf.gam &lt;- gam(y ~ s(x), data=xy)\nplot(df.gam, residuals=TRUE)\n\n4.16\n\nlibrary(mgcViz)\nohms.tpBIC &lt;- gam(kohms ~ s(juice, bs=\"tp\"), data=fruitohms, \n                  gamma=log(nrow(fruitohms))/2, method=\"REML\")\nohms.gamViz &lt;- mgcViz::getViz(ohms.tpBIC)   # Convert to a `gamViz` object              \ng1 &lt;- plot(sm(ohms.gamViz, 1))  # Graphics object for term 1 (of 1)\ng1 + l_fitLine(colour = \"red\") + l_rug(mapping = aes(x=x, y=y), alpha = 0.4) +\n     l_ciLine(mul = 2, colour = \"blue\", linetype = 2) +  # Multiply SE by `mul`\n     l_points(shape = 19, size = 1, alpha = 0.5)\n\n4.16a\n\nplot(sm(ohms.gamViz, 1), nsim = 20) + l_ciLine() + l_fitLine() + l_simLine()\n\n4.16b\n\ngam(Gas ~ Insul+s(Temp, by=Insul), data=whiteside) |&gt; \n   getViz() -&gt; gas.gamViz\nplot(sm(gas.gamViz,1), nsim = 20) + l_ciLine() + l_fitLine() + l_simLine()\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/ch4.R\")\n}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Exploiting the linear model framework</span>"
    ]
  },
  {
    "objectID": "ch5.html",
    "href": "ch5.html",
    "title": "5  Chapter 5: Generalized linear models and survival analysis",
    "section": "",
    "text": "Packages required (with dependencies)\nDAAG car mgcv colorspace HistData gamlss dplyr tidyr MASS ggplot2 latticeExtra qgam VGAM survival HistData\nAdditionally, knitr is required in order to process the Rmd source file.\n\n\nSection 5.1 Generalized linear models\n\nSubsection 5.1.1: Linking the expected value to the covariate\n\n## Simplified plot showing the logit link function\np &lt;- (1:39)/40\nlogitp &lt;- log(p/(1 - p))\nplot(p, logitp, xlab = \"Proportion\", ylab = \"logit(p)\", type = \"l\", pch = 1)\n\n\npar(las=0)\np &lt;- seq(from=1, to=99, by=1)/100; n&lt;- 150; eps=0.001\ngitp &lt;- log(p/(1 - p))\nplot(p, gitp, xlab = \"\", ylab = \"\", type = \"l\", pch = 1,\nlas=1, xlim=0:1, xaxs=\"i\", fg=\"gray\")\nmtext(side = 1, line = 1.75, expression(\"Proportion \"*pi))\nmtext(side = 2, line = 1.75,\nexpression(\"logit(\"*pi*\") = log(Odds)\"))\nmtext(side = 3, line = 0.5, \"A: Logit link\", adj=0, cex=1.0)\npval &lt;- c(0.001, 0.01, 0.1, 0.5, 0.9, 0.99, 0.999)\npar(mgp = c(2.5, 0.5, 0))\n##  axis(1, at=c(0,1), lwd=0, labels=c(0,1), xpd=TRUE)\naxis(4, adj=0.075, at = log(pval/(1 - pval)), las=1,\ncol=\"gray\", labels = paste(pval), lwd=0, lwd.ticks=1)\nseP &lt;- sqrt(p*(1-p)/100)\nplot(p, seP, xlab = \"\", ylab = \"\", type = \"l\", pch = 1,\nlas=1, xlim=0:1, xaxs=\"i\", fg=\"gray\")\n##  axis(1, at=c(0,1), lwd=0, labels=c(0,1), xpd=TRUE)\nmtext(side = 1, line = 1.75, expression(\"Proportion \"*pi))\nmtext(side = 2, line = 2.25, expression(\"SD[\"*p*\"], \"*n*\"=100\"))\nmtext(side = 3, line = 0.5,\nexpression(\"B: SD[\"*p*\"], \"*n*\"=100\"), adj=0, cex=1.0)\nseLP &lt;- (p*(1-p)/n)*((p+eps)*(1-p+eps))^-2\nplot(p, seLP, xlab = \"\", ylab = \"\", type = \"l\", pch = 1,\nlas=1, xlim=0:1, xaxs=\"i\", fg=\"gray\")\n##  axis(1, at=c(0,1), lwd=0, labels=c(0,1), xpd=TRUE)\nmtext(side = 1, line = 1.75, expression(\"Proportion \"*pi))\nmtext(side = 2, line = 1.75, expression(\"SD[logit(\"*p*\")], \"*n*\"=100\"))\nmtext(side = 3, line = 0.5, \"C: SD[logit(p)]\", adj=0, cex=1.0)\n\n\n\nSubsection 5.1.2: Noise terms need not be normal\n\n\nSubsection 5.1.3: Variation that is greater than binomial or Poisson\n\nLeast squares versus logistic regression\n\n\n\nSubsection 5.1.4: Log odds in contingency tables\n\n\nSubsection 5.1.5: Logistic regression with a continuous explanatory variable\n\nanestot &lt;- aggregate(DAAG::anesthetic[, c(\"move\",\"nomove\")],\nby=list(conc=DAAG::anesthetic$conc), FUN=sum)\n## The column 'conc', because from the 'by' list, is then a factor.\n## The next line recovers the numeric values\nanestot$conc &lt;- as.numeric(as.character(anestot[[\"conc\"]]))\nanestot$total &lt;- apply(anestot[, c(\"move\",\"nomove\")], 1 , sum)\nanestot$prop &lt;- anestot$nomove/anestot$total\n\n\npar(mgp=c(2.5,.5,0))\nanesthetic &lt;- DAAG::anesthetic\nz &lt;- table(anesthetic$nomove, anesthetic$conc)\ntot &lt;- apply(z, 2, sum)\nprop &lt;- z[2,  ]/(tot)\noprop &lt;- sum(z[2,  ])/sum(tot)\nconc &lt;- as.numeric(dimnames(z)[[2]])\npar(las=0)\nplot(conc, prop, xlab = \"Concentration\", ylab = \"Proportion\",\n     xlim=c(0.5, 2.5), ylim = c(0, 1), pch = 16, axes=F)\naxis(1, cex=0.9, lwd=0, lwd.ticks=1)\naxis(2, at=c(0, 0.5, 1.0), cex=0.9, lwd=0, lwd.ticks=1)\naxis(2, at=c(0.25, 0.75), cex=0.9, lwd=0, lwd.ticks=1)\nbox(col=\"gray\")\nchh &lt;- par()$cxy[2]\nchw &lt;- par()$cxy[1]\ntext(conc - 0.3 * chw, prop-sign(prop-0.5)*chh/4, paste(tot),\nadj = 1, cex=0.65)\nabline(h = oprop, lty = 2)\n\n\n## Fit model directly to the 0/1 data in nomove\nanes.glm &lt;- glm(nomove ~ conc, family=binomial(link=\"logit\"),\n                data=DAAG::anesthetic)\n## Fit model to the proportions; supply total numbers as weights\nanes1.logit &lt;- glm(prop ~ conc, family=binomial(link=\"logit\"),\n                  weights=total, data=anestot)\n\n\nDAAG::sumry(anes.glm, digits=2)\n\n\nA note on model output\n\n## Tp get coefficients, SEs, and associated statistics, specify:\nprint(coef(summary(anes.glm)), digits=2)\n## Get full default output\nsummary(anes.glm, digits=2)\n\n\n\n\n\nSection 5.2 Logistic multiple regression\n\nfrogs &lt;- DAAG::frogs\n\n\n## Presence/absence information: data frame frogs (DAAGS)\nsuppressMessages(library(ggplot2))\np &lt;- ggplot(frogs, aes(easting, northing)) +\n  geom_point(size=3, alpha=0.25) + coord_fixed() +\n  xlab(\"Meters east of reference point\")+ylab(\"Meters north\") +\n  theme(axis.title=element_text(size=11), axis.text=element_text(size=8))\np + geom_point(data=subset(frogs, pres.abs==1),\n               aes(easting, northing), alpha=1, shape=3, col=\"white\", size=1.5)\n\n\nfrogs &lt;- within(frogs, {maxSubmin &lt;- meanmax-meanmin\n                        maxAddmin &lt;- meanmax+meanmin})\n\n\nImplications for the Variance Inflation Factor\n\n\nSubsection 5.2.1: Choose explanatory terms, and fit model\n\n## Find power transformations\nuseCols &lt;- c('distance','NoOfPools','NoOfSites','avrain','maxAddmin','maxSubmin')\ntfrogs &lt;- car::powerTransform(frogs[,useCols], family=\"yjPower\")\n## Create, for later use, a matrix with variables transformed as suggested\ntransY &lt;- car::yjPower(frogs[,useCols], coef(tfrogs, round=TRUE))\nsummary(tfrogs, digits=2)\n\n\nfrogs0.glm &lt;- glm(formula = pres.abs ~ log(distance) + log(NoOfPools)+\n                  sqrt(NoOfSites) + avrain + maxAddmin + maxSubmin,\n                  family = binomial, data = frogs)\nDAAG::sumry(frogs0.glm, digits=1)\n\n\n## Check effect of omitting sqrt(NoOfSites) and avrain from the model\n## ~ . takes the existing formula. Precede terms to be\n## omitted by '-'.  (For additions, precede with '+')\nfrogs.glm &lt;- update(frogs0.glm, ~ . -sqrt(NoOfSites)-avrain)\nfrogsAlt.glm &lt;- update(frogs.glm, ~ . -maxAddmin+altitude)\nAIC(frogs0.glm, frogs.glm,frogsAlt.glm)\n\n\nrbind(\n'frogs0.glm'=coef(frogs0.glm)[c('log(distance)','log(NoOfPools)','maxAddmin','maxSubmin')],\n'frogs.glm'=coef(frogs.glm)[c('log(distance)','log(NoOfPools)','maxAddmin','maxSubmin')]\n)\ncoef(frogsAlt.glm)[c('log(distance)','log(NoOfPools)','altitude','maxSubmin')]\n\n\ncoef(summary(frogs.glm))\n\n\n\nSubsection 5.2.2: Fitted values\n\n## Use of `predict()` and `fitted()` --- examples\nfitted(frogs.glm)    # Fitted values' scale of response\npredict(frogs.glm, type=\"response\")  # Same as fitted(frogs.glm)\npredict(frogs.glm, type=\"link\")      # Scale of linear predictor\n## For approximate SEs, specify\npredict(frogs.glm, type=\"link\", se.fit=TRUE)\n\n\nlibrary(ggplot2)\nfrogs$Prob. &lt;- fitted(frogs.glm)\nfrogs$presAbs &lt;- factor(frogs$pres.abs)\np &lt;- ggplot(frogs, aes(easting, northing, color=Prob.)) +\n  geom_point(size=2, alpha=0.5) + coord_fixed() +\n  xlab(\"Meters east of reference point\")+ylab(\"Meters north\") +\n   theme(axis.title=element_text(size=9), axis.text=element_text(size=6))\np2 &lt;- p+scale_color_gradientn(colours=colorspace::heat_hcl(10,h=c(0,-100),\n                              l=c(75,40), c=c(40,80), power=1)) +\n  guides(fill=guide_legend(title=NULL))\np2 + geom_point(data=subset(frogs, presAbs==1),\n                aes(easting, northing), alpha=1, shape=3, col=\"white\", size=1)\n\n\n\nSubsection 5.2.3: Plots that show the contributions of explanatory variables\n\nopar &lt;- par(mgp=c(2.1,.4,0), mfrow=c(1,3))\nCholera &lt;- HistData::Cholera\nfitP2.glm &lt;- glm(cholera_deaths ~ offset(log(popn)) + water +\n                 log(elevation+3) + poly(poor_rate,2) +I(elevation==350),\n                 data=Cholera, family=quasipoisson)\nCholera[[\"water\"]] &lt;- factor(Cholera[[\"water\"]], labels=c(\"Battersea\",\n                             \"NewRiver\",\"Kew\"))\ntermplot(fitP2.glm, partial=T, se=TRUE, pch =1,\nylabs=rep(\"Partial residual\",3), terms='water', fg=\"gray\")\naxis(1, at=2, labels=\"NewRiver\", lwd=0, line=0.75)\ntermplot(fitP2.glm, partial=T, se=TRUE, pch =1,\n         ylabs=rep(\"Partial residual\",3), terms='log(elevation + 3)', fg=\"gray\")\ntermplot(fitP2.glm, partial=T, se=TRUE, pch =1,\n         ylabs=rep(\"Partial residual\",3), terms='poly(poor_rate, 2)', fg=\"gray\")\npar(opar)\n\n\n\nSubsection 5.2.4: Cross-validation estimates of predictive accuracy\n\nDAAG::CVbinary(frogs.glm)\n\n\nset.seed(19)\nfrogs.acc &lt;- frogs0.acc &lt;- numeric(6)\nfor (j in 1:6){\n  randsam &lt;- sample(1:10, 212, replace=TRUE)\n  ## Sample 212 values (one per pbservation) from 1:10\n  frogs.acc[j] &lt;- DAAG::CVbinary(frogs.glm, rand=randsam,\n                                print.details=FALSE)$acc.cv\n  frogs0.acc[j] &lt;- DAAG::CVbinary(frogs0.glm, rand=randsam,\n  print.details=FALSE)$acc.cv\n}\nprint(rbind(\"frogs (all variables)\" = frogs.acc,\n            \"frogs0 (selected variables)\" = frogs0.acc), digits=3)\n\n\n\nSubsection 5.2.5: Cholera deaths in London — 1849 to 1855\n\nBy air, or by water — the 1849 epidemic\n\nopar &lt;- par(mgp=c(2.1,.4,0), mfrow=c(1,3))\nCholera &lt;- HistData::Cholera\nfitP2.glm &lt;- glm(cholera_deaths ~ offset(log(popn)) + water +\n                 log(elevation+3) + poly(poor_rate,2) +I(elevation==350),\n                 data=Cholera, family=quasipoisson)\nCholera[[\"water\"]] &lt;- factor(Cholera[[\"water\"]], labels=c(\"Battersea\",\n                             \"NewRiver\",\"Kew\"))\ntermplot(fitP2.glm, partial=T, se=TRUE, pch =1,\nylabs=rep(\"Partial residual\",3), terms='water', fg=\"gray\")\naxis(1, at=2, labels=\"NewRiver\", lwd=0, line=0.75)\ntermplot(fitP2.glm, partial=T, se=TRUE, pch =1,\n         ylabs=rep(\"Partial residual\",3), terms='log(elevation + 3)', fg=\"gray\")\ntermplot(fitP2.glm, partial=T, se=TRUE, pch =1,\n         ylabs=rep(\"Partial residual\",3), terms='poly(poor_rate, 2)', fg=\"gray\")\npar(opar)\n\n\n\nThe 1854 epidemic — a natural experiment\n\n\n\n\nSection 5.3 Logistic models for categorical data – an example\n\n## Create data frame from multi-way table UCBAdmissions (datasets)\n## dimnames(UCBAdmissions)  # Check levels of table margins\nUCB &lt;- as.data.frame.table(UCBAdmissions[\"Admitted\", , ], responseName=\"admit\")\nUCB$reject &lt;- as.data.frame.table(UCBAdmissions[\"Rejected\", , ])$Freq\nUCB$Gender &lt;- relevel(UCB$Gender, ref=\"Male\")\n## Add further columns total and p (proportion admitted)\nUCB$total &lt;- UCB$admit + UCB$reject\nUCB$pAdmit &lt;- UCB$admit/UCB$total\n\n\nUCB.glm &lt;- glm(pAdmit ~ Dept*Gender, family=binomial, data=UCB, weights=total)\n## Abbreviated `anova()` output:\nanova(UCB.glm, test=\"Chisq\") |&gt;\n capture.output() |&gt; tail(8) |&gt; (\\(x)x[-c(2,3)])() |&gt; cat(sep='\\n')\n\n\nround(signif(coef(summary(UCB.glm)),4), 3)\n\n\n\nSection 5.4 Models for counts — poisson, quasipoisson, and negative binomial\n\nSubsection 5.4.1: Data on aberrant crypt foci\n\npar(pty=\"s\")\nplot(count ~ endtime, data=DAAG::ACF1, pch=16, fg=\"gray\")\n\n\nACF.glm &lt;- glm(formula = count ~ endtime + I(endtime^2),\n               family = poisson(link=\"identity\"), data = DAAG::ACF1)\nDAAG::sumry(ACF.glm, digits=2)\n\n\nunique(round(predict(ACF.glm),2))\n\n\nsum(resid(ACF.glm, type=\"pearson\")^2)/19\n\n\nACFq.glm &lt;- glm(formula = count ~ endtime + I(endtime^2),\nfamily = quasipoisson, data = DAAG::ACF1)\nprint(coef(summary(ACFq.glm)), digits=2)\n\n\nsapply(split(residuals(ACFq.glm), DAAG::ACF1$endtime), var)\n\n\nfligner.test(resid(ACFq.glm) ~ factor(DAAG::ACF1$endtime))\n\n\n\nSubsection 5.4.2: Moth habitat example\n\n## Number of moths by habitat: data frame DAAG::moths\nmoths &lt;- DAAG::moths\ntab &lt;- rbind(Number=table(moths[, 4]),\n             sapply(split(moths[, -4], moths$habitat), apply, 2, sum))\n\n\n## Number of zero counts, by habitats\nwith(droplevels(subset(moths, A==0)), table(habitat))\n\n\nlibrary(lattice)\ngph &lt;- dotplot(habitat ~ A+P, data=DAAG::moths, xlab=\"Number of moths\", outer=TRUE,\n               strip=strip.custom(factor.levels=paste(\"Number of species\",c(\"A\",\"B\"))),\n               panel=function(x, y, ...){\npanel.dotplot(x,y, pch=1, ...)\nav &lt;- sapply(split(x,y),mean)\nypos &lt;- factor(names(av), levels=names(av))\nlpoints(ypos~av, pch=3, col=\"gray45\", cex=1.25)\n},\nkey=list(text=list(c(\"Individual transects\", \"Mean\")),\npoints=list(pch=c(1,3), cex=c(1,1.25), col=c(\"black\",\"gray45\")),\ncolumns=2), scales=list(tck=0.5, alternating=1))\nbw9 &lt;- list(fontsize=list(text=9, points=5))\nupdate(gph, par.settings=bw9)\n\n\nAstats &lt;- with(DAAG::moths, sapply(split(A, habitat),\nfunction(x)c(Amean=mean(x),Avar=var(x))))\navlength &lt;- with(DAAG::moths, sapply(split(meters, habitat), mean))\nround(rbind(Astats, avlen=avlength),1)\n\n\nA quasipoisson model\n\nA.glm &lt;- glm(A ~ habitat + log(meters), family=quasipoisson,\ndata=DAAG::moths)\nDAAG::sumry(A.glm, digits=1)\n\n\nsubset(DAAG::moths, habitat==\"Bank\")\n\n## Analysis with tighter convergence criterion\nA.glm &lt;- update(A.glm, epsilon=1e-10)\nprint(coef(summary(A.glm)), digits=2)\n\nAfitSE &lt;- predict(A.glm, se=TRUE)$se.fit\ncfSE &lt;- with(DAAG::moths, c(AfitSE[habitat==\"Bank\"],\nrange(AfitSE[habitat!=\"Bank\"])))\nround(setNames(cfSE, c(\"SEbank\", \"SEotherMIN\", \"SEotherMAX\")), digits=2)\n\n\n\nA more satisfactory choice of reference level\n\nmoths &lt;- DAAG::moths\nmoths$habitat &lt;- relevel(moths$habitat, ref=\"Lowerside\")\nAlower.glm &lt;- glm(A ~ habitat + log(meters),\n                  family = quasipoisson, data = moths)\nprint(coef(summary(Alower.glm)), digits=1)\n\n\n\n\nSubsection 5.4.3: Models with negative binomial errors\n\ndframe &lt;- data.frame(sigma1A =(Astats[2,]-Astats[1,])/Astats[1,]^2,\nsigma2A =(Astats[2,]-Astats[1,])/Astats[1,]^1,\nmu = Astats[1,], habitat=colnames(Astats))\nbw9 &lt;- list(fontsize=list(text=9, points=5), pch=1:7)\nxyplot(sigma1A+sigma2A ~ mu, groups=habitat, outer=TRUE,\ndata=subset(dframe,habitat!=\"Bank\"),\npar.settings=bw9, auto.key=list(columns=4),\nstrip=strip.custom(factor.levels=paste(\"Model\",c(\"NBI\",\"NBII\"))),\nxlab=\"Mean number of species A moths\",\nylab=expression(\"Estimate of \"*sigma))\n\n\nlibrary(gamlss, quietly=TRUE)\nnoBank &lt;- subset(moths, habitat!='Bank')\nmothsCon.lss &lt;- gamlss(A ~ log(meters)+habitat, family=NBI(), data=noBank,\n                       trace=F)\nmothsVary.lss &lt;- gamlss(A ~ log(meters)+habitat, family=NBI(),\n                        sigma.formula=~habitat, trace=FALSE, data=noBank)\n\n\nLR.test(mothsCon.lss, mothsVary.lss)\n\n\n## mothsCon.lss &lt;- gamlss(A ~ log(meters)+habitat,family=NBI(),data=noBank)\n## summary(mothsCon.lss, type=\"qr\")   ## Main part of output\n\n\nDiagnostic plots\n\nplot(mothsCon.lss, panel=panel.smooth)\n\n\n\nUse of the square root link function\n\nAsqrt.lss &lt;- gamlss(A ~ habitat + sqrt(meters), trace=FALSE,\n                    family = NBI(mu.link='sqrt'), data = moths)\n\n\n## Asqrt.lss &lt;- gamlss(A ~ habitat + sqrt(meters),\n##                     family = NBI(mu.link='sqrt'), data = moths)\n## summary(Asqrt.lss, type=\"qr\")   ## Main part of output\nout &lt;- capture.output(summary(Asqrt.lss, digits=1))[-(3:10)]\ncat(out, sep=\"\\n\")\n\n\n\n\nSubsection 5.4.4: Negative binomial versus alternatives — hurricane deaths\n\nAside – a quasibinomial binomial fit\n\nordx &lt;- with(DAAG::hurricNamed, order(BaseDam2014))\nhurric &lt;- DAAG::hurricNamed[ordx,]\n# Ordering a/c values of BaseDam2014 simplifies later code\nhurr.glm &lt;- glm(deaths ~ log(BaseDam2014), family=quasipoisson, data=hurric)\nplot(hurr.glm, col=adjustcolor('black', alpha=0.4),\n     cex.caption=0.95, sub.caption=rep(\"\",4), fg=\"gray\")\n\n\n\nNegative binomial versus power transformed scale\n\n\nFit a negative binomial (NBI) model\n\nlibrary(gamlss)\nhurrNB.gamlss &lt;- gamlss::gamlss(deaths ~ log(BaseDam2014), family=NBI(),\n                                data=hurric[-56,])\nmures &lt;- resid(hurrNB.gamlss, what=\"mu\")\nzres &lt;- resid(hurrNB.gamlss, what=\"z-scores\")  ## equivalent normal quantiles\n\n\ntable(sign(mures))\n\n\n\nFit linear model to power transformed response\n\nhurr.lm &lt;- lm(car::yjPower(deaths,-0.2) ~ log(BaseDam2014), data=hurric[-56,])\n## Use the following function to transform from power scale to log scale\npowerTOlog &lt;- function(z, lambda)log(lambda*z+1)/lambda\n## Calculate fitted values, and transform to log(deaths+1) scale\nhatPower &lt;- powerTOlog(predict(hurr.lm), lambda=-0.2)\nresPower &lt;- log(hurric[-56,\"deaths\"]+1) - hatPower\n\n\ntable(sign(resPower))\n\n\n\nCompare NBI and power transform fits with smoothed quantiles\n\nlibrary(qgam, quietly=TRUE)\nhat68.8 &lt;- predict(qgam(log(deaths+1) ~ s(log(BaseDam2014)), qu=.648,\n                        data=hurric[-56,]))\nhat40.9 &lt;- predict(qgam(log(deaths+1) ~ s(log(BaseDam2014)), qu=.409,\n                        data=hurric[-56,]))\n\n\nxvar &lt;- log(hurric$BaseDam2014)[-56]\nplot(log(deaths+1) ~ log(BaseDam2014), data=hurric, xaxt=\"n\", yaxt=\"n\",\n  cex=4, pch=\".\", fg=\"gray\", col=adjustcolor(\"black\",alpha.f=0.65),\n  xlab=\"Damage, millions of US$ in 2014\", ylab=\"Deaths\")\naxis(1, at=log(c(1,10,1000, 100000)),\n  labels=paste(c(1,10,1000, 100000)), lwd=0, lwd.ticks=1)\naxis(2, at=log(c(0,10,100,1000)+1),\n  labels=paste(c(0,10,100,1000)), lwd=0, lwd.ticks=1)\n## Negative binomial regression fitted values\nhatNB &lt;- fitted(hurrNB.gamlss)\nlines(xvar, log(hatNB+1), col=\"blue\", lty=2)\nwith(hurric, text(log(BaseDam2014)[56], log(deaths+1)[56], \"Audrey\", pos=3),\n     cex=0.72)\n## Show fit from power transform model\nlines(xvar, hatPower, col=\"blue\", lty=1)\n## Show 68.8\\% and 40.1\\% fits from regression smooths\nlines(hat68.8 ~ xvar, lty=2, col='red')\nlines(hat40.9 ~ xvar, lty=1, col='red')\nlegend(\"topleft\", col=rep(c('blue','red'),c(2,2)), lty=rep(2:1,2), cex=0.8,\n       y.intersp=0.75, legend=c(\"Negative binomial fit\",\"Power transform fit\",\n                                \"68.8% quantile\", \"40.9% quantile\"), bty=\"n\")\nmtext(side=3, \"A: Deaths vs damage\", line=0.5, cex=1.15, adj=0)\n## Quantile-quantile plot -- negative binomial model\nqqnorm(zres, main=\"\", fg=\"gray\", cex=0.5,\n  col=adjustcolor(\"black\",alpha.f=0.65)); qqline(zres, col=2)\nmtext(side=3, \"B: Q-Q plot\", line=0.5, cex=1.15, adj=0)\n\n\n## a) Fitted and empirical centiles from hurrNB.gamlss\npc &lt;- t(centiles.split(hurrNB.gamlss, xvar=log(hurric$BaseDam2014)[-56],\n   cent=c(5,10,25,50,75,90,95), xcut.points=log(c(150, 1500)),\n   plot=FALSE))\nrownames(pc) &lt;- c(\"up to 150M\", \"150M to 1500M\", \"above 1500M\")\nround(pc,2)\n\n\nhurrP.gamlss &lt;- gamlss(car::yjPower(deaths, -0.2) ~ log(BaseDam2014), data=hurric)\n\n\n## Fitted and empirical centiles from hurrP.gamlss\npc &lt;- t(centiles.split(hurrP.gamlss, xvar=log(hurric$BaseDam2014),\ncent=c(5,10,25,50,75,90,95),\nxcut.points=log(c(150, 1500)), plot=FALSE))\nrownames(pc) &lt;- c(\"up to 150M\", \"150M to 1500M\", \"above 1500M\")\nround(pc,2)\n\n\n\n\n\nSection 5.5 Fitting smooths\n\nSubsection 5.5.1: Handedness of first-class cricketers in the UK\n\ntab &lt;- with(DAAG::cricketer, table(left,dead))\ncolnames(tab) &lt;- c('live','dead')\ntab &lt;- cbind(addmargins(tab, margin=2), prop.table(tab, margin=1))\ntab\n\n\nlibrary(mgcv)\nlibrary(latticeExtra)\nDAAG::cricketer |&gt; dplyr::count(year, left, name=\"Freq\") -&gt; handYear\nnames(handYear)[2] &lt;- \"hand\"\nbyYear &lt;- tidyr::pivot_wider(handYear, names_from='hand', values_from=\"Freq\")\nhand.gam &lt;- gam(cbind(left,right) ~ s(year), data=byYear, family=binomial)\nconst &lt;- attr(predict(hand.gam, type='terms'), \"constant\")\n  ## `const` is the mean on the scale of the linear predictor\nplot(hand.gam, shift=const, trans=function(x)exp(x)/(1+exp(x)), ylim=c(.05,.4),\n     xlab=\"\", ylab=\"Proportion lefhanded\", rug=FALSE, fg=\"gray\",\n     main=list(\"Proportion lefthanded, with 2SE limits\",font=1,cex=1.2))\n  ## Add `const`, then apply inverse link function.\n  ## Plots estimated proportions (i.e., on the scale of the response)\nwith(byYear, points(year, I(left/(left+right)), cex=0.8, col=\"gray50\"))\nleftrt.gam &lt;- gam(Freq ~ hand + s(year, by=factor(hand)), data=handYear,\n                  family=poisson)\nleftrt.pred &lt;- predict(leftrt.gam, se=T, type='response')\nhandYear &lt;- cbind(handYear, as.data.frame(leftrt.pred))\ncol2 &lt;- DAAG::DAAGtheme(color=T)$superpose.symbol$col[c(2,2,1)]\ngph.key &lt;- list(space=\"top\", columns=3, lines=list(lty=c(1,2,1), lwd=2, col=col2),\n                text=list(c(\"left\",expression(4.4%*%\"left\"),\"right\")), cex=1.2)\ngph &lt;- xyplot(leftrt.pred$fit ~ year, groups=hand, ylab=list(\"Number born\", cex=1.2),\n              type=\"l\", xlab=\"\", data=handYear, key=gph.key, col=col2[c(3,1)], lwd=2)\ngph1 &lt;- xyplot(Freq~year, groups=hand, data=handYear, col=col2[c(3,1)])\ngph2 &lt;- xyplot(I(4.4*fit) ~ year, data=subset(handYear, hand==\"left\"),\n               type=\"l\", lty=2, lwd=2, col=col2[2])\nupdate(gph+as.layer(gph1)+as.layer(gph2), par.settings=DAAG::DAAGtheme(color=TRUE),\n       scales=list(cex=1.2))\n\n\n\n\nSection 5.6 Additional notes on generalized linear models\n\nSubsection 5.6.1: Residuals, and estimating the dispersion\n\nOther choices of link function for binomial models\n\n\nQuasi models — estimating the dispersion\n\n\n\nSubsection 5.6.2: Standard errors and \\(z\\)- or \\(t\\)-statistics for binomial models\n\nfac &lt;- factor(LETTERS[1:4])\np &lt;- c(73, 30, 11, 2)/500\nn &lt;- rep(500,4)\nround(signif(coef(summary(glm(p ~ fac, family=binomial, weights=n))), 6), 6)\n\n\np &lt;- c(0.001,0.002,(1:99)/100,0.998,0.999)\nfor(i in 1:3){\nlink &lt;- c(\"logit\", \"probit\", \"cloglog\")[i]\nfun &lt;- make.link(link)$linkfun\nx &lt;- fun(p)\nu &lt;- glm(p ~ x, family=binomial(link=link), weights=rep(1000,103))\nif  (i==1)\nplot(x, hatvalues(u), type=\"l\", ylab=\"Leverage\", xaxt=\"n\", fg='gray',\nyaxt=\"n\",\nylim=c(0, 0.0425), yaxs=\"i\", xlab=\"Fitted proportion\") else {\nphat &lt;- predict(u, type=\"response\")\nlines(log(phat/(1-phat)), hatvalues(u), type=\"l\",\ncol=c(\"black\",\"black\",\"gray\")[i], lwd=0.75,\nlty=c(1,2,1)[i])\n}\n}\npos=c(0.001,0.002, 0.005, 0.01,0.02,0.05,0.1,0.25,0.5,0.75,0.9,0.95,0.98,0.99, 0.995, 0.998, 0.999)\nsub1 &lt;- seq(from=1,to=17, by=2)\nsub3 &lt;- seq(from=2,to=16, by=2)\naxis(1, at=log(pos/(1-pos))[sub1], labels=paste(pos)[sub1],\ncex.axis=0.7, lwd=0, lwd.ticks=1)\naxis(3, at=log(pos/(1-pos))[sub3], labels=paste(pos)[sub3],\ncex.axis=0.7, lwd=0, lwd.ticks=1)\naxis(2, at=c(0,.01,.02,.03), cex.axis=.7, lwd=0, lwd.ticks=1)\nlegend(\"topleft\", lty=c(1,2,1),\nlegend=c(\"logit link\", \"probit link\", \"cloglog link\"),\ncol=c(\"black\",\"black\",\"gray\"), bty=\"n\", cex=0.8)\n\n\n\n\nSection 5.7 Models with an ordered categorical or categorical response\n\nlibrary(VGAM)\ninhaler &lt;-  data.frame(freq=c(99,76,41,55,2,13),\n  choice=rep(c(\"inh1\",\"inh2\"), 3),\n  ease=ordered(rep(c(\"easy\",\"re-read\",\"unclear\"), rep(2,3))))\ninhaler1.vglm &lt;-  vglm(ease ~ 1, weights=freq, data=inhaler,\n  cumulative(link=\"logitlink\"), subset=inhaler$choice==\"inh1\")\ninhaler2.vglm &lt;-  vglm(ease ~ 1, weights=freq, data=inhaler,\n  cumulative(link=\"logitlink\"), subset=inhaler$choice==\"inh2\")\n\n\n## Inhaler 1\nround(coef(summary(inhaler1.vglm)),3)\n## Inhaler 2\nround(coef(summary(inhaler2.vglm)),3)\n\n\ninhaler.vglm &lt;-  vglm(ease ~ choice, weights=freq, data=inhaler,\ncumulative(link=\"logitlink\", parallel=FALSE))\nround(coef(summary(inhaler.vglm)),3)\n\n\ninhalerP.vglm &lt;-  vglm(ease ~ choice, weights=freq, data=inhaler,\ncumulative(link=\"logitlink\", parallel=TRUE))\nround(coef(summary(inhalerP.vglm)),3)\n\n\npred &lt;- predict(inhalerP.vglm, se.fit=TRUE, newdata=inhaler[1:2,])\ncolnames(pred$se.fit) &lt;- paste(\"SE\", colnames(pred$se.fit))\nfitvals &lt;- with(pred, cbind(fitted.values, se.fit))\ncolnames(fitvals) &lt;- gsub('link', '', colnames(fitvals))\nround(fitvals, 2)\n\n\nd &lt;- deviance(inhalerP.vglm) - deviance(inhaler.vglm)\n## Refer to chi-squared distribution with 1 degree of freedom\nc(Difference=d, \"p-Value\"=pchisq(3.416, df=1, lower.tail=FALSE))\n\n\nSubsection 5.7.2: Loglinear Models\n\n\n\nSection 5.8 Survival analysis\n\nsuppressMessages(library(survival))\n\n\ndf &lt;- data.frame(x0 = c(1, 5, 1, 2, 14, 10, 12, 19)*30,\nx1 = c(46, 58, 85, 67, 17, 85, 18, 42)*30,\nfail = c(1, 0, 0, 1, 1, 0, 0, 1))\nplot(c(0, 2610), c(0.65, 8.15), type = \"n\",\nxlab = \"Days from beginning of study\",\nylab = \"Subject number\", axes = F)\n##  mtext(side = 1, line = 2.5, \"Days from beginning of study\", adj = 0.5)\nm &lt;- dim(df)[1]\npar(las=2)\naxis(2, at = (1:m), labels = paste((m:1)), lwd=0, lwd.ticks=1)\npar(las=1)\nabline(v = 600, lty = 4, col=\"gray40\")\nabline(v = 2550)\nmtext(side = 3, line = 0.5, at = c(600, 2550),\ntext = c(\"\\nEnd of recruitment\",\n\"\\nEnd of study\"), cex = 0.9)\nlines(rep((0:8) * 300, rep(3, 9)), rep(c(-0.4, -0.2, NA), 9),\nxpd = T)\nmtext(side = 1, line = 1.0, at = (0:8) * 300,\ntext = paste((0:8) * 300), adj = 0.5)\nchw &lt;- par()$cxy[1]\nxx &lt;- as.vector(t(cbind(df[, 1], df[, 2] - 0.25 * chw,\nrep(NA, m))))\nyy &lt;- as.vector(t(cbind(matrix(rep(m:1, 2), ncol = 2),\nrep(NA, m))))\nlines(as.numeric(xx), as.numeric(yy))\npoints(df[, 1], m:1, pch = 16)\ntext(df[, 1]-0.25*chw, m:1, paste(df[,1]), pos=1, cex=0.75)\nfail &lt;- as.logical(df$fail)\npoints(df[fail, 2], (m:1)[fail], pch = 15)\npoints(df[!fail, 2], (m:1)[!fail], pch = 0)\ntext(df[, 2]+0.25*chw, m:1, paste(df[,2]), pos=1, cex=0.75)\npar(xpd=TRUE)\nlegend(0, 11.5, pch = 16, legend = \"Entry\", y.intersp=0.15)\nlegend(1230, 11.5, pch = c(15, 0),\nlegend = c(\"Dead\", \"Censored\"), ncol=2, y.intersp=0.15)\n\n\nSubsection 5.8.1: Analysis of the Aids2 data\n\nstr(MASS::Aids2, vec.len=2)\n\n\nbloodAids &lt;- subset(MASS::Aids2, T.categ==\"blood\")\nbloodAids$days &lt;- bloodAids$death-bloodAids$diag\nbloodAids$dead &lt;- as.integer(bloodAids$status==\"D\")\n\n\nbloodAids &lt;- subset(MASS::Aids2, T.categ==\"blood\")\nbloodAids$days &lt;- bloodAids$death-bloodAids$diag\nbloodAids$dead &lt;- as.integer(bloodAids$status==\"D\")\nplot(survfit(Surv(days, dead) ~ sex, data=bloodAids),\n     col=c(2,4), conf.int=TRUE, lty=1, fg=\"gray\",\n     xlab=\"Days from diagnosis\", ylab=\"Survival probability\")\nlegend(\"top\", legend=levels(bloodAids$sex), lty=c(1,1),\n       col=c(2,4), horiz=TRUE, bty=\"n\")\n\n\n## Pattern of censoring for male homosexuals\nhsaids &lt;- subset(MASS::Aids2, sex==\"M\" & T.categ==\"hs\")\nhsaids$days &lt;- hsaids$death-hsaids$diag\nhsaids$dead &lt;- as.integer(hsaids$status==\"D\")\ntable(hsaids$status,hsaids$death==11504)\n\n\nhsaids &lt;- subset(MASS::Aids2, sex==\"M\" & T.categ==\"hs\")\nhsaids$days &lt;- hsaids$death-hsaids$diag\nhsaids$dead &lt;- as.integer(hsaids$status==\"D\")\nhsaids.surv &lt;- survfit(Surv(days, dead) ~ 1, data=hsaids)\nplot(hsaids.surv, col=\"gray\", conf.int=F, tcl=-0.4, fg=\"gray\")\npar(new=TRUE)\nplot(hsaids.surv,col=1, conf.int=F,mark.time=F, fg=\"gray\",\nxlab=\"Days from diagnosis\", ylab=\"Estimated survival probabality\")\nchw &lt;- par()$cxy[1]\nchh &lt;- par()$cxy[2]\nsurv &lt;- hsaids.surv$surv\nxval &lt;- c(200,700,1400,1900)\nhat &lt;- approx(hsaids.surv$time, surv, xout=xval)$y\nfor(i in 1:2) arrows(xval[i], hat[i], 0, hat[i],\nlength=0.05, col=\"gray\")\nlines(rep(xval[1],2), hat[1:2], col=\"gray\")\n##    lines(rep(xval[3],2), hat[3:4], col=\"gray\")\n## Offset triangle 1\nchw &lt;- par()$cxy[1]\nlines(xval[c(1,2,1,1)]+650, hat[c(2,2,1,2)]+0.2,col=\"gray40\")\nxy1 &lt;- c(mean(xval[c(1,1,2)]), mean(hat[c(1,2,2)]))\narrows(xy1[1], xy1[2], xy1[1]+650, xy1[2]+0.2, col=\"gray40\", length=0.1)\ntext(xval[1]-0.1*chw+650, hat[1]+0.2,\npaste(round(hat[1],2)), col=\"gray20\",cex=0.75, adj=1)\ntext(xval[1]+650-0.1*chw, hat[2]+0.2,\npaste(round(hat[2],2)), col=\"gray20\",cex=0.75, adj=1)\ntext(mean(xval[1:2])+650, hat[2]+0.2-0.5*chh,\npaste(round(diff(xval[1:2]))), col=\"gray20\", cex=0.75)\ntext(xval[1]+650-0.5*chw, mean(hat[1:2]+0.2), paste(round(hat[1]-hat[2],3)),\nsrt=90, adj=0.5, col=\"gray20\", cex=0.75)\n\n\n\nSubsection 5.8.4: Hazard rates\n\n\nSubsection 5.8.5: The Cox proportional hazards model\n\nbloodAids.coxph &lt;- coxph(Surv(days, dead) ~ sex, data=bloodAids)\nprint(summary(bloodAids.coxph), digits=6)\n\n\n## Add `age` as explanatory variable\nbloodAids.coxph1 &lt;- coxph(Surv(days, dead) ~ sex+age, data=bloodAids)\n\n\nbloodAids &lt;- subset(MASS::Aids2,T.categ==\"blood\")\nbloodAids &lt;- within(bloodAids, {days &lt;- death-diag\ndead &lt;- as.integer(status==\"D\")})\nbloodAids.coxph &lt;- coxph(Surv(days, dead) ~ sex, data = bloodAids)\nplot(cox.zph(bloodAids.coxph), cex=0.75, bty=\"n\")\nbox(col=\"gray\")\n\n\ncox.zph(bloodAids.coxph)\n\n\ncricketer &lt;- DAAG::cricketer\nkia4.coxph &lt;- coxph(Surv(life, kia) ~ left/poly(year,4),\n                    data = cricketer, model=T)\nkia6.coxph &lt;- update(kia4.coxph, . ~ left/poly(year,6),\n                     data = cricketer, model=T)\n# Type `plot(cox.zph(kia6.coxph)` to plot the two graphs\n# Perhaps check also `AIC(kia4.coxph, kia6.coxph)`\ncox.zph(kia6.coxph)\n\n\nplot(cox.zph(kia6.coxph), cex=0.75, bty=\"n\")\nbox(col=\"gray\")\n\n\n\n\nSection 5.9: Transformations for proportions and counts\n\n\nSection 5.10: Further reading\n\n\nExercises (5.11)\n5.1\n\ninhibition &lt;- rbind(\nconc =c(0.1,0.5, 1,10,20,30,50,70,80,100,150),\nno  = c(7,  1, 10, 9, 2, 9, 13, 1, 1,  4,  3),\nyes = c(0,  0, 3, 4, 0, 6, 7, 0, 0,  1,  7)\n)\ncolnames(inhibition) &lt;- rep(\"\", ncol(inhibition))\ninhibition\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/ch5.R\")\n}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 5: Generalized linear models and survival analysis</span>"
    ]
  },
  {
    "objectID": "ch6.html",
    "href": "ch6.html",
    "title": "6  Chapter 6: Time series models",
    "section": "",
    "text": "Packages required (plus any dependencies)\nDAAG ggsci latticeExtra ggplot2 mice car forecast mgcv tseries\nAdditionally, knitr and Hmisc are required in order to process the Rmd source file.\n\n\nSection 6.1: Time series – some basic ideas\n\nSubsection 6.1.1: Time series objects\n\nclass(\"lakeHuron\")\n\n[1] \"character\"\n\n## Use `time()` to extract the `time` attribute\nrange(time(LakeHuron))\n\n[1] 1875 1972\n\n## Use `window()` to subset a time series\nLHto1925 &lt;- window(LakeHuron, from=1875, to=1925)\n\n\njobs &lt;- DAAG::jobs\nnames(jobs)\n\n[1] \"BC\"       \"Alberta\"  \"Prairies\" \"Ontario\"  \"Quebec\"   \"Atlantic\" \"Date\"    \n\nallRegions &lt;- ts(jobs[, -7])   # Create multivariate time series\ntime(allRegions)               # Times run from 1 to 24\n\nTime Series:\nStart = 1 \nEnd = 24 \nFrequency = 1 \n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n\nallRegions &lt;- ts(jobs[, -7], start=c(1995,1), frequency=12)\nallRegions[,\"BC\"]              # Extract jobs data for British Columbia\n\n      Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec\n1995 1752 1737 1765 1762 1754 1759 1766 1775 1777 1771 1757 1766\n1996 1786 1784 1791 1800 1800 1798 1814 1803 1796 1818 1829 1840\n\njobsBC &lt;- ts(jobs[, \"BC\"], start=c(1995,1), frequency=12)\n  # Obtain equivalent of `allRegions[,\"BC\"]` directly from `jobs` dataset\n\n\n\nSubsection 6.1.2: Preliminary graphical exploration\n\n\n\n\n\n\n\n\n\n\n## Plot depth measurements: ts object LakeHuron (datasets)\nplot(LakeHuron, ylab=\"depth (in feet)\", xlab = \"Time (in years)\", fg=\"gray\")\n\n\nlag.plot(LakeHuron, lags=3, do.lines=FALSE)\n\n\n\nSubsection 6.1.3: The autocorrelation and partial autocorrelation function\n\n\n\n\n\n\n\n\n\n\npar(oma=c(0,0,1.5,0))\npar(pty=\"s\")\nlag.plot(LakeHuron, set.lags=1:4,do.lines=F, oma=c(0,1.5,1.5,1.5),\nfg=\"gray\", layout=c(1,4), cex.lab=1.15, asp=1)\nmtext(side=3, line=0.5, \"A: Lag plots\", adj=0, cex=0.85, outer=TRUE)\n\n\n\n\n\n\n\n\n\n\n\nlibrary(lattice)\ncol3 &lt;- c(\"gray80\",rev(ggsci::pal_npg()(2)))\nlag.max &lt;- 15\noffset &lt;- 0.18\nci95 &lt;- 2/sqrt(length(LakeHuron))\nar2 &lt;- ar(LakeHuron)\ngph.key &lt;- list(x=0.975, y=0.965, corner=c(1,1), columns=1, cex=0.85,\n                 text=list(c(\"Lake Huron data\",\"AR1 process\",\"AR2 process\")),\n                 lines=list(lwd=c(3,1.5,1.5), col=col3,lend=2),\n                 padding.text=1)\nparsetBC &lt;- list(fontsize=list(text=8, points=5), \n                 superpose.line=list(col=col3, lty=rep(1,3),\n                 lwd=c(3,1.5,1.5)))\nparsetBC &lt;- modifyList(parsetBC,list(grid.pars = list(lineend = \"butt\")))\nlev3 &lt;- factor(c(\"acfData\",\"acfAR1\",\"acfAR2\"),\n               levels=c(\"acfData\",\"acfAR1\",\"acfAR2\"))\nacfData &lt;- acf(LakeHuron, main=\"\", plot=FALSE, lag.max=lag.max)$acf\npacfData &lt;- pacf(LakeHuron, main=\"\", plot=FALSE, lag.max=lag.max)$acf\nacfAR1 &lt;- ARMAacf(ar=0.8, lag.max=lag.max)\nacfAR2 &lt;- ARMAacf(ar=ar2$ar, ma=0, lag.max=lag.max)\npacfAR1 &lt;- ARMAacf(ar=0.8, lag.max=lag.max, pacf=TRUE)\npacfAR2 &lt;- ARMAacf(ar=ar2$ar, ma=0, lag.max=lag.max, pacf=TRUE)\nxy &lt;- data.frame(acf=c(acfData,acfAR1,acfAR2),\nLag=c(rep(0:lag.max,3))+rep(c(0,-offset,offset),\nrep(lag.max+1,3)),\ngp=rep(lev3, rep(lag.max+1,3)))\ngphB &lt;- xyplot(acf ~ Lag, data = xy, groups=gp, type=c(\"h\"),\n              par.strip.text = list(cex = 0.85), lend=2,origin=0, \n              ylim=c(-0.325, 1.04),key=gph.key, par.settings=parsetBC, \n          panel=function(x,y,...){\n            panel.xyplot(x,y,...)\n            panel.abline(h=0, lwd=0.8)\n            panel.abline(h=ci95, lwd=0.8, lty=2)\n            panel.abline(h=-ci95, lwd=0.8, lty=2) } )\nxyp &lt;- data.frame(pacf=c(pacfData,pacfAR1,pacfAR2),\n                  Lag=c(rep(1:lag.max,3))+c(rep(c(0,-offset,offset),\n                  rep(lag.max,3))), gp=rep(lev3, rep(lag.max,3)))\ngphC &lt;- xyplot(pacf ~ Lag, data = xyp, groups=gp, type=c(\"h\"),\n               par.strip.text = list(cex = 0.85), lend=2,\n               ylab = \"Partial correlation\", origin=0, ylim=c(-0.325, 1.04),\n               key=gph.key, par.settings=parsetBC, \n               panel=function(x,y,...){\n                 panel.xyplot(x,y,...)\n                 panel.abline(h=0, lwd=0.8)\n                 panel.abline(h=ci95, lwd=0.8, lty=2)\n                 panel.abline(h=-ci95, lwd=0.8, lty=2) } )\nprint(update(gphB, scales=list(alternating=FALSE, tck=0.5),\n             ylab = \"Autocorrelation\",\n             main=list(\"B: Autocorelation -- Data vs AR processes\", \n                       font=1, x=0, y=0.25, just=\"left\", cex=1)), \n                       pos=c(0,0,0.5,0.9))\nprint(update(gphC, \n        scales=list(x=list(at=c(1,5,10,15)), alternating=FALSE, tck=0.5),\n        ylab = \"Partial autocorrelation\",\n        main=list(\"C: Partial autocorrelation -- Data vs AR processes\", \n                  font=1, x=0, y=0.25, just=\"left\", cex=1)),\n        pos=c(0.5,0,1,0.9),newpage=FALSE)\n\n\nacf(LakeHuron)\n## pacf(LakeHuron) gives the plot of partial autocorrelations\n\n\n\nSubsection 6.1.4: Autoregressive (AR) models\n\nThe AR(1) model\n\n## Yule-Walker autocorrelation estimate\nLH.yw &lt;- ar(LakeHuron, order.max = 1, method = \"yw\")  # autocorrelation estimate\n# order.max = 1 for AR(1) model\nLH.yw$ar                  # autocorrelation estimate of alpha\n\n[1] 0.8319112\n\n## Maximum likelihood estimate\nLH.mle &lt;- ar(LakeHuron, order.max = 1, method = \"mle\")\nLH.mle$ar                 # maximum likelihood estimate of alpha\n\n[1] 0.837546\n\nLH.mle$x.mean             # estimated series mean\n\n[1] 579.1141\n\nLH.mle$var.pred           # estimated innovation variance\n\n[1] 0.5092867\n\n\n\n\nThe general AR(p) model\n\nar(LakeHuron, method=\"mle\")\n\n\nCall:\nar(x = LakeHuron, method = \"mle\")\n\nCoefficients:\n      1        2  \n 1.0437  -0.2496  \n\nOrder selected 2  sigma^2 estimated as  0.4788\n\n\n\n\n~Moving average (MA) processes\n\n\n\n\n\n\n\n\n\n\n\n\nSubsection 6.1.5: ~Autoregressive moving average (ARMA) models – theory\n\n\nSubsection 6.1.6: Automatic model selection?\n\nlibrary(forecast, quietly=TRUE)\n(aaLH &lt;- auto.arima(LakeHuron, approximation=F, stepwise=F))\n\nSeries: LakeHuron \nARIMA(2,1,1) \n\nCoefficients:\n         ar1      ar2      ma1\n      0.9712  -0.2924  -0.9108\ns.e.  0.1137   0.1030   0.0712\n\nsigma^2 = 0.5003:  log likelihood = -102.54\nAIC=213.07   AICc=213.51   BIC=223.37\n\n\n\n## Check that model removes most of the correlation structure\nacf(resid(aaLH, type=\"innovation\"))   # `type=\"innovation\"` is the default\n\n\n\n\n\n\n\n\n\nauto.arima(LakeHuron)\n\nSeries: LakeHuron \nARIMA(0,1,0) \n\nsigma^2 = 0.5588:  log likelihood = -109.11\nAIC=220.22   AICc=220.26   BIC=222.79\n\n\n\n(aaLH0 &lt;- auto.arima(LakeHuron, d=0, approximation=F, stepwise=F))\n\nSeries: LakeHuron \nARIMA(1,0,1) with non-zero mean \n\nCoefficients:\n         ar1     ma1      mean\n      0.7449  0.3206  579.0555\ns.e.  0.0777  0.1135    0.3501\n\nsigma^2 = 0.4899:  log likelihood = -103.25\nAIC=214.49   AICc=214.92   BIC=224.83\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot(forecast(aaLH0,  h=12))  ## `level=c(80,95)` is the default\nfcETS &lt;- forecast(LakeHuron, h=12)\nplot(fcETS)\nplot(forecast(aaLH,  h=12, level=c(80,95)))  # Panel B; ARIMA(2,1,1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nauto.arima(LakeHuron, d=0, max.Q=0, approximation=F, stepwise=F)\n\nSeries: LakeHuron \nARIMA(1,0,1) with non-zero mean \n\nCoefficients:\n         ar1     ma1      mean\n      0.7449  0.3206  579.0555\ns.e.  0.0777  0.1135    0.3501\n\nsigma^2 = 0.4899:  log likelihood = -103.25\nAIC=214.49   AICc=214.92   BIC=224.83\n\n\n\nUse of simulation as a check\n\n\n\n\n\n\n\n\n\n\noldpar &lt;- par(mfrow=c(2,2), mar=c(3.1,4.6,2.6, 1.1))\nfor(i in 1:2){\n  simts &lt;- arima.sim(model=list(order=c(0,0,3), ma=c(0,0,0.25*i)), n=98)\n  acf(simts, main=\"\", xlab=\"\")\n  mtext(side=3, line=0.5, paste(\"ma3 =\", 0.25*i), adj=0)\n}\npar(oldpar)\n\n\n\n\n\n\n\n\n\nset.seed(29)         # Ensure that results are reproducible\nestMAord &lt;- matrix(0, nrow=4, ncol=20)\nfor(i in 1:4){\n  for(j in 1:20){\n    simts &lt;- arima.sim(n=98, model=list(ma=c(0,0,0.125*i)))\n    estMAord[i,j] &lt;- auto.arima(simts, start.q=3)$arma[2] }\n}\ndetectedAs &lt;- table(row(estMAord), estMAord)\ndimnames(detectedAs) &lt;- list(ma3=paste(0.125*(1:4)),\nOrder=paste(0:(dim(detectedAs)[2]-1)))\n\n\nprint(detectedAs)\n\n       Order\nma3      0  1  2  3  4\n  0.125 18  2  0  0  0\n  0.25  11  3  0  6  0\n  0.375  7  2  2  7  2\n  0.5    1  1  0 13  5\n\n\n\n\n\nSubsection 6.1.7: Seasonal effects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsuppressPackageStartupMessages(library(ggplot2))\nmdb12AVt1980on &lt;- window(DAAG::mdbAVtJtoD, c(1980,1))\nAVt.ets &lt;- ets(mdb12AVt1980on)\nautoplot(AVt.ets, main=\"\", fg=\"gray\") + \n  ggplot2::ggtitle(\"A: Components of ETS fit\") +\n    theme(plot.title = element_text(hjust=0, vjust=0.5, size=11))\nmonthplot(mdb12AVt1980on, col.base=2,  fg=\"gray\")\ntitle(\"B: Seasonal component, SI ratio\", \n      font.main=1, line=1, adj=0, cex=1.25)\n\n\n\n\n\n\n\n\n\n\n\nbomreg &lt;- DAAG::bomregions2021\n## Plot time series mdbRain, SOI, and IOD: ts object bomregions2021 (DAAG)\ngph &lt;- xyplot(ts(bomreg[, c(\"mdbRain\", \"mdbAVt\", \"SOI\", \"IOD\")], start=1900), \n              xlab=\"\", type=c('p','smooth'), scales=list(alternating=rep(1,3)))\nupdate(gph, layout=c(4,1), par.settings=DAAG::DAAGtheme(color=F))\n\n\nsuppressPackageStartupMessages(library(mgcv))\nbomreg &lt;- within(DAAG::bomregions2021, mdbrtRain &lt;- mdbRain^0.5)\n## Check first for a sequential correlation structure, after\n## fitting smooth terms s(CO2), s(SOI), and s(IOD)\nlibrary(mgcv)\nmdbrtRain.gam &lt;- gam(mdbrtRain~s(CO2) + s(SOI) + s(IOD), data=bomreg)\nauto.arima(resid(mdbrtRain.gam))\n\nSeries: resid(mdbrtRain.gam) \nARIMA(0,0,0) with zero mean \n\nsigma^2 = 4.424:  log likelihood = -263.82\nAIC=529.64   AICc=529.67   BIC=532.44\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot(mdbrtRain.gam, residuals=T, cex=2, fg=\"gray\")\n## Do also `gam.check(mdbrtRain.gam)` (Output looks fine)\n\n\nanova(mdbrtRain.gam)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nmdbrtRain ~ s(CO2) + s(SOI) + s(IOD)\n\nApproximate significance of smooth terms:\n         edf Ref.df      F  p-value\ns(CO2) 2.144  2.673  4.345  0.01090\ns(SOI) 2.058  2.637 12.581 2.72e-06\ns(IOD) 1.000  1.000  9.696  0.00233\n\n\n\nBox.test(resid(mdbrtRain.gam), lag=10, type=\"Ljung\")\n\n\n    Box-Ljung test\n\ndata:  resid(mdbrtRain.gam)\nX-squared = 11.935, df = 10, p-value = 0.2894\n\n\n\n## Examine normality of estimates of \"residuals\" \nqqnorm(resid(mdbrtRain.gam))\n\n\nThe mdbAVt series\n\nmdbAVt.gam &lt;- gam(mdbAVt ~ s(CO2)+s(SOI)+s(IOD), data=bomreg)\nauto.arima(resid(mdbAVt.gam))\n\nSeries: resid(mdbAVt.gam) \nARIMA(0,0,0) with zero mean \n\nsigma^2 = 0.1689:  log likelihood = -59.33\nAIC=120.67   AICc=120.7   BIC=123.39\n\nanova(mdbAVt.gam)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nmdbAVt ~ s(CO2) + s(SOI) + s(IOD)\n\nApproximate significance of smooth terms:\n         edf Ref.df      F p-value\ns(CO2) 1.855  2.312 41.842 &lt; 2e-16\ns(SOI) 1.000  1.000 10.688 0.00145\ns(IOD) 1.000  1.000  0.005 0.94403\n\n\n\nmdbAVt1.gam &lt;- gam(mdbAVt ~ s(CO2)+s(SOI), data=bomreg)\n\n\nplot(mdbAVt1.gam, residuals=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfaclevs &lt;- c(\"A: Rainfall\", expression(\"B: Average Temp (\"^o*\"C)\"))\nfitrain &lt;- fitted(mdbrtRain.gam) \nfitAVt &lt;- c(rep(NA,10), fitted(mdbAVt1.gam))\ngph &lt;- xyplot(mdbrtRain+mdbAVt~Year,data=bomreg, outer=T, xlab=\"\", ylab=\"\",\n  scales=list(y=list(relation='free', \n              at=list(sqrt((3:8)*100),(33:39)/2), \n              labels=list((3:8)*100,(33:39)/2)), x=list(alternating=rep(1,2))),\n  strip=strip.custom(factor.levels=faclevs))\ngph + latticeExtra::as.layer(xyplot(fitrain+fitAVt~Year, outer=T,\n                                    scales=list(y=list(relation='free')), \n                                    data=bomreg, pch=3, col=2))\n\n\n## Use `auto.arima()` to choose the ARIMA order:\naaFitCO2 &lt;- with(bomreg[-(1:10),], auto.arima(mdbAVt, xreg=cbind(CO2,SOI)))\n## Try including a degree 2 polynomial term\naaFitpol2CO2 &lt;- with(bomreg[-(1:10),], \n                     auto.arima(mdbAVt, xreg=cbind(poly(CO2,2),SOI)))\ncbind(AIC(aaFitCO2, aaFitpol2CO2), BIC=BIC(aaFitCO2, aaFitpol2CO2))\n\n             df      AIC BIC.df  BIC.BIC\naaFitCO2      7 125.3765      7 144.4060\naaFitpol2CO2  5 129.0109      5 142.6033\n\n\n\n\n\nSubsection 6.1.8: The gamm() function, with a correlated errors model\n\nSOI.gam &lt;- gam(SOI~s(Year), data=bomreg)\nauto.arima(resid(SOI.gam))             # sigma^2 = 43.4\n\nSeries: resid(SOI.gam) \nARIMA(0,0,2) with zero mean \n\nCoefficients:\n         ma1      ma2\n      0.2070  -0.1557\ns.e.  0.0943   0.1008\n\nsigma^2 = 43.43:  log likelihood = -402.2\nAIC=810.41   AICc=810.61   BIC=818.82\n\n## The following breaks the model into two parts -- gam and lme\nSOI.gamm &lt;- gamm(SOI~s(Year), data=bomreg)       \nres &lt;- resid(SOI.gamm$lme, type=\"normalized\")\nauto.arima(res)                        # sigma^2 = 0.945\n\nSeries: res \nARIMA(0,0,2) with zero mean \n\nCoefficients:\n         ma1      ma2\n      0.2070  -0.1557\ns.e.  0.0943   0.1008\n\nsigma^2 = 0.9446:  log likelihood = -168.68\nAIC=343.37   AICc=343.57   BIC=351.78\n\n## Extract scale estimate for `gam` component of SOI.gamm\nsummary(SOI.gamm$gam)[['scale']]       # 45.98\n\n[1] 45.98091\n\n  # Note that 45.98 x .945 ~= 43.4\n\n\nSOIma2.gamm &lt;- gamm(SOI~s(Year), data=bomreg, correlation=corARMA(q=2))\ncoef(SOIma2.gamm$lme$modelStruct$corStruct, unconstrained = FALSE)  # MA2 ests\n\n    Theta1     Theta2 \n 0.2070115 -0.1557351 \n\nSOIar2.gamm &lt;- gamm(SOI~s(Year), data=bomreg, correlation=corARMA(p=2))\ncoef(SOIar2.gamm$lme$modelStruct$corStruct, unconstrained = FALSE)  # AR2 ests\n\n      Phi1       Phi2 \n 0.2257966 -0.2097561 \n\ncbind(AIC(SOI.gam, SOIma2.gamm$lme, SOIar2.gamm$lme), \n      BIC=BIC(SOI.gam, SOIma2.gamm$lme, SOIar2.gamm$lme)[,2])\n\n                df      AIC      BIC\nSOI.gam          3 819.2646 827.6767\nSOIma2.gamm$lme  6 816.4090 833.2331\nSOIar2.gamm$lme  6 815.5117 832.3358\n\n\n\nThe dataset airquality (153 days, New York, 1972)\n\n## Add time in days from May 1 to data. \nairq &lt;- cbind(airquality[, 1:4], day=1:nrow(airquality))\n  # Column 5 ('day' starting May 1) replaces columns 'Month' & 'Day')\n## Check numbers of missing values   # Solar.R:7; Ozone:37\nmice::md.pattern(airq, plot=FALSE)   # Final row has totals missing.\n\n    Wind Temp day Solar.R Ozone   \n111    1    1   1       1     1  0\n35     1    1   1       1     0  1\n5      1    1   1       0     1  1\n2      1    1   1       0     0  2\n       0    0   0       7    37 44\n\n\n\nsmoothPars &lt;- list(col.smooth='red', lty.smooth=2, spread=0)\ncar::spm(airq, cex.labels=1.2, regLine=FALSE, oma=c(1.95,3,4,3), gap=.15,\n         col=adjustcolor('blue', alpha.f=0.3), smooth=smoothPars, fg=\"gray\")\n\n\n\n\n\n\n\n\n\ncar::powerTransform(gam(Ozone ~ s(Solar.R)+s(Wind)+s(Temp)+s(day), data=airq))\n\nEstimated transformation parameter \n       Y1 \n0.2306506 \n\nairq$rt4Ozone &lt;- airq$Ozone^0.25\n\n\nOzone.gam &lt;- gam(rt4Ozone ~ s(Solar.R)+s(Wind)+s(Temp)+s(day), data=airq)\nauto.arima(resid(Ozone.gam))  # Independent errors model appears OK\n\nSeries: resid(Ozone.gam) \nARIMA(0,0,0) with zero mean \n\nsigma^2 = 0.05897:  log likelihood = -0.4\nAIC=2.79   AICc=2.83   BIC=5.5\n\n## Check model terms\nanova(Ozone.gam)   # For GAM models, this leaves out terms one at a time\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nrt4Ozone ~ s(Solar.R) + s(Wind) + s(Temp) + s(day)\n\nApproximate significance of smooth terms:\n             edf Ref.df      F  p-value\ns(Solar.R) 2.284  2.871  7.701 0.000159\ns(Wind)    2.448  3.104  9.497 1.29e-05\ns(Temp)    4.275  5.287 13.305  &lt; 2e-16\ns(day)     1.000  1.000  0.495 0.483507\n\n## The term in `day` has no explanatory, and will be removed\nOzone1.gam &lt;- update(Ozone.gam, formula=rt4Ozone ~ s(Solar.R)+s(Wind)+s(Temp))\n\n\n\n\nSubsection 6.1.9: A calibration problem with time series errors\n\nflakes &lt;- DAAG::frostedflakes\ncalib.arima &lt;- with(flakes, auto.arima(IA400, xreg=Lab))\ncalib.arima\n\nSeries: IA400 \nRegression with ARIMA(0,0,1) errors \n\nCoefficients:\n         ma1  intercept    xreg\n      0.3876     6.9235  0.8323\ns.e.  0.0859     2.5634  0.0679\n\nsigma^2 = 3.278:  log likelihood = -199.81\nAIC=407.62   AICc=408.04   BIC=418.04\n\n\n\nwith(flakes, coef(auto.arima(IA400/Lab, approximation=F, stepwise=F)))\n\n      ma1 intercept \n0.3718151 1.0173303 \n\nwith(flakes, coef(auto.arima(IA400-Lab, approximation=F, stepwise=F)))\n\n      ma1 intercept \n0.3838733 0.6201945 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSection 6.2: Nonlinear time series\n\nx &lt;- numeric(999)  # x will contain the ARCH(1) errors\nx0 &lt;- rnorm(1)\nfor (i in 1:999){\nx0 &lt;- rnorm(1, sd=sqrt(.25 + .95*x0^2))\nx[i] &lt;- x0\n}\n\n\nsuppressPackageStartupMessages(library(tseries))\ngarch(x, order = c(0, 1), trace=FALSE)\n\n\nCall:\ngarch(x = x, order = c(0, 1), trace = FALSE)\n\nCoefficient(s):\n    a0      a1  \n0.2804  0.9633  \n\n\n\n\nSection 6.3: Further reading\n\nSpatial statistics\n\n\nOther time series models and packages\n\n\n\nSection 6.4: Exercises\n6.4\n\nxx &lt;- matrix(x, ncol=1000)\n\n6.7\n\nlibrary(tseries)\ndata(ice.river)\nriver1 &lt;- diff(log(ice.river[, 1]))\n\n6.9\n\nlibrary(forecast)\nEu1 &lt;- window(EuStockMarkets[,1], end = c(1996, 260))\nEu1nn &lt;- nnetar(Eu1)\nEu1f &lt;- forecast(Eu1nn, end=end(EuStockMarkets[,1]))\nplot(Eu1f, ylim=c(1400, 7000))\nlines(EuStockMarkets[,1])\n\n\n\n\n\n\n\n\n6.10a\n\nairq &lt;- cbind(airquality[, 1:4], day=1:nrow(airquality))\n  # Column 5 ('day' starting May 1) replaces columns 'Month' & 'Day')\nlibrary(mgcv)\ntemp.gam &lt;- gam(Temp~s(day), data=airq)\ntempAR1.gamm &lt;- gamm(Temp~s(day), data=airq, correlation=corAR1())\nplot(temp.gam, res=T, cex=2)\nplot(tempAR1.gamm$gam, res=T, cex=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.10b\n\n(Phi &lt;- coef(tempAR1.gamm$lme$modelStruct$corStruct, unconstrained = FALSE) )\nSigma &lt;- sqrt(tempAR1.gamm$gam$sig2)\n## Simulate an AR1 process with this parameter\nAR1.sim &lt;- arima.sim(model=list(ar=Phi), n=nrow(airq), sd=Sigma)\nsimSeries &lt;- AR1.sim+fitted(tempAR1.gamm$gam)\nplot(I(1:nrow(airq)), simSeries)\n## Compare with initial series\nplot(I(1:nrow(airq)), airq$Temp)\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/ch6.R\")\n}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 6: Time series models</span>"
    ]
  },
  {
    "objectID": "ch7.html",
    "href": "ch7.html",
    "title": "7  Chapter 7: Multilevel models, and repeated measures",
    "section": "",
    "text": "Packages required (plus any dependencies)\nDAAG lme4 afex MASS utils devtools qra glmmTMB DHARMa MEMSS forecast splines gamlss plotrix nlme\nAdditionally, knitr and Hmisc are required in order to process the Rmd source file.\n\nHmisc::knitrSet(basename=\"mva\", lang='markdown', fig.path=\"figs/g\", w=7, h=7)\noldopt &lt;- options(digits=4, formatR.arrow=FALSE, width=70, scipen=999)\nlibrary(knitr)\n## knitr::render_listings()\nopts_chunk[['set']](cache.path='cache-', out.width=\"80%\", fig.align=\"center\", \n                    fig.show='hold', size=\"small\", ps=10, strip.white = TRUE,\n                    comment=NA, width=70, tidy.opts = list(replace.assign=FALSE))\n\n\n\nSection 7.1 Corn yield data — analysis using aov()\n\nCorn yield measurements example\n\n\n\n\n\n\n\n\n\n\nant111b &lt;- within(DAAG::ant111b, Site &lt;- reorder(site, harvwt, FUN=mean))\ngph &lt;- lattice::stripplot(Site ~ harvwt, data=ant111b,\n                          xlab=\"Harvest weight of corn\")\nupdate(gph, par.settings=DAAG::DAAGtheme(color=FALSE), scales=list(tck=0.5))\n\n\nant111b &lt;- DAAG::ant111b\nant111b.aov &lt;- aov(harvwt ~ 1 + Error(site), data=ant111b)\n\n\nsummary(ant111b.aov)\n\n\nError: site\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nResiduals  7   70.3    10.1               \n\nError: Within\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nResiduals 24   13.9   0.578               \n\n\n\n\nInterpreting the mean squares\n\n\nDetails of the calculations\n\n\nPractical use of the analysis of variance results\n\n\nRandom effects vs. fixed effects\n\n\nNested factors – a variety of applications\n\n\nSubsection 7.1.1: A More Formal Approach\n\nRelations between variance components and mean squares\n\n\nInterpretation of variance components\n\n\nIntra-class correlation\n\n\n\n\nSection 7.2 Analysis using lme4::lmer()\n\nlibrary(lme4)\nant111b.lmer &lt;- lmer(harvwt ~ 1 + (1 | site), data=ant111b)\n\n\n## Note that there is no degrees of freedom information.\nprint(ant111b.lmer, ranef.comp=\"Variance\")\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: harvwt ~ 1 + (1 | site)\n   Data: ant111b\nREML criterion at convergence: 94.42\nRandom effects:\n Groups   Name        Variance\n site     (Intercept) 2.368   \n Residual             0.578   \nNumber of obs: 32, groups:  site, 8\nFixed Effects:\n(Intercept)  \n       4.29  \n\n\n\nThe processing of output from lmer()\n\ncoef(summary(ant111b.lmer))\n\n            Estimate Std. Error t value\n(Intercept)    4.292     0.5604   7.659\n\n\n\n\nFitted values and residuals in lmer()\n\ns2W &lt;- 0.578; s2L &lt;- 2.37; n &lt;- 4\nsitemeans &lt;- with(ant111b, sapply(split(harvwt, site), mean))\ngrandmean &lt;- mean(sitemeans)\nshrinkage &lt;- (n*s2L)/(n*s2L+s2W)\n## Check  that fitted values equal BLUPs, and compare with site means\nBLUP &lt;- grandmean + shrinkage*(sitemeans - grandmean)\nBLUP &lt;- fitted(ant111b.lmer)[match(names(sitemeans), ant111b$site)]\nBLUP &lt;- grandmean + ranef(ant111b.lmer)$site[[1]]\n\n\nrbind(BLUP=BLUP, sitemeans=sitemeans)\n\n           DBAN  LFAN  NSAN  ORAN  OVAN  TEAN  WEAN  WLAN\nBLUP      4.851 4.212 2.217 6.764 4.801 3.108 5.455 2.925\nsitemeans 4.885 4.207 2.090 6.915 4.833 3.036 5.526 2.841\n\n\n\n\n*Uncertainty in the parameter estimates — profile likelihood and alternatives\n\nprof.lmer &lt;- profile(ant111b.lmer)\nCI95 &lt;- confint(prof.lmer, level=0.95)\nrbind(\"sigmaL^2\"=CI95[1,]^2, \"sigma^2\"=CI95[2,]^2)\n\n          2.5 % 97.5 %\nsigmaL^2 0.7965  6.936\nsigma^2  0.3444  1.079\n\n\n\nCI95[3,]\n\n 2.5 % 97.5 % \n 3.128  5.456 \n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(lattice)\ngph &lt;- xyplot(prof.lmer, conf=c(50, 80, 95, 99)/100,\n              aspect=0.8, between=list(x=0.35))\nupdate(gph, scales=list(tck=0.5), ylab=\"Normal deviate\")\n\n\n\nModeling more than two levels of random variation\n\n\n\nSection 7.3 Survey data, with clustering\n\n\n\n\n\n\n\n\n\n\n## Means of like (data frame science: DAAG), by class\nscience &lt;- DAAG::science\nclassmeans &lt;- with(science, aggregate(like, by=list(PrivPub, Class), mean))\n# NB: Class identifies classes independently of schools\n#     class identifies classes within schools\nnames(classmeans) &lt;- c(\"PrivPub\", \"Class\", \"avlike\")\ngph &lt;- bwplot(~avlike|PrivPub, layout=c(1,2), xlab=\"Average score\",\n              panel=function(x,y,...){panel.bwplot(x,y,...)\n              panel.rug(x,y,...)}, data=classmeans)\nupdate(gph, scales=list(tcl=0.4))\n\n\nSubsection 7.3.1: Alternative models\n\nscience &lt;- DAAG::science\nscience.lmer &lt;- lmer(like ~ sex + PrivPub + (1 | school) +\n                     (1 | school:class), data = science,\n                     na.action=na.exclude)\n\n\nprint(VarCorr(science.lmer), comp=\"Variance\", digits=2)\n\n Groups       Name        Variance\n school:class (Intercept) 0.32    \n school       (Intercept) 0.00    \n Residual                 3.05    \n\n\n\nprint(coef(summary(science.lmer)), digits=2)\n\n              Estimate Std. Error t value\n(Intercept)       4.72      0.162    29.1\nsexm              0.18      0.098     1.9\nPrivPubpublic     0.41      0.186     2.2\n\n\n\nsummary(science.lmer)$ngrps\n\nschool:class       school \n          66           41 \n\n\n\nscience1.lmer &lt;- lmer(like ~ sex + PrivPub + (1 | school:class),\n                      data = DAAG::science, na.action=na.exclude)\n\n\nprint(VarCorr(science1.lmer), comp=\"Variance\", digits=3)\n\n Groups       Name        Variance\n school:class (Intercept) 0.321   \n Residual                 3.052   \n\nprint(coef(summary(science1.lmer)), digits=2)\n\n              Estimate Std. Error t value\n(Intercept)       4.72      0.162    29.1\nsexm              0.18      0.098     1.9\nPrivPubpublic     0.41      0.186     2.2\n\n\n\nopt &lt;- options(contrasts=c(\"contr.sum\",\"contr.poly\"))\n  # Change is otherwise made as and if required for individual factors\n  # prior to fitting model, and a warning message is generated.\nafex::mixed(like ~ sex + PrivPub + (1 | school:class), method=\"KR\", type=2,\n            data = na.omit(science),  sig_symbols=rep(\"\",4), progress=FALSE)\n\nMixed Model Anova Table (Type 2 tests, KR-method)\n\nModel: like ~ sex + PrivPub + (1 | school:class)\nData: na.omit(science)\n   Effect         df    F p.value\n1     sex 1, 1379.49 3.44    .064\n2 PrivPub   1, 60.44 4.91    .030\n\noptions(opt)       # Reset to previous contrasts setting\n\n\nMore detailed examination of the output\n\n## Use profile likelihood\npp &lt;- profile(science1.lmer, which=\"theta_\")\n# which=\"theta_\": all random parameters\n# which=\"beta_\": fixed effect parameters\nvar95 &lt;- confint(pp, level=0.95)^2\n# Square to get variances in place of SDs\nrownames(var95) &lt;- c(\"sigma_Class^2\", \"sigma^2\")\nsignif(var95, 3)\n\n              2.5 % 97.5 %\nsigma_Class^2 0.178  0.511\nsigma^2       2.830  3.300\n\n\n\n## Fit model and generate quantities that will be plotted\nscience1.lmer &lt;- lmer(like ~ sex + PrivPub + (1 | school:class),\ndata = science, na.action=na.omit)\n## Panel A: random site effects vs number in class\nranf &lt;- ranef(obj = science1.lmer, drop=TRUE)[[\"school:class\"]]\nflist &lt;- science1.lmer@flist[[\"school:class\"]]\nprivpub &lt;- science[match(names(ranf), flist), \"PrivPub\"]\nnum &lt;- unclass(table(flist)); numlabs &lt;- pretty(num)\n## Panel B: Within class variance estimates vs numbers\nres &lt;- residuals(science1.lmer)\nvars &lt;- tapply(res, INDEX=list(flist), FUN=var)*(num-1)/(num-2)\n## Panel C: Normal probability of random site effects (`ranf`)\n## Panel D: Normal probability of residuals (`res`)\n\n\n\n\n\n\n\n\n\n\n\nopar &lt;- par(oma=c(0,0,1.5,0))\n## Panel A: Plot effect estimates vs number\nxlab12 &lt;- \"# in class (square root scale)\"\nplot(sqrt(num), ranf, xaxt=\"n\", pch=c(1,3)[as.numeric(privpub)], cex=0.8,\n     xlab=xlab12, ylab=\"Estimate of class effect\", fg=\"gray\")\nlines(lowess(sqrt(num[privpub==\"private\"]),\nranf[privpub==\"private\"], f=1.1), lty=2)\nlines(lowess(sqrt(num[privpub==\"public\"]),\nranf[privpub==\"public\"], f=1.1), lty=3)\naxis(1, at=sqrt(numlabs), labels=paste(numlabs), lwd=0, lwd.ticks=1)\n## Panel B: Within class variance estimates vs numbers\nplot(sqrt(num), vars, pch=c(1,3)[unclass(privpub)], cex=0.8,\n     xlab=xlab12, ylab=\"Within-class variance\", fg=\"gray\")\nlines(lowess(sqrt(num[privpub==\"private\"]),\n      as.vector(vars)[privpub==\"private\"], f=1.1), lty=2)\nlines(lowess(sqrt(num[privpub==\"public\"]),\nas.vector(vars)[privpub==\"public\"], f=1.1), lty=3)\n## Panel C: Normal quantile-quantile plot of site effects\nqqnorm(ranf, ylab=\"Ordered site effects\", cex=0.8, main=\"\",\n       col=\"gray40\", fg=\"gray\")\n## Panel D: Normal quantile-quantile plot of residuals\nqqnorm(res, ylab=\"Ordered w/i class residuals\", cex=0.8, main=\"\",\n       col=\"gray40\", fg=\"gray\")\npar(fig = c(0, 1, 0, 1), oma = c(0, 0, 0, 0), mar = c(0, 0, 0, 0),\nnew = TRUE)\nplot(0, 0, type = \"n\", bty = \"n\", xaxt = \"n\", yaxt = \"n\")\nlegend(x=\"top\", legend=c(\"Private     \", \"Public\"), pch=c(1,3),\n       lwd=c(1,1), lty=2:3, cex=1.25,\n       xjust=0.5, yjust=0.8, horiz=TRUE, merge=FALSE, bty=\"n\")\npar(opar)\n\n\n\n\nSubsection 7.3.2: Instructive, though faulty, analyses\n\nIgnoring class as the random effect\n\nscience2.lmer &lt;- lmer(like ~ sex + PrivPub + (1 | school),\ndata = science, na.action=na.exclude)\nprint(coef(summary(science2.lmer)), digits=3)\n\n              Estimate Std. Error t value\n(Intercept)      4.738      0.163   29.00\nsexm             0.197      0.101    1.96\nPrivPubpublic    0.417      0.185    2.25\n\n\n\n## NB: Output is misleading\nprint(VarCorr(science2.lmer), comp=\"Variance\", digits=3)\n\n Groups   Name        Variance\n school   (Intercept) 0.166   \n Residual             3.219   \n\n\n\n\nIgnoring the random structure in the data\n\n## Faulty analysis, using lm\nscience.lm &lt;- lm(like ~ sex + PrivPub, data=science)\nround(coef(summary(science.lm)), digits=4)\n\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)     4.7402     0.0996  47.616   0.0000\nsexm            0.1509     0.0986   1.531   0.1261\nPrivPubpublic   0.3951     0.1051   3.759   0.0002\n\n\n\n\n\nSubsection 7.3.3: Predictive accuracy\n\n\n\nSection 7.4 A multilevel experimental design\n\n\n\n\n\n\n\n\n\n\npar(mar=rep(0.25,4))\nMASS::eqscplot(c(0,13),c(4.0,13),type=\"n\",xlab=\"\",ylab=\"\", asp=1, axes=F)\neps &lt;- 0.1\nsuby &lt;- 12\nvines&lt;-function(x=1,y=1,subp=0, suby=12){\nlines(c(y,y,y+1,y+1,y), suby-c(x,x+1,x+1,x,x),lwd=0.5)\npoints(c(y+.2,y+.2,y+.8,y+.8),suby-c(x+.2,x+.8,x+.8,x+.2),pch=3,cex=0.65)\ntext(y+.5,suby-(x+.5),paste(subp))\n}\nk&lt;-0\nfor(i in c(1,3,5,7)){k&lt;-k+1; vines(1,i,c(3,1,0,2)[k])}\nk&lt;-0\nfor(i in c(1,3,5,7)){k&lt;-k+1; vines(4,i,c(2,1,0,3)[k])}\nk &lt;- 0\nfor(i in c(1,4,4,1)){k&lt;-k+1\nj&lt;-c(9,9,11,11)[k]\nvines(i,j,c(3,2,1,0)[k])\n}\nlines(c(2*eps,2.85,NA,10.15,13-2*eps), suby-c(3,3,NA,3,3),lty=2)\nlines(c(0,2.85,NA,10.15,13),suby-c(0,0,NA,0,0),lty=2)\nlines(c(0,4.5,NA,8.5,13),suby-c(8,8,NA,8,8),lty=2)\nlines(rep(0,5),suby-c(0,1.25,NA,6.75,8),lty=2)\nlines(rep(13,5),suby-c(0,1.25,NA,6.75,8),lty=2)\nlines(c(9,9,12,12,9)+c(-eps,-eps,eps,eps,-eps),\n      suby-(c(1,5,5,1,1)+c(-eps,eps,eps,-eps,-eps)), lwd=1)\nlines(c(1,1,8,8,1)+c(-eps,-eps,eps,eps,-eps),\n      suby-c(c(1,2,2,1,1)+c(-eps,eps,eps,-eps,-eps)), lwd=1)\nlines(c(1,1,8,8,1)+c(-eps,-eps,eps,eps,-eps),\n      suby-c(c(1,2,2,1,1)+3+c(-eps,eps,eps,-eps,-eps)), lwd=1)\ntext(6.5,suby,\"6 meters height artifical shelter belt\")\ntext(0,suby-4,\"9 meters height shelter belt\", srt=90)\ntext(13,suby-4,\"19 meters height shelter belt\", srt=-90)\ntext(6.5,suby-8,\"Willow shelter belt\")\ntext(0.5,suby-6.5,\"0 Unshaded   \\n1 Shaded Aug-Dec   \\n2 Dec-Feb   \\n3 Feb-May\", adj=0)\ntext(6.5,suby-3,\"16 meters height willow shelter belt\")\noffset &lt;- c(4.75, 4.75-sqrt(3)*0.5)/6\narrows(x0=0,y0=12.1, x1=0+offset[1], y1=12.1+offset[2], length=0.15)\ntext(0.17, 12.55,\"N\")\n\n\nSubsection 7.4.1: The analysis of variance (anova) table\n\n## Analysis of variance: data frame kiwishade (DAAG)\nkiwishade &lt;- DAAG::kiwishade\nkiwishade.aov &lt;- aov(yield ~ shade + Error(block/shade),\ndata=kiwishade)\nsummary(kiwishade.aov)\n\n\nError: block\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nResiduals  2    172    86.2               \n\nError: block:shade\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nshade      3   1395     465    22.2 0.0012\nResiduals  6    126      21               \n\nError: Within\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nResiduals 36    439    12.2               \n\n\n\n\nSubsection 7.4.2: Expected values of mean squares\n\nmodel.tables(kiwishade.aov, type=\"means\", cterms=\"shade\")\n\nTables of means\nGrand mean\n      \n96.53 \n\n shade \nshade\n   none Aug2Dec Dec2Feb Feb2May \n 100.20  103.23   89.92   92.77 \n\n\n\n## Calculate treatment means\nwith(kiwishade, sapply(split(yield, shade), mean))\n\n   none Aug2Dec Dec2Feb Feb2May \n 100.20  103.23   89.92   92.77 \n\n\n\n\nSubsection 7.4.3: * The analysis of variance sums of squares breakdown\n\n## For each plot, calculate mean, and differences from the mean\nvine &lt;- paste(\"vine\", rep(1:4, 12), sep=\"\")\nvine1rows &lt;- seq(from=1, to=45, by=4)\nkiwivines &lt;- unstack(kiwishade, yield ~ vine)\nkiwimeans &lt;- apply(kiwivines, 1, mean)\nkiwivines &lt;- cbind(kiwishade[vine1rows,  c(\"block\",\"shade\")],\nMean=kiwimeans, kiwivines-kiwimeans)\nkiwivines &lt;- with(kiwivines, kiwivines[order(block, shade), ])\nmean(kiwimeans)      # the grand mean\n\n[1] 96.53\n\n\n\n\n\n\n\n\n\n\n\n\nkiwishade &lt;- DAAG::kiwishade\nkiwimeans &lt;- with(kiwishade, aggregate(yield,by=list(block,shade),mean))\nnames(kiwimeans) &lt;- c(\"block\",\"shade\",\"meanyield\")\nplotmeans.lm &lt;- lm(meanyield~block+shade, data=kiwimeans)\neffects &lt;- predict(plotmeans.lm, type=\"terms\")\nkiwishade.lm &lt;- lm(yield ~ block*shade, data=kiwishade)\ny &lt;- c(effects[,\"block\"]/sqrt(2) * sqrt(16),\neffects[,\"shade\"]/sqrt(3) * sqrt(12),\nresiduals(plotmeans.lm)/sqrt(6) * sqrt(4),\nresiduals(kiwishade.lm)/sqrt(12))\nn &lt;- rep(4:1, c(12, 12, 12, 48))\ngps &lt;- rep(c(\"block effect\\n(ms=86.2)\", \"shade\\n(464.8)\",\n\"plot\\n(20.9)\", \"vine\\n(12.2)\"), c(12, 12, 12, 48))\ngps &lt;- factor(gps, levels = rev(c(\"block effect\\n(ms=86.2)\",\n\"shade\\n(464.8)\", \"plot\\n(20.9)\", \"vine\\n(12.2)\")))\ngps.sd &lt;- sapply(split(y,gps), sd)\ngps.av &lt;- sapply(split(y,gps), mean)\nplot(range(y), range(n)+c(-0.5, 0.5), xlab=\"\", ylab=\"\", yaxt=\"n\", type=\"n\", fg=\"gray\")\ntext(y, n+0.15, \"|\", cex=0.8, col=adjustcolor(\"black\", alpha.f=0.5))\nun &lt;- 1:4\nsapply(un, function(j){lines(gps.av[j]+c(-gps.sd[j], gps.sd[j]),\nrep(j-0.15,2), col=\"gray\")\nlines(rep(gps.av[j],2)-gps.sd[j],\nj-0.15+c(-.06, .06), col=\"gray\")\nlines(rep(gps.av[j],2)+gps.sd[j],\nj-0.15+c(-.06, .06), col=\"gray\")\n})\nmtext(side=1,line=2.25, cex=0.9,\ntext=\"Variation in Yield (kg)\\n(Add to grand mean of yield = 96.53)\")\npar(las=2)\naxis(2, at=1:4, labels=levels(gps), lwd=0, lwd.ticks=1)\n\n\n\nSubsection 7.4.4: The variance components\n\n\nSubsection 7.4.5: The mixed model analysis\n\nkiwishade.lmer &lt;- lmer(yield ~ shade + (1|block) + (1|block:plot),\ndata=kiwishade)\n# block:shade is an alternative to block:plot\n\n\nprint(kiwishade.lmer, ranef.comp=\"Variance\", digits=3)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: yield ~ shade + (1 | block) + (1 | block:plot)\n   Data: kiwishade\nREML criterion at convergence: 252\nRandom effects:\n Groups     Name        Variance\n block:plot (Intercept)  2.19   \n block      (Intercept)  4.08   \n Residual               12.18   \nNumber of obs: 48, groups:  block:plot, 12; block, 3\nFixed Effects:\n (Intercept)  shadeAug2Dec  shadeDec2Feb  shadeFeb2May  \n      100.20          3.03        -10.28         -7.43  \n\n\n\nResiduals and estimated effects\n\n\n\n\n\n\n\n\n\n\n## Panel A\nparsetA &lt;- modifyList(DAAG::DAAGtheme(color=FALSE),\n                      list(layout.heights=list(key.top=2.25, axis.top=.75)))\nif (!exists(\"kiwishade.lmer\"))\nkiwishade.lmer &lt;- lme4::lmer(yield ~ shade + (1|block/shade), data=kiwishade)\npk1 &lt;- xyplot(residuals(kiwishade.lmer) ~ fitted(kiwishade.lmer)|block,\n              groups=shade, layout=c(3,1), par.strip.text=list(cex=1.0),\n              data=kiwishade, grid=TRUE,\n              xlab=\"Fitted values (Treatment + block + plot effects)\",\n              ylab=\"Residuals\",\n  main=list(label=\"A: Standardized residuals after fitting block and plot effects\",\n          cex=1.05, x=0.01, y=0, font=1, just=\"left\"),\n             auto.key=list(space='top', points=TRUE, columns=4))\nprint(update(pk1, scales=list(x=list(alternating=TRUE), tck=0.35),\n             par.settings=parsetA), position=c(0,.52,1,1))\n## Panel B\nploteff &lt;- ranef(kiwishade.lmer, drop=TRUE)[[1]]\nnam &lt;- names(sort(ploteff, decreasing=TRUE)[1:4])\nparsetB &lt;- modifyList(DAAG::DAAGtheme(color=FALSE),\n                      list(layout.heights=list(axis.top=0.85)))\npk2 &lt;- qqmath(ploteff, ylab=\"Plot effect estimates\",\n              key=list(x=0, y=0.98, corner=c(0,1),\n              text=list(labels=nam, cex=0.65)),\n              main=list(label=\"B: Normal Q-Q plot of plot effects\",\n                        cex=1.05, x=0.01, y=0.25, font=1, just=\"left\"),\n              xlab=\"Normal quantiles\")\nprint(update(pk2, scales=list(tck=0.35), par.settings=parsetB),\n      newpage=FALSE, position=c(0,0,.5,.5))\n## Panel C\nsimvals &lt;- simulate(kiwishade.lmer, nsim=3, seed=23)\nsimranef &lt;- function(y)ranef(lme4::refit(kiwishade.lmer, y))[[1]]\nsimeff &lt;- apply(simvals, 2, function(y) scale(simranef(y)))\nsimeff &lt;- as.data.frame(simeff)\npk3 &lt;- qqmath(~ sim_1+sim_2+sim_3, data=simeff,\n              ylab=\"Simulated plot effects\\n(2 sets, standardized)\",\n              Qs=list(tck=0.35), aspect=1,\n               main=list(label=\"C: 3 simulations -- normal Q-Q plots\",\n                         x=0.01, y=0.25, cex=1.05, font=1, just=\"left\"),\n               xlab=\"Normal quantiles\")\nprint(update(pk3, scales=list(tck=0.35), par.settings=parsetB),\n      newpage=FALSE, position=c(0.48,0,1,.5))\n\n\nwith(kiwishade, sapply(split(yield, shade), mean))\n\n   none Aug2Dec Dec2Feb Feb2May \n 100.20  103.23   89.92   92.77 \n\n\n\n\n\nSubsection 7.4.6: Predictive accuracy\n\n\n\nSection 7.5 Within and between subject effects\n\n\n\n\n\n\n\n\n\n\nlibrary(lattice)\ntinting &lt;- within(DAAG::tinting, targtint &lt;- paste(target,toupper(tint),sep=':'))\ngphA &lt;- bwplot(targtint~log(csoa) | sex*agegp, data=tinting, layout=c(4,1), between=list(x=0.25))\nmainA &lt;- list(\"A: Boxplots for `log(csoa)`, by combinations of `target` and `tint`\",\n              font=1, x=0.125, y=0.125, cex=1.0, just='left')\ngphB &lt;- bwplot(targtint~log(it) | sex*agegp, data=tinting, layout=c(4,1), between=list(x=0.25))\nmainB &lt;- list(\"B: Boxplots for `log(it)`, by combinations of `target` and `tint`\",\n              font=1, x=0.125, y=0.125, cex=1.0, just='left')\nprint(update(gphA, main=mainA, xlab=\"\"), position=c(0, 0.48, 1, 1.0), more=T)\nprint(update(gphB, main=mainB, xlab=\"\"), position=c(0,0,1,0.52))\n\n\nModel fitting criteria\n\n\nSubsection 7.5.1: Model selection\n\n## Capitalize tinting$agegp\nlevels(tinting$agegp) &lt;- R.utils::capitalize(levels(tinting$agegp))\n## Fit all interactions: data frame tinting (DAAG)\nit3.lmer &lt;- lmer(log(it) ~ tint*target*agegp*sex + (1 | id),\n                 data=DAAG::tinting, REML=FALSE)\n\n\nit2.lmer &lt;- lmer(log(it) ~ (tint+target+agegp+sex)^2 + (1 | id),\n                 data=DAAG::tinting, REML=FALSE)\n\n\nit1.lmer &lt;- lmer(log(it)~(tint+target+agegp+sex) + (1 | id),\n                 data=DAAG::tinting, REML=FALSE)\n\n\nanova(it1.lmer, it2.lmer, it3.lmer)\n\nData: DAAG::tinting\nModels:\nit1.lmer: log(it) ~ (tint + target + agegp + sex) + (1 | id)\nit2.lmer: log(it) ~ (tint + target + agegp + sex)^2 + (1 | id)\nit3.lmer: log(it) ~ tint * target * agegp * sex + (1 | id)\n         npar   AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)\nit1.lmer    8  1.14 26.8   7.43    -14.9                    \nit2.lmer   17 -3.74 50.7  18.87    -37.7 22.88  9     0.0065\nit3.lmer   26  8.15 91.5  21.93    -43.9  6.11  9     0.7288\n\n\n\n## Code that gives the first four such plots, for the observed data\ninteraction.plot(tinting$tint, tinting$agegp, log(tinting$it))\ninteraction.plot(tinting$target, tinting$sex, log(tinting$it))\ninteraction.plot(tinting$tint, tinting$target, log(tinting$it))\ninteraction.plot(tinting$tint, tinting$sex, log(tinting$it))\n\n\n\nSubsection 7.5.2: Estimates of model parameters\n\nit2.reml &lt;- update(it2.lmer, REML=TRUE)\nprint(coef(summary(it2.reml)), digits=2)\n\n                       Estimate Std. Error t value\n(Intercept)              3.6191      0.130   27.82\ntint.L                   0.1609      0.044    3.64\ntint.Q                   0.0210      0.045    0.46\ntargethicon             -0.1181      0.042   -2.79\nagegpolder               0.4712      0.233    2.02\nsexm                     0.0821      0.233    0.35\ntint.L:targethicon      -0.0919      0.046   -2.00\ntint.Q:targethicon      -0.0072      0.048   -0.15\ntint.L:agegpolder        0.1308      0.049    2.66\ntint.Q:agegpolder        0.0697      0.052    1.34\ntint.L:sexm             -0.0979      0.049   -1.99\ntint.Q:sexm              0.0054      0.052    0.10\ntargethicon:agegpolder  -0.1389      0.058   -2.38\ntargethicon:sexm         0.0779      0.058    1.33\nagegpolder:sexm          0.3316      0.326    1.02\n\n# NB: The final column, giving degrees of freedom, is not in the\n# summary output. It is our addition.\n\n\nsubs &lt;- with(tinting, match(unique(id), id))\nwith(tinting, table(sex[subs], agegp[subs]))\n\n   \n    Younger Older\n  f       9     4\n  m       4     9\n\n\n\n\n\nSection 7.6 A mixed model with a betabinomial error\n\nSubsection 7.6.1: The betabinomial distribution\n\n## devtools::install_github(\"jhmaindonald/qra\")  # Use if not found on CRAN\n\n\nNotation\n\n\nSource of data\n\nHawCon &lt;- within(as.data.frame(qra::HawCon), {\n  trtGp &lt;- gsub(\"Fly\", \"\", paste0(CN,LifestageTrt))\n  trtGp &lt;- factor(trtGp, levels=sort(unique(trtGp))[c(1,5,2,6,3,7,4,8)])\n  trtGpRep &lt;- paste0(CN,LifestageTrt,\":\",RepNumber)\n  scTime &lt;- scale(TrtTime)  # Centering and scaling can help model fit\n  })\n\n\n## Load packages that will be used\nsuppressMessages(library(lme4)); suppressMessages(library(glmmTMB))\n\n\nlibrary(splines)\nform &lt;- cbind(Dead,Live)~0+trtGp/TrtTime+(1|trtGpRep)\n## Add the quadratic term from a degree 2 orthogonal polynomial\nform2s &lt;- update(form, . ~ . + scale(scTime^2))\n  ## Scale \"corrections\" to reduce risk of potential numerical problems\nHCbb.cll &lt;- glmmTMB(form, dispformula=~trtGp+ns(scTime,2),\n                    family=glmmTMB::betabinomial(link=\"cloglog\"), data=HawCon)\nHCbb2s.cll &lt;- update(HCbb.cll, formula=form2s)\nHCbb.logit &lt;- glmmTMB(form, dispformula=~trtGp+ns(scTime,2),\n                      family=glmmTMB::betabinomial(link=\"logit\"), data=HawCon)\nHCbb2s.logit &lt;- update(HCbb.logit, formula=form2s)\n\n\nsummary(HCbb2s.cll)$coefficients$cond[\"scale(scTime^2)\",]    ## CLL\n\n  Estimate Std. Error    z value   Pr(&gt;|z|) \n  -0.09130    0.06143   -1.48627    0.13721 \n\nsummary(HCbb2s.logit)$coefficients$cond[\"scale(scTime^2)\",]  ## Logit\n\n  Estimate Std. Error    z value   Pr(&gt;|z|) \n   0.43219    0.17766    2.43268    0.01499 \n\n\n\nAICtab &lt;- AIC(BB=HCbb.cll,HCbb2s.cll,HCbb.logit,HCbb2s.logit)\nAICtab[['Details']] &lt;- c(paste0('BB: Compl. log-log', c('', ', added curve')),\n                          paste0('BB: Logit', c('', ', added curve')))\nAICtab[order(AICtab[[\"AIC\"]]), ]\n\n             df   AIC                         Details\nHCbb2s.cll   28 709.9              BB: Compl. log-log\nHCbb.cll     27 710.0 BB: Compl. log-log, added curve\nHCbb2s.logit 28 717.2          BB: Logit, added curve\nHCbb.logit   27 721.7                       BB: Logit\n\n\n\n\n\n\n\n\n\n\n\n\ndat &lt;- expand.grid(trtGp=factor(levels(HawCon$trtGp), levels=levels(HawCon$trtGp)),\nTrtTime=pretty(range(HawCon$TrtTime),15), trtGpRep=NA)\nab &lt;- qra::getScaleCoef(HawCon$scTime)\ndat$scTime &lt;- with(dat,(TrtTime-ab[1])/ab[2])\nhatClog &lt;- predict(HCbb2s.cll, newdata=dat)\nhatClog &lt;- predict(HCbb2s.cll, se=T, newdata=dat)\nhatLgt &lt;- predict(HCbb2s.logit, newdata=dat)\nhatLgt &lt;- predict(HCbb2s.logit, se=T, newdata=dat)\ndat2 &lt;- cbind(rbind(dat,dat), link=rep(c('clog','logit'), rep(nrow(dat),2)))\ndat2 &lt;- within(dat2, {fit&lt;-c(hatClog$fit,hatLgt$fit);\nse.fit&lt;-c(hatClog$se.fit,hatLgt$se.fit);\nlink=rep(c('clog','logit'), rep(nrow(dat),2))})\ndat2 &lt;- within(dat2, {lwr&lt;-fit-2*se.fit; upr&lt;-fit+2*se.fit})\nlibrary(lattice)\nmy.panel.bands &lt;- function(x, y, upper, lower, fill, col,\nsubscripts, ..., font, fontface)\n{\nupper &lt;- upper[subscripts]\nlower &lt;- lower[subscripts]\npanel.lines(x,lower, ...)\npanel.lines(x,upper, ...)\n}\npanel2 &lt;- function(x, y, ...){\npanel.superpose(x, y, panel.groups = my.panel.bands, type='l', lty=3, alpha=0.25,...)\npanel.xyplot(x, y, type='l', lwd=2, cex=0.6, ...)\n}\nparset &lt;- simpleTheme(col=rep(4:1,rep(2,4)), lty=rep(1:2, 4), lwd=2)\n## c('solid','1141')\np &lt;- c(.02,.2,.5,.8, .99, .999968)\ncloglog &lt;- make.link('cloglog')$linkfun\nlogit &lt;- make.link('logit')$linkfun\nfitpos &lt;- list(cloglog(p), logit(p))\nlab &lt;- paste(p)\nlim &lt;- list(cloglog(c(0.02, 0.99998)), logit(c(0.02, 0.99998)))\nlim &lt;- lapply(lim, function(x)c(x[1],x[1]+diff(x)*1.2))\n\ngph &lt;- xyplot(fit~TrtTime|link, outer=TRUE, data=dat2, groups=trtGp,\n              upper = dat2$upr, lower = dat2$lwr, panel = panel2,\n              xlab=\"Days in coolstorage\", ylab=\"Fitted value\",\n              auto.key=list(text=levels(HawCon$trtGp), columns=4,\n                            points=FALSE, lines=TRUE),\n              par.settings=parset, layout=c(2,1),\n              scales=list(x=list(at=c(2,6,10,14)),\n              y=list(relation='free',\n              at=fitpos, labels=lab, limits=lim), alternating=c(1,1)))\nupdate(gph, strip=strip.custom(factor.levels=c(\"A: Complementary log-log link\",\n\"B: Logit link\")))\n\n\n\n\n\n\n\n\n\n\n\nparset &lt;- DAAG::DAAGtheme(color=TRUE, col.points=rep(1:4,rep(2,4)),\n                          pch=rep(1:4,2), lwd=2)\nHawCon$rho2clog &lt;- qra::getRho(HCbb.cll)\nHawCon$dispClog &lt;- with(HawCon, 1+(Total-1)*rho2clog)\npar(oma=c(0,0,2,0))\ntitles=c(expression(\"A: \"*rho*\", cloglog link\"),expression(\"B: \"*rho*\", logit link\"),\nexpression(\"C: Dispersion \"*Phi*\" (Panel A)\"))\nlibrary(lattice)\nHawCon$rho2logit &lt;- qra::getRho(HCbb.logit)\nxyplot(rho2clog+rho2logit+dispClog ~ TrtTime, groups=trtGp, data=HawCon,\n       outer=TRUE, between=list(x=0.25), par.settings=parset,\n   scales=list(x=list(alternating=FALSE,at=c(4,8,12), labels=paste(c(4,8,12))),\n               y=list(relation='free',tick.number=4)),\n   auto.key=list(columns=4, between.columns=2, between=1),\n   xlab=\"Days in coolstorage\", ylab=\"Parameter Estimate\",\n   strip=strip.custom(factor.levels=titles))\n\n\n\n\nSubsection 7.6.2: Diagnostic checks\n\n\n\n\n\n\n\n\n\n\npar(oma=c(0,0,2,0.5))\n## Code for plots, excluding main titles\nset.seed(29)\nsimRef &lt;- DHARMa::simulateResiduals(HCbb.cll, n=250, seed=FALSE)\nDHARMa::plotQQunif(simRef)\ntitle(main=\"A: Q-Q plot, quantile residuals\", adj=0, line=2.5, \n      font.main=1, cex.main=1.25)\nDHARMa::plotResiduals(simRef, form=HawCon$trtGp)\naxis(1, at=c(2,4,6,8), labels=levels(HawCon$trtGp)[c(2,4,6,8)],\n     gap.axis=0, line=1, lwd=0)\ntitle(main=\"B: Boxplots comparing treatment groups\", adj=0, line=2.5,\n      font.main=1, cex.main=1.25)\nDHARMa::plotResiduals(simRef)\ntitle(main=\"C: Plot against model predictions\", adj=0, font.main=1, line=3.25,\n       cex.main=1.25)\nDHARMa::plotResiduals(simRef, form=HawCon$Total)\ntitle(main=\"D: Plot against total number\", adj=0, font.main=1, line=3.25,\n       cex.main=1.25)\n\n\n\nSubsection 7.6.3: Lethal time estimates and confidence intervals\n\nshorten &lt;- function(nam, leaveout=c('trtGp','Fly',':')){\nfor(txt in leaveout){\nnam &lt;- gsub(txt,'', nam, fixed=TRUE)\n}\nnam\n}\n\n\nLTbb.cll &lt;- qra::extractLT(p=0.99, obj=HCbb.cll, link=\"cloglog\",\n                        a=1:8, b=9:16, eps=0, df.t=NULL)[,-2]\nrownames(LTbb.cll) &lt;- gsub(\"trtGp|Fly|:\", '', rownames(LTbb.cll), perl=T)\n\n\nLTbb.logit &lt;- qra::extractLT(p=0.99, obj=HCbb.logit, link=\"logit\",\n                             a=1:8, b=9:16, eps=0, offset=0,\ndf.t=NULL)[,-2]\nrownames(LTbb.logit) &lt;- shorten(rownames(LTbb.logit))\n\n\n\n\n\n\n\n\n\n\n\nlibrary(plotrix)\ngpNam &lt;- rownames(LTbb.cll)\nordEst &lt;- order(LTbb.cll[,1])\ncol5 &lt;- c(\"blue\",\"lightslateblue\",\"blueviolet\",'gray50','gray80')\nplotCI(1:8-0.1, y=LTbb.cll[ordEst,1], ui=LTbb.cll[ordEst,3],\n  li=LTbb.cll[ordEst,2], lwd=2, col=col5[1], xaxt=\"n\",\n  fg=\"gray\", xlab=\"\", ylab=\"LT99 Estimate (days)\",\n  xlim=c(0.8,8.2), ylim=c(0,19.5))\nplotCI(1:8+0.1, y=LTbb.logit[ordEst,1], ui=LTbb.logit[ordEst,3],\n  li=LTbb.logit[ordEst,2], lwd=2, col=col5[2], xaxt=\"n\", add=TRUE)\naxis(1, labels=FALSE, tck=0.02, col.ticks=\"gray40\")\ntext(x = 1:length(gpNam)+0.1,\n  ## Move labels to just below bottom of chart.\n  y = par(\"usr\")[3] - 0.8,\n  labels = gpNam[ordEst], ## Use names from the data list.\n    xpd = NA,        ## Change the clipping region\n  srt = 45,          ## Rotate the labels by 45 degrees.\n  adj = 0.95)        ## Adjust the labels to almost 100% right-justified\ngrid()\nlegend(\"topleft\", legend=c(\"BB-clog\", \"BB-logit\"),\n       inset=c(0.01,0.01), lty=c(1,1), col=col5[1:2],\ntext.col=col5[1:2], bty=\"n\",y.intersp=0.85)\n\n\nHCbin.cll &lt;- glmmTMB(cbind(Dead,Live)~0+trtGp/TrtTime+(scTime|trtGp),\n                     family=binomial(link=\"cloglog\"), data=HawCon)\nLTbin.cll &lt;- qra::extractLT(p=0.99, obj=HCbin.cll,\n                            a=1:8, b=9:16, eps=0, df.t=NULL)[,-2]\nrownames(LTbin.cll) &lt;- shorten(rownames(LTbin.cll))\n\n\nA warning against over-interpretation\n\n\n\n\nSection 7.7 Observation level random effects — the moths dataset\n\nmoths &lt;- DAAG::moths\nmoths$transect &lt;- 1:41  # Each row is from a different transect\nmoths$habitat &lt;- relevel(moths$habitat, ref=\"Lowerside\")\nA.glmer &lt;-  glmer(A~habitat+sqrt(meters)+(1|transect),\nfamily=poisson(link=sqrt), data=moths)\nprint(summary(A.glmer), show.resid=FALSE, correlation=FALSE, digits=3)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( sqrt )\nFormula: A ~ habitat + sqrt(meters) + (1 | transect)\n   Data: moths\n\n     AIC      BIC   logLik deviance df.resid \n   212.6    229.7    -96.3    192.6       31 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n transect (Intercept) 0.319    0.564   \nNumber of obs: 41, groups:  transect, 41\n\nFixed effects:\n                 Estimate Std. Error z value   Pr(&gt;|z|)\n(Intercept)        1.7322     0.3513    4.93 0.00000082\nhabitatBank       -2.0415     0.9377   -2.18      0.029\nhabitatDisturbed  -1.0359     0.4071   -2.54      0.011\nhabitatNEsoak     -0.7319     0.4323   -1.69      0.090\nhabitatNWsoak      2.6787     0.5101    5.25 0.00000015\nhabitatSEsoak      0.1178     0.3923    0.30      0.764\nhabitatSWsoak      0.3900     0.5260    0.74      0.458\nhabitatUpperside  -0.3135     0.7549   -0.42      0.678\nsqrt(meters)       0.0675     0.0631    1.07      0.285\n\n\n\nsuppressPackageStartupMessages(library(gamlss))\nA1quasi.glm &lt;- glm(A~habitat, data=moths, family=quasipoisson(link=sqrt))\nAsqrt.lss &lt;- gamlss(A ~ habitat + sqrt(meters), trace=FALSE,\n                    family = NBI(mu.link='sqrt'), data = moths)\nA1.glmer &lt;- glmer(A~habitat+(1|transect), data=moths, family=poisson(link=sqrt))\n\n\nCglm &lt;- coef(summary(A1quasi.glm))\nCglmer &lt;- coef(summary(A1.glmer))\nfitAll &lt;- cbind(\"quasi-Coef\"=Cglm[-1,1], \"quasi-SE\"=Cglm[-1,2],\n  \"NBI-Coef\"=coef(Asqrt.lss)[2:8], \"NBI-SE\"=c(0.94,.41,.43,.51,.39,.53,.75),\n  \"glmer-Coef\"=Cglmer[-1,1], \"glmer-SE\"=Cglmer[-1,2])\nrownames(fitAll) &lt;- substring(rownames(fitAll),8)\nround(fitAll, 2)  # NB, all SEs are for the difference from 'Lowerside'\n\n          quasi-Coef quasi-SE NBI-Coef NBI-SE glmer-Coef glmer-SE\nBank           -2.13     0.86    -2.19   0.94      -1.99     0.95\nDisturbed      -1.07     0.41    -1.00   0.41      -1.13     0.40\nNEsoak         -0.61     0.43    -0.80   0.43      -0.57     0.41\nNWsoak          2.73     0.54     2.67   0.51       2.73     0.51\nSEsoak          0.16     0.41     0.09   0.39       0.19     0.39\nSWsoak          0.45     0.54     0.28   0.53       0.54     0.51\nUpperside       0.23     0.45    -0.41   0.75       0.36     0.43\n\n\n\ndetach(\"package:glmmTMB\", character.only=TRUE)\n\n\n\nSection 7.8 Repeated measures in time\n\nThe theory of repeated measures modeling\n\n\n*Correlation structure\n\n\nDifferent approaches to repeated measures analysis\n\n\nSubsection 7.8.1: Example – random variation between profiles\n\nhumanpower1 &lt;- DAAG::humanpower1\n\n\n\n\n\n\n\n\n\n\n\n## Plot points and fitted lines\nxyplot(o2 ~ wattsPerKg, groups=id, data=humanpower1,\n       par.settings=DAAG::DAAGtheme(color=F), scales=list(tck=0.5),\n       panel=function(x,y,subscripts,groups,...){\n                yhat &lt;- fitted(lm(y ~ groups*x))\n                panel.superpose(x, y, subscripts, groups, pch=1:5, cex=1.2)\n                panel.superpose(x, yhat, subscripts, groups, type=\"b\", cex=0.5)\n                },\n       auto.key=list(columns=5, lines=T),\n       xlab=\"Watts per kilogram\",\n       ylab=expression(\"Oxygen intake (\"*ml.min^{-1}*.kg^{-1}*\")\"))\n\n\nSeparate lines for different athletes\n\n## Calculate intercepts and slopes; plot Slopes vs Intercepts\n## Uses the function lmList() from the lme4 package\nhumanpower1 &lt;- DAAG::humanpower1\nhp.lmList &lt;- lmList(o2 ~ wattsPerKg | id, data=humanpower1)\ncoefs &lt;- coef(hp.lmList)\nround(coefs,3)\n\n  (Intercept) wattsPerKg\n1      -1.155      15.35\n2       1.916      13.65\n3     -12.008      18.81\n4       8.029      11.83\n5      11.553      10.36\n\nc(\"Correlation between intercept and slope\"=cor(coefs[,1],coefs[,2]))\n\nCorrelation between intercept and slope \n                                -0.9975 \n\n\n\n\nA random coefficients model\n\nhp.lmer &lt;- lmer(o2 ~ wattsPerKg + (wattsPerKg | id), data=humanpower1)\n\n\nhp.lmer &lt;- lmer(o2 ~ wattsPerKg + (0+wattsPerKg | id), data=humanpower1)\nprint(summary(hp.lmer), digits=3)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: o2 ~ wattsPerKg + (0 + wattsPerKg | id)\n   Data: humanpower1\n\nREML criterion at convergence: 129.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.9117 -0.8978  0.0598  0.7854  1.5382 \n\nRandom effects:\n Groups   Name       Variance Std.Dev.\n id       wattsPerKg 0.211    0.46    \n Residual            5.776    2.40    \nNumber of obs: 28, groups:  id, 5\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)    1.299      2.220    0.59\nwattsPerKg    14.204      0.715   19.88\n\nCorrelation of Fixed Effects:\n           (Intr)\nwattsPerKg -0.936\n\n\n\nsort(coef(lmList(o2 ~ wattsPerKg | id, data=humanpower1))[,1])\n\n[1] -12.008  -1.155   1.916   8.029  11.553\n\n\n\n\n\nSubsection 7.8.2: Orthodontic measurements on children\n\nOrthodont &lt;- MEMSS::Orthodont\nOrthodont$logdist &lt;- log(Orthodont$distance)\n\n\n\n\n\n\n\n\n\n\n\n## Plot showing pattern of change for each of the 25 individuals\ngph &lt;- xyplot(distance ~ age | Subject, groups=Sex, data=Orthodont,\n              scales=list(x=list(rot=90, tick.number=3), y=list(log=2), tck=0.5), type=c(\"p\",\"r\"),\n              layout=c(11,3))\nupdate(gph, xlab=list(\"Age\", cex=1.4), ylab=list(\"Distance\", cex=1.4),\n       par.settings=DAAG::DAAGtheme(color=FALSE, fontsize=list(text=7, points=4)))\n\n\nPreliminary data exploration\n\n## Use lmList() to find the slopes\nab &lt;- cbind(coef(lmList(distance ~ age | Subject, Orthodont)),\n            coef(lmList(logdist ~ age|Subject, data=Orthodont)))\nnames(ab) &lt;- c(\"a\", \"b\",\"alog\",\"blog\")\n## Obtain intercept at x=mean(x)=11, for each subject.\n## (For each subject, this is independent of the slope)\nab &lt;- within(ab, {ybar = a + b*11; b=b; ylogbar = alog + blog * 11;\n                  blog=blog; sex = substring(rownames(ab), 1 ,1)\n                  })\nbySex &lt;- sapply(split(ab, ab$sex), function(z)range(z$b))\nextremes &lt;- with(ab, ybar %in% range(ybar) | b %in% bySex)\n\n\n\n\n\n\n\n\n\n\n\nfundiff &lt;- function(x)range(x)+diff(range(x))*c(-0.015, 0.04)\nlims &lt;- sapply(subset(ab, select=c('ybar',\"b\",\"ylogbar\",\"blog\")), fundiff)\nplot(b ~ ybar, col=c(F=\"gray40\", M=\"black\")[sex], data=ab,\n     fg=\"gray\", xlim=lims[,\"ybar\"], ylim=lims[,\"b\"],\n     pch=c(F=1, M=3)[sex], xlab=\"Mean distance\", ylab=\"Slope\")\nwith(subset(ab, extremes), text(b ~ ybar,\n    labels=rownames(ab)[extremes], pos=4, xpd=TRUE))\nmtext(side=3, line=0.75,\"A: Profiles for distances\", adj=0)\n# Type 'qqnorm(ab$b)' to see the extent of M13's outlyingness\n## For Panel B, repeat with logdist replacing distance\nplot(blog ~ ylogbar, col=c(F=\"gray40\", M=\"black\")[sex], data=ab, fg=\"gray\",\n     pch=c(F=1, M=3)[sex], xlim=lims[,\"ylogbar\"], ylim=lims[,\"blog\"],\n     xlab=\"Mean log distance\", ylab=\"Slope\")\nwith(subset(ab, extremes),\n     text(blog ~ ylogbar, labels=rownames(ab)[extremes], pos=4, xpd=TRUE))\nmtext(side=3, line=0.75,\"B: Logarithms of distances\", adj=0)\n\n\n## Compare male slopes with female slopes\nextreme.males &lt;- match(c(\"M04\",\"M13\"), rownames(ab))\nwith(ab[-extreme.males,],\nt.test(blog[sex==\"F\"], blog[sex==\"M\"], var.equal=TRUE))\n\n\n    Two Sample t-test\n\ndata:  blog[sex == \"F\"] and blog[sex == \"M\"]\nt = -2.3, df = 23, p-value = 0.03\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.0160529 -0.0009191\nsample estimates:\nmean of x mean of y \n  0.02115   0.02963 \n\n# Specify var.equal=TRUE, to allow comparison with anova output\n\n\n\nA random coefficients model\n\nkeep &lt;- !(Orthodont$Subject%in%c(\"M04\",\"M13\"))\nOrthodont$scAge &lt;- with(Orthodont, age-11)  ## Center values of age\northdiffx.lmer &lt;- lmer(logdist ~ Sex * scAge + (scAge | Subject),\n                       data=Orthodont, subset=keep)\n\n\nrePCA(orthdiffx.lmer)\n\n$Subject\nStandard deviations (1, .., p=2):\n[1] 1.632 0.000\n\nRotation (n x k) = (2 x 2):\n         [,1]     [,2]\n[1,] -0.99993 -0.01153\n[2,] -0.01153  0.99993\n\nattr(,\"class\")\n[1] \"prcomplist\"\n\n\n\northdiff.lmer &lt;- lmer(logdist ~ Sex * scAge + (1 | Subject),\n                      data=Orthodont, subset=keep)\nsummary(orthdiff.lmer)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logdist ~ Sex * scAge + (1 | Subject)\n   Data: Orthodont\n Subset: keep\n\nREML criterion at convergence: -232\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-3.312 -0.510  0.014  0.543  3.945 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Subject  (Intercept) 0.00633  0.0796  \n Residual             0.00238  0.0488  \nNumber of obs: 100, groups:  Subject, 25\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)    3.11451    0.02510  124.11\nSexMale        0.09443    0.03354    2.82\nscAge          0.02115    0.00329    6.42\nSexMale:scAge  0.00849    0.00440    1.93\n\nCorrelation of Fixed Effects:\n            (Intr) SexMal scAge \nSexMale     -0.748              \nscAge        0.000  0.000       \nSexMal:scAg  0.000  0.000 -0.748\n\n\n\nOrthodont2 &lt;- droplevels(subset(Orthodont, keep))\nopt &lt;- options(contrasts=c(\"contr.sum\",\"contr.poly\"))\northdiff.mixed &lt;- afex::mixed(logdist ~ Sex * scAge + (1 | Subject), type=2,\n                              method='S', data=Orthodont2)\n  ## NB `type` refers to type of test, NOT `error` type.\noptions(opt)       # Reset to previous contrasts setting\northdiff.mixed\n\nMixed Model Anova Table (Type 2 tests, S-method)\n\nModel: logdist ~ Sex * scAge + (1 | Subject)\nData: Orthodont2\n     Effect       df          F p.value\n1       Sex 1, 23.00    7.93 **    .010\n2     scAge 1, 73.00 140.72 ***   &lt;.001\n3 Sex:scAge 1, 73.00     3.72 +    .058\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\n\ncontrasts(Orthodont2[['Subject']]) &lt;- 'contr.sum'\ncontrasts(Orthodont2[['Sex']]) &lt;- 'contr.sum'\n\n\northdiffa.lmer &lt;- update(orthdiff.lmer, formula=. ~ . -Sex:scAge)\nAIC(orthdiffa.lmer,orthdiff.lmer)\n\n               df    AIC\northdiffa.lmer  5 -227.4\northdiff.lmer   6 -220.1\n\n\n\n\nCorrelation between successive times\n\nres &lt;- resid(orthdiff.lmer)\nSubject &lt;- factor(Orthodont$Subject[keep])\north.arma &lt;- sapply(split(res, Subject),\n                    function(x)forecast::auto.arima(x)$arma[c(1,6,2)])\northsum &lt;- apply(orth.arma,2,sum)\north.arma[, orthsum&gt;0]\n\n     F08 M06\n[1,]   1   1\n[2,]   0   0\n[3,]   1   1\n\n\n\n\nFitting a sequential correlation structure\n\nlibrary(nlme)\nkeep &lt;- !(Orthodont$Subject%in%c(\"M04\",\"M13\"))\nOrthodont2 &lt;- droplevels(subset(Orthodont,keep))\northdiff.lme &lt;- lme(logdist ~ Sex * scAge, random = ~1|Subject,\n                    cor=corCAR1(0.1, form=~1|Subject), data=Orthodont2)\n## For AR1 models `phi` is the sequential correlation estimate\northdiff.lme$modelStruct$corStruct\n\nCorrelation structure of class corCAR1 representing\n            Phi \n0.0000000003311 \n\n\n\n\n*The variance for the difference in slopes\n\n\n\n\nSection 7.9: Further notes on multilevel models\n\nSubsection 7.9.1: Sources of variation – complication or focus of interest?\n\n\nSubsection 7.9.2: Predictions from models with a complex error structure\n\nConsequences from assuming an overly simplistic error structure\n\n\n\nSubsection 7.9.3: An historical perspective on multilevel models\n\n\nSubsection 7.9.4: Meta-analysis\n\n\nSubsection 7.9.5: Functional data analysis\n\n\nSubsection 7.9.6: Error structure in explanatory variables\n\n\n\nExercises (7.12)\n7.1\n\nn.omit &lt;- 2\ntake &lt;- rep(TRUE, 48)\ntake[sample(1:48,2)] &lt;- FALSE\nkiwishade.lmer &lt;- lmer(yield ~ shade + (1|block) + (1|block:plot),\n                       data = kiwishade,subset=take)\nvcov &lt;- VarCorr(kiwishade.lmer)\nprint(vcov, comp=\"Variance\")\n\n Groups     Name        Variance\n block:plot (Intercept)  2.35   \n block      (Intercept)  3.28   \n Residual               12.45   \n\n\n7.6\n\ncult.lmer &lt;- lmer(ct ~ Cultivar + Dose + factor(year) +\n                       (-1 + Dose | gp), data = DAAG::sorption, REML=TRUE)\ncultdose.lmer &lt;- lmer(ct ~ Cultivar/Dose + factor(year) +\n                           (-1 + Dose | gp), data = DAAG::sorption, REML=TRUE)\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/ch7.R\")\n}",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 7: Multilevel models, and repeated measures</span>"
    ]
  },
  {
    "objectID": "ch8.html",
    "href": "ch8.html",
    "title": "8  Chapter 8: Tree-based Classification and Regression",
    "section": "",
    "text": "Packages required (plus any dependencies)\nDAAG latticeExtra plot rpart rpart.plot MASS ggplot2 car randomForest\nAdditionally, knitr and Hmisc are required in order to process the Rmd source file.\n\nHmisc::knitrSet(basename=\"treebased\", lang='markdown', fig.path=\"figs/g\", w=7, h=7)\noldopt &lt;- options(digits=4, formatR.arrow=FALSE, width=70, scipen=999)\nlibrary(knitr)\n## knitr::render_listings()\nopts_chunk[['set']](cache.path='cache-', out.width=\"80%\", fig.align=\"center\", \n                    fig.show='hold', size=\"small\", ps=10, strip.white = TRUE,\n                    comment=NA, width=70, tidy.opts = list(replace.assign=FALSE))\n\n\nsuppressPackageStartupMessages(library(latticeExtra))\n\n\n\nWhen are tree-based methods appropriate?\n\n\nSection 8.1: Tree-based methods — uses and basic notions\n\nExamples that will be used to demonstrate the methodology\n\n\nSubsection 8.1.1: Detecting email spam~– an initial look\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnam &lt;- c(\"crl.tot\", \"dollar\", \"bang\", \"money\", \"n000\", \"make\")\nnr &lt;- sample(1:dim(DAAG::spam7)[1],500)\nyesno&lt;-DAAG::spam7$yesno[nr]\nspam7a &lt;- DAAG::spam7[nr,c(nam,\"yesno\")]\nformab &lt;- as.formula(paste(paste(nam, collapse='+'), '~ yesno'))\nspamtheme &lt;- DAAG::DAAGtheme(color = TRUE, pch=3)\nlattice::bwplot(formab, data=spam7a, outer=T, horizontal=F, layout=c(7,1),\n  scales=list(relation='free'), ylab=\"\", par.settings=spamtheme,\n  between=list(x=0.5),\n  main=list(\"A: Raw data values\", y=1.0, font=1, cex=1.25))\nspam7b &lt;- cbind(log(spam7a[,-7]+0.001), yesno=spam7a[,7])\nyval &lt;-c(0.001, 0.001,0.01,0.1,1,10,100,1000,10000)\nlattice::bwplot(formab, data=spam7b, outer=T, horizontal=F, layout=c(7,1),\n                scales=list(relation='free', \n                y=list(at=log(yval+0.001), labels=yval, rot=90)),\n                ylab=\"\", par.settings=spamtheme, between=list(x=0.5),\nmain=list(expression(\"B: Boxplots, using \"*log(x+001)*\" scale\"),\n          y=1.0, font=1, cex=1.25))\n\n\n## Obtain 500-row sample; repeat the first plot (of crl.tot)\nspam.sample &lt;- spam7[sample(seq(1,4601), 500, replace=FALSE), ]\nboxplot(split(spam.sample$crl.tot, spam.sample$yesno))\n\n\n\n\n\n\n\n\n\n\n\nsuppressMessages(library(rpart))\nset.seed(31)      ## Reproduce tree shown in text\nspam.rpart &lt;- rpart(formula = yesno ~ crl.tot + dollar + bang + money + n000 +\n                    make,  method=\"class\", model=TRUE, data=DAAG::spam7)\nrpart.plot::rpart.plot(spam.rpart, type=0, under=TRUE, branch.lwd=0.4,\n                       nn.lwd=0.4, box.palette=\"auto\", tweak=1.25)\n\n\nprintcp(spam.rpart, digits=3)\n\n\nClassification tree:\nrpart(formula = yesno ~ crl.tot + dollar + bang + money + n000 + \n    make, data = DAAG::spam7, method = \"class\", model = TRUE)\n\nVariables actually used in tree construction:\n[1] bang    crl.tot dollar \n\nRoot node error: 1813/4601 = 0.394\n\nn= 4601 \n\n      CP nsplit rel error xerror   xstd\n1 0.4766      0     1.000  1.000 0.0183\n2 0.0756      1     0.523  0.547 0.0154\n3 0.0116      3     0.372  0.388 0.0135\n4 0.0105      4     0.361  0.384 0.0134\n5 0.0100      5     0.350  0.382 0.0134\n\n\n\n\nSubsection 8.1.2: Choosing the number of splits\n\n\n\nSection 8.2: Splitting criteria, with illustrative examples\n\n\n\n\n\n\n\n\n\n\ntree.df &lt;- data.frame(fac = factor(rep(c('f1','f2'), 3)),\nx = rep(1:3, rep(2, 3)), Node = 1:6)\nu.tree &lt;- rpart(Node ~ fac + x, data = tree.df,\n                control = list(minsplit = 2, minbucket = 1, cp = 1e-009))\nrpart.plot::rpart.plot(u.tree, type=0, under=TRUE, branch.lwd=0.25,\n                       nn.lwd=0.25, box.palette=\"Grays\", tweak=1.6)\n\n\nChoosing the split~– regression trees\n\n\nSubsection 8.2.1: Within and between sums of squares\n\n\nSubsection 8.2.2: Choosing the split~– classification trees\n\n\nSubsection 8.2.3: Tree-based regression versus loess regression smoothing\n\n\n\n\n\n\n\n\n\n\nu.lo &lt;- loess(Mileage~Weight, data = car.test.frame, span = 2/3)\nplot(Mileage~Weight, data=car.test.frame, xlab = \"Weight\",\n     ylab = \"Miles per gallon\", sub = \"\", fg=\"gray\")\nxy &lt;- with(car.test.frame, loess.smooth(Weight, Mileage))\nord&lt;-order(xy$x)\nlines(xy$x[ord],xy$y[ord])\n\n\n## loess fit to Mileage vs Weight: data frame car.test.frame (rpart)\nwith(rpart::car.test.frame, scatter.smooth(Mileage ~ Weight))\n\n\n\n\n\n\n\n\n\n\n\npar(fig=c(0, 0.32, 0,1))\nset.seed(37)\ncar.tree &lt;- rpart(Mileage ~ Weight, data = car.test.frame)\nrpart.plot::rpart.plot(car.tree, type=0, under=TRUE,\n                       box.palette=\"Grays\", tweak=1.05)\npar(fig=c(0.3,1, 0,1), new=TRUE)\nset.seed(37)\ncar2.tree &lt;- rpart(Mileage~Weight, data=car.test.frame, control =\n                   list(minsplit = 10, minbucket = 5, cp = 0.0001))\nrpart.plot::rpart.plot(car2.tree, type=0, under=TRUE,\nbox.palette=\"auto\", tweak=1.05)\n\n\n## Panel A: Split criteria were left a their defaults\ncar.tree &lt;- rpart(Mileage ~ Weight, data = car.test.frame)\nrpart.plot::rpart.plot(car.tree, type=0, under=TRUE)\n## Panel B: Finer grained splits\ncar2.tree &lt;- rpart(Mileage ~ Weight, data=car.test.frame, method=\"anova\",\n                   control = list(minsplit = 10, minbucket = 5, cp = 0.0001))\n## See `?rpart::rpart.control` for details of control options.\n\n\ndat &lt;- data.frame(Weight=seq(from=min(car.test.frame$Weight),\nto=max(car.test.frame$Weight)))\npred &lt;- predict(car.tree, newdata=dat)\npred2 &lt;- predict(car2.tree, newdata=dat)\nlwr &lt;- dat$Weight[c(1,diff(pred)) != 0]\nupr &lt;- dat$Weight[c(diff(pred),1) != 0]\nxy2 &lt;- with(car.test.frame, loess.smooth(Weight, Mileage, evaluation=2011))\nlwrLO &lt;- xy2$y[c(1,diff(pred)) != 0]\nuprLO &lt;- xy2$y[c(diff(pred),1) != 0]\nround(rbind(lwr,upr,lwrLO,uprLO,\npred[c(diff(pred),1)!=0],pred2[c(diff(pred),1)!=0]),1)\n\n         723    903   1243   2011\nlwr   1845.0 2568.0 2748.0 3088.0\nupr   2567.0 2747.0 3087.0 3855.0\nlwrLO   36.2   27.1   25.1   22.3\nuprLO   27.1   25.1   22.3   18.8\n        30.9   25.6   23.8   20.4\n        28.9   25.6   24.1   18.7\n\n\n\n\nSubsection 8.2.4: Predictive accuracy, and the cost-complexity tradeoff\n\n\nSubsection 8.2.5: Cross-validation\n\n\nSubsection 8.2.6: The cost-complexity parameter\n\nvignette(\"longintro\", package=\"rpart\")\n\n\n\nSubsection 8.2.7: Prediction error versus tree size\n\n\n\n\n\n\n\n\n\n\n\n\nSection 8.3: The practicalities of tree construction – two examples\n\nSubsection 8.3.1: Data for female heart attack patients\n\nmifem &lt;- DAAG::mifem\nsummary(mifem)     # data frame mifem (DAAG)\n\n outcome         age          yronset     premi    smstat   diabetes\n live:974   Min.   :35.0   Min.   :85.0   y :311   c :390   y :248  \n dead:321   1st Qu.:57.0   1st Qu.:87.0   n :928   x :280   n :978  \n            Median :63.0   Median :89.0   nk: 56   n :522   nk: 69  \n            Mean   :60.9   Mean   :88.8            nk:103           \n            3rd Qu.:66.0   3rd Qu.:91.0                             \n            Max.   :69.0   Max.   :93.0                             \n highbp   hichol   angina   stroke   \n y :813   y :452   y :472   y : 153  \n n :406   n :655   n :724   n :1063  \n nk: 76   nk:188   nk: 99   nk:  79  \n                                     \n                                     \n                                     \n\n\n\nset.seed(29)      # Make results reproducible\nmifem.rpart &lt;- rpart(outcome ~ ., method=\"class\", data = mifem, cp = 0.0025)\n\n\n## Tabular equivalent of Panel A from `plotcp(mifem.rpart)`\nprintcp(mifem.rpart, digits=3)\n\n\nClassification tree:\nrpart(formula = outcome ~ ., data = mifem, method = \"class\", \n    cp = 0.0025)\n\nVariables actually used in tree construction:\n[1] age      angina   diabetes hichol   premi    smstat   stroke  \n[8] yronset \n\nRoot node error: 321/1295 = 0.248\n\nn= 1295 \n\n       CP nsplit rel error xerror   xstd\n1 0.20249      0     1.000  1.000 0.0484\n2 0.00561      1     0.798  0.829 0.0453\n3 0.00467     13     0.717  0.875 0.0462\n4 0.00312     17     0.698  0.860 0.0459\n5 0.00250     18     0.695  0.863 0.0460\n\n\n\ncat(c(\". . .\", capture.output(printcp(mifem.rpart, digits=3))[-(1:9)]), \n    sep=\"\\n\")\n\n. . .\nRoot node error: 321/1295 = 0.248\n\nn= 1295 \n\n       CP nsplit rel error xerror   xstd\n1 0.20249      0     1.000  1.000 0.0484\n2 0.00561      1     0.798  0.829 0.0453\n3 0.00467     13     0.717  0.875 0.0462\n4 0.00312     17     0.698  0.860 0.0459\n5 0.00250     18     0.695  0.863 0.0460\n\n\n\n\n\n\n\n\n\n\n\n\nplotcp(mifem.rpart, fg=\"gray\", tcl=-0.25)\nmifemb.rpart &lt;- prune(mifem.rpart, cp=0.01)  ## Prune tree back to 2 leaves\nrpart.plot::rpart.plot(mifemb.rpart, under=TRUE, type=4,\n                       box.palette=0, tweak=1.0)\n\n\n\nSubsection 8.3.2: The one-standard-deviation rule\n\n\nSubsection 8.3.3: Printed Information on Each Split\n\nprint(mifemb.rpart)\n\nn= 1295 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 1295 321 live (0.7521 0.2479)  \n  2) angina=y,n 1196 239 live (0.8002 0.1998) *\n  3) angina=nk 99  17 dead (0.1717 0.8283) *\n\n\n\nset.seed(59)\nspam7a.rpart &lt;- rpart(formula = yesno ~ crl.tot + dollar + bang +\n                      money + n000 + make, method=\"class\", cp = 0.002,\n                      model=TRUE, data = DAAG::spam7)\n\n\nprintcp(spam7a.rpart, digits=3)\n\n\nClassification tree:\nrpart(formula = yesno ~ crl.tot + dollar + bang + money + n000 + \n    make, data = DAAG::spam7, method = \"class\", model = TRUE, \n    cp = 0.002)\n\nVariables actually used in tree construction:\n[1] bang    crl.tot dollar  money   n000   \n\nRoot node error: 1813/4601 = 0.394\n\nn= 4601 \n\n        CP nsplit rel error xerror   xstd\n1  0.47656      0     1.000  1.000 0.0183\n2  0.07557      1     0.523  0.550 0.0154\n3  0.01158      3     0.372  0.390 0.0135\n4  0.01048      4     0.361  0.386 0.0134\n5  0.00634      5     0.350  0.374 0.0133\n6  0.00552     10     0.317  0.360 0.0130\n7  0.00441     11     0.311  0.357 0.0130\n8  0.00386     12     0.307  0.352 0.0129\n9  0.00276     16     0.291  0.339 0.0127\n10 0.00221     17     0.288  0.339 0.0127\n11 0.00200     18     0.286  0.336 0.0127\n\n\n\ncpdf &lt;- signif(as.data.frame(spam7a.rpart$cptable),3)\nminRow &lt;- which.min(cpdf[,\"xerror\"])\nupr &lt;- sum(cpdf[minRow, c(\"xerror\",\"xstd\")])\ntakeRow &lt;- min((1:minRow)[cpdf[1:minRow,\"xerror\"]&lt;upr])\nnewNsplit &lt;- cpdf[takeRow, 'nsplit']\ncpval &lt;- mean(cpdf[c(takeRow-1,takeRow),\"CP\"])\n\n\n\n\n\n\n\n\n\n\n\nspam7b.rpart &lt;- prune(spam7a.rpart, cp=cpval)\nrpart.plot::rpart.plot(spam7b.rpart, under=TRUE, box.palette=\"Grays\", tweak=1.65)\n\n\nHow does the one standard error rule affect accuracy of estimates?\n\nrequireNamespace('randomForest', quietly=TRUE)\nDAAG::compareTreecalcs(data=DAAG::spam7, fun=\"rpart\")\n\n rpSEcvI    rpcvI rpSEtest   rptest  nSErule   nREmin \n  0.1396   0.1387   0.1269   0.1217   7.0000   8.0000 \n\n\n\nacctree.mat &lt;- matrix(0, nrow=100, ncol=6)\nspam7 &lt;- DAAG::spam7\nfor(i in 1:100)\nacctree.mat[i,] &lt;- DAAG::compareTreecalcs(data=spam7, fun=\"rpart\")\n\n\n\nHow is the standard error calculated?\n\n\nWhen are tree-based methods appropriate?\n\n\n\n\nSection 8.4: From one tree to a forest – a more global optimality\n\nsuppressPackageStartupMessages(library(randomForest))\nspam7.rf &lt;- randomForest(yesno ~ ., data=spam7, importance=TRUE)\nspam7.rf\n\n\nCall:\n randomForest(formula = yesno ~ ., data = spam7, importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 11.56%\nConfusion matrix:\n     n    y class.error\nn 2651  137     0.04914\ny  395 1418     0.21787\n\n\n\nz &lt;- tuneRF(x=spam7[, -7], y=spam7$yesno, plot=FALSE)\n\nmtry = 2  OOB error = 11.8% \nSearching left ...\nmtry = 1    OOB error = 12.67% \n-0.07366 0.05 \nSearching right ...\nmtry = 4    OOB error = 11.76% \n0.003683 0.05 \n\n\n\nzdash &lt;- t(z[,2,drop=F])\ncolnames(zdash) &lt;- paste0(c(\"mtry=\",rep(\"\",2)), z[,1])\nround(zdash,3)\n\n         mtry=1     2     4\nOOBError  0.127 0.118 0.118\n\n\n\nimportance(spam7.rf)\n\n            n      y MeanDecreaseAccuracy MeanDecreaseGini\ncrl.tot 46.73  54.19                70.57           248.10\ndollar  56.21  55.35                76.13           431.75\nbang    91.66 100.46               115.95           588.53\nmoney   33.09  51.87                53.49           206.51\nn000    58.25  15.74                62.29           115.46\nmake    13.67  21.76                26.72            41.13\n\n\n\nSubsection 8.4.1: Prior probabilities\n\nPima.tr &lt;- MASS::Pima.tr\ntable(Pima.tr$type)\n\n\n No Yes \n132  68 \n\n\n\nset.seed(41)     # This seed should reproduce the result given here\nPima.rf &lt;- randomForest(type ~ ., data=Pima.tr)\n## The above is equivalent to:\n## Pima.rf &lt;- randomForest(type ~ ., data=Pima.tr, sampsize=200)\nround(Pima.rf$confusion,3)\n\n     No Yes class.error\nNo  110  22       0.167\nYes  32  36       0.471\n\n\n\ntab &lt;- prop.table(table(Pima.tr$type))\n\n\nPima.rf &lt;- randomForest(type ~ ., data=Pima.tr, sampsize=c(68,68))\n\n\nPima.rf &lt;- randomForest(type ~ ., data=Pima.tr, sampsize=c(132,68))\n\n\n\nSubsection 8.4.2: A low-dimensional representation of observations\n\n\nSubsection 8.4.3: Models with a complex error structure\n\n\n\nSection 8.5: Additional notes – one tree, or many trees?\n\nSubsection 8.5.1: Differences between rpart() and randomForest()\n\nError rates – rpart() versus randomForest()\n\n\n\n\n\n\n\n\n\n\n## Accuracy comparisons\nacctree.mat &lt;- matrix(0, nrow=100, ncol=8)\ncolnames(acctree.mat) &lt;- c(\"rpSEcvI\", \"rpcvI\", \"rpSEtest\", \"rptest\",\n                          \"n.SErule\", \"nre.min.12\", \"rfOOBI\", \"rftest\")\nfor(i in 1:100)acctree.mat[i,] &lt;- DAAG::compareTreecalcs(data=spam7, cp=0.0004,\n                                          fun=c(\"rpart\", \"randomForest\"))\nacctree.df &lt;- data.frame(acctree.mat)\nlims &lt;- range(acctree.mat[, c(4,7,8)], na.rm=TRUE)\ncthrublack &lt;- adjustcolor(\"black\", alpha.f=0.75)\n# Panel A\nplot(rfOOBI ~ rftest, data=acctree.df, xlab=\"Error rate - subset II\", xlim=lims,\n     ylim=lims, ylab=\"OOB Error - fit to subset I\", col=cthrublack, fg=\"gray\")\nabline(0,1)\nmtext(side=3, line=0.5, \"A\", adj=0)\n# Panel B\nplot(rptest ~ rftest, data=acctree.df, xlab=\"Error rate - subset II\",\n     ylab=\"rpart Error rate, subset II\", xlim=lims, ylim=lims,\n     col=cthrublack, fg=\"gray\")\nabline(0,1)\nmtext(side=3, line=0.5, \"B\", adj=0)\n\n\nacctree.mat &lt;- matrix(0, nrow=100, ncol=8)\ncolnames(acctree.mat) &lt;- c(\"rpSEcvI\", \"rpcvI\", \"rpSEtest\", \"rptest\",\n                          \"n.SErule\", \"nre.min.12\", \"rfcvI\", \"rftest\")\nfor(i in 1:100)acctree.mat[i,] &lt;- DAAG::compareTreecalcs(data=spam7,\n                                          fun=c(\"rpart\", \"randomForest\"))\n## Panel A: Plot `rfOOBI` against `rftest`\n## Panel B: Plot `rptest` against `rftest`\n\n\n\nTimes required for computation\n\n\n\nSubsection 8.5.2: Tree-based methods, versus other approaches\n\nTree-based methods may usefully complement other approaches\n\n\n\nSubsection 8.5.3: Further notes\n\nPruning as variable selection\n\n\n\n\nSection 8.6: Further reading and extensions\n\n\nExercises (8.7)\n8.5\n\nsapply(MASS::biopsy, function(x)sum(is.na(x)))   ## Will omit rows with NAs\n\n   ID    V1    V2    V3    V4    V5    V6    V7    V8    V9 class \n    0     0     0     0     0     0    16     0     0     0     0 \n\nbiops &lt;- na.omit(MASS::biopsy)[,-1]               ## Omit also column 1 (IDs)\n## Examine list element names in randomForest object\nnames(randomForest(class ~ ., data=biops))\n\n [1] \"call\"            \"type\"            \"predicted\"      \n [4] \"err.rate\"        \"confusion\"       \"votes\"          \n [7] \"oob.times\"       \"classes\"         \"importance\"     \n[10] \"importanceSD\"    \"localImportance\" \"proximity\"      \n[13] \"ntree\"           \"mtry\"            \"forest\"         \n[16] \"y\"               \"test\"            \"inbag\"          \n[19] \"terms\"          \n\n\n8.5a\n\n## Repeated runs, note variation in OOB accuracy.\nfor(i in 1:10) {\n  biops.rf &lt;- randomForest(class ~ ., data=biops)  \n  OOBerr &lt;- mean(biops.rf$err.rate[,\"OOB\"])\n  print(paste(i, \": \", round(OOBerr, 4), sep=\"\"))\n  print(round(biops.rf$confusion,4))\n}\n\n[1] \"1: 0.0288\"\n          benign malignant class.error\nbenign       432        12      0.0270\nmalignant      7       232      0.0293\n[1] \"2: 0.0344\"\n          benign malignant class.error\nbenign       431        13      0.0293\nmalignant      9       230      0.0377\n[1] \"3: 0.0308\"\n          benign malignant class.error\nbenign       433        11      0.0248\nmalignant      9       230      0.0377\n[1] \"4: 0.0307\"\n          benign malignant class.error\nbenign       431        13      0.0293\nmalignant      6       233      0.0251\n[1] \"5: 0.0311\"\n          benign malignant class.error\nbenign       433        11      0.0248\nmalignant      8       231      0.0335\n[1] \"6: 0.0301\"\n          benign malignant class.error\nbenign       431        13      0.0293\nmalignant      6       233      0.0251\n[1] \"7: 0.0312\"\n          benign malignant class.error\nbenign       433        11      0.0248\nmalignant      7       232      0.0293\n[1] \"8: 0.0301\"\n          benign malignant class.error\nbenign       433        11      0.0248\nmalignant      8       231      0.0335\n[1] \"9: 0.0285\"\n          benign malignant class.error\nbenign       433        11      0.0248\nmalignant      6       233      0.0251\n[1] \"10: 0.0301\"\n          benign malignant class.error\nbenign       432        12      0.0270\nmalignant      7       232      0.0293\n\n\n8.5b\n\n## Repeated train/test splits: OOB accuracy vs test set accuracy.\nfor(i in 1:10){\n  trRows &lt;- sample(1:dim(biops)[1], size=round(dim(biops)[1]/2))\n  biops.rf &lt;- randomForest(class ~ ., data=biops[trRows, ],\n    xtest=biops[-trRows,-10], ytest=biops[-trRows,10])\n  oobErr &lt;- mean(biops.rf$err.rate[,\"OOB\"])\n  testErr &lt;- mean(biops.rf$test$err.rate[,\"Test\"])\nprint(round(c(oobErr,testErr),4))\n}\n\n[1] 0.0282 0.0340\n[1] 0.0366 0.0327\n[1] 0.0360 0.0226\n[1] 0.0330 0.0311\n[1] 0.0360 0.0384\n[1] 0.0357 0.0353\n[1] 0.0248 0.0399\n[1] 0.0442 0.0270\n[1] 0.0407 0.0268\n[1] 0.0347 0.0242\n\n\n8.5c\n\nrandomForest(class ~ ., data=biops, xtest=biops[,-10], ytest=biops[,10])\n\n\nCall:\n randomForest(formula = class ~ ., data = biops, xtest = biops[,      -10], ytest = biops[, 10]) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 3\n\n        OOB estimate of  error rate: 2.93%\nConfusion matrix:\n          benign malignant class.error\nbenign       433        11     0.02477\nmalignant      9       230     0.03766\n                Test set error rate: 0%\nConfusion matrix:\n          benign malignant class.error\nbenign       444         0           0\nmalignant      0       239           0\n\n\n8.7\n\n## Run model on total data\nrandomForest(as.factor(type) ~ ., data=Pima.tr)\n\n\nCall:\n randomForest(formula = as.factor(type) ~ ., data = Pima.tr) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 28%\nConfusion matrix:\n     No Yes class.error\nNo  109  23      0.1742\nYes  33  35      0.4853\n\nrowsamp &lt;- sample(dim(Pima.tr)[1], replace=TRUE)\nrandomForest(as.factor(type) ~ ., data=Pima.tr[rowsamp, ])\n\n\nCall:\n randomForest(formula = as.factor(type) ~ ., data = Pima.tr[rowsamp,      ]) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 8.5%\nConfusion matrix:\n     No Yes class.error\nNo  129   3     0.02273\nYes  14  54     0.20588\n\n\n8.8a\n\nd500 &lt;- ggplot2::diamonds[sample(1:nrow(ggplot2::diamonds), 500),]\nunlist(sapply(d500, class))  # Check the class of the 10 columns\n\n    carat      cut1      cut2    color1    color2  clarity1  clarity2 \n\"numeric\" \"ordered\"  \"factor\" \"ordered\"  \"factor\" \"ordered\"  \"factor\" \n    depth     table     price         x         y         z \n\"numeric\" \"numeric\" \"integer\" \"numeric\" \"numeric\" \"numeric\" \n\ncar::spm(d500)       # If screen space is limited do two plots, thus:\n  # 1) variables 1 to 5 and 7 (`price`); 2) variables 6 to 10\nplot(density(d500[, \"price\", drop = T]))         # Distribution is highly skew\nMASS::boxcox(price~., data=ggplot2::diamonds)  # Suggests log transformation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.8b\n\ndiamonds &lt;- ggplot2::diamonds; Y &lt;- diamonds[,\"price\", drop=T]\nlibrary(rpart)\nd7.rpart &lt;- rpart(log(Y) ~ ., data=diamonds[,-7], cp=5e-7) # Complex tree\nd.rpart &lt;- prune(d7.rpart, cp=0.0025)            \nprintcp(d.rpart)   # Relative to `d7.rpart`, simpler and less accurate\n\n\nRegression tree:\nrpart(formula = log(Y) ~ ., data = diamonds[, -7], cp = 0.0000005)\n\nVariables actually used in tree construction:\n[1] carat   clarity color   x       y      \n\nRoot node error: 55531/53940 = 1\n\nn= 53940 \n\n       CP nsplit rel error xerror    xstd\n1  0.7244      0     1.000  1.000 0.00409\n2  0.0885      1     0.276  0.276 0.00141\n3  0.0661      2     0.187  0.188 0.00104\n4  0.0290      3     0.121  0.121 0.00075\n5  0.0105      4     0.092  0.092 0.00059\n6  0.0072      5     0.082  0.082 0.00054\n7  0.0071      6     0.074  0.072 0.00049\n8  0.0030      7     0.067  0.068 0.00047\n9  0.0026      8     0.064  0.065 0.00046\n10 0.0026      9     0.062  0.061 0.00044\n11 0.0025     10     0.059  0.059 0.00044\n\nnmin &lt;- which.min(d7.rpart$cptable[,'xerror'])\ndOpt.rpart &lt;- prune(d7.rpart, cp=d7.rpart$cptable[nmin,'CP'])\nprint(dOpt.rpart$cptable[nmin])\n\n[1] 0.0000008743\n\n(xerror12 &lt;- dOpt.rpart$cptable[c(nrow(d.rpart$cptable),nmin), \"xerror\"])\n\n     11    1931 \n0.05934 0.01074 \n\n ## Subtract from 1.0 to obtain R-squared statistics\n\n\nrbind(\"d.rpart\"=d.rpart[['variable.importance']],\n      \"dOpt.rpart\"=dOpt.rpart[['variable.importance']]) |&gt;\n  (\\(x)100*apply(x,1,function(x)x/sum(x)))() |&gt; round(1) |&gt; t()\n\n              y    x carat    z clarity table color depth cut\nd.rpart    23.8 23.1  22.9 22.4     4.8   2.8   0.2   0.1 0.1\ndOpt.rpart 23.5 22.9  22.7 22.2     5.2   2.7   0.5   0.2 0.1\n\n\n8.9\n\nY &lt;- ggplot2::diamonds[,\"price\", drop=T]\nsamp5K &lt;- sample(1:nrow(diamonds), size=5000)\n(diamond5K.rf &lt;- randomForest(x=diamonds[samp5K,-7], y=log(Y[samp5K]),\n                   xtest=diamonds[-samp5K,-7], ytest=log(Y[-samp5K])))\n\n\nCall:\n randomForest(x = diamonds[samp5K, -7], y = log(Y[samp5K]), xtest = diamonds[-samp5K,      -7], ytest = log(Y[-samp5K])) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 3\n\n          Mean of squared residuals: 0.01434\n                    % Var explained: 98.59\n                       Test set MSE: 0.01\n                    % Var explained: 98.65\n\n## Omit arguments `xtest` and `ytest` if calculations take too long\n\n\nsort(importance(diamond5K.rf)[,1], decreasing=T) |&gt; \n  (\\(x)100*x/sum(x))() |&gt; round(1) |&gt; t()\n\n        y carat  x    z clarity color depth table cut\n[1,] 33.5  23.6 21 17.2     2.6   1.2   0.4   0.3 0.2\n\n\n8.9a\n\n(diamond5KU.rf &lt;- randomForest(x=diamonds[samp5K,-7], y=Y[samp5K],\n                   xtest=diamonds[-samp5K,-7], ytest=Y[-samp5K]))\n\n\nCall:\n randomForest(x = diamonds[samp5K, -7], y = Y[samp5K], xtest = diamonds[-samp5K,      -7], ytest = Y[-samp5K]) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 3\n\n          Mean of squared residuals: 450742\n                    % Var explained: 97.11\n                       Test set MSE: 493916\n                    % Var explained: 96.9\n\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/ch8.R\")\n}",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 8: Tree-based Classification and Regression</span>"
    ]
  },
  {
    "objectID": "ch9.html",
    "href": "ch9.html",
    "title": "9  Chapter 9: Multivariate data exploration and discrimination",
    "section": "",
    "text": "Packages required (plus any dependencies)\nPackages used are: DAAG MASS RColorBrewer teigen BiocManager DAAGbio hddplot lmtest splines cobalt mice datasets car micemd oz randomForest ggplot2 latticeExtra mvtnorm teigen limma hddplot mgcv MatchIt sandwich gridExtra DAAGbio mlbench (exercise).\nAdditionally, knitr and Hmisc are required in order to process the Rmd source file.\n\nHmisc::knitrSet(basename=\"mva\", lang='markdown', fig.path=\"figs/g\", w=7, h=7)\noldopt &lt;- options(digits=4, formatR.arrow=FALSE, width=70, scipen=999)\nlibrary(knitr)\nopts_chunk[['set']](cache.path='cache-', out.width=\"80%\", fig.align=\"center\", \n                    fig.show='hold', ps=10, strip.white = TRUE,\n                    comment=NA, width=70, tidy.opts = list(replace.assign=FALSE))\n\n\n\nSection 9.1: Multivariate exploratory data analysis\n\n## Make the lattice package and the possum dataset available\nlibrary(latticeExtra)\npossum &lt;- DAAG::possum\n\n\nSubsection 9.1.1: Scatterplot matrices\n\n## Colors distinguish sexes; symbols distinguish sites\nsitenames &lt;- row.names(DAAG::possumsites)[c(1,2,4:6,3,7)]\nkey &lt;- list(points = list(pch=0:6), text=list(sitenames),\n            columns=4, between=1, between.columns=2)\ncolr &lt;- c(\"red\",\"blue\")\nvnames &lt;- c(\"tail\\nlength\",\"foot\\nlength\", \"conch\\nlength\")\ngphA &lt;- with(possum, splom(~ possum[, 9:11], pch=(0:6)[site], col=colr[sex],\n            xlab=\"\",  varnames=vnames, key=key, axis.line.tck=0.6))\ngphB &lt;- with(possum, cloud(earconch~taill+footlgth, data=possum, \n  col=colr[sex], key=key, pch = (0:6)[site], \n  zlab=list(\"earconch\", rot=90), zoom=0.925))\nupdate(c(\"A: Scatterplot matrix\"=gphA, \"B: Cloud plot\"=gphB),\n       between=list(x=1))\n\n\n\nSubsection 9.1.2: Principal components analysis\n\nPreliminary data scrutiny\n\n## Ratios of largest to smallest values: possum[, 6:14] (DAAG)\npossum &lt;- DAAG::possum\nsapply(na.omit(possum[, 6:14]), function(x)round(max(x)/min(x),2))\n\n\n## Principal components calculations: possum[, 6:14] (DAAG)\nhere &lt;- complete.cases(possum[, 6:14])\npossum.prc &lt;- prcomp(log(possum[here, 6:14]))\nscores &lt;- cbind(predict(possum.prc), possum[here, c('sex', 'site')])\n\n\n## For parset, key and colr; see code for Fig 9.1\npchr &lt;- c(3,4,0,8,2,10,1)\nparset &lt;- list(fontsize=list(text=10, points=6), cex=0.75, pch=pchr, alpha=0.8)\nkey &lt;- modifyList(key, list(columns=1, space=\"right\"))\ngph &lt;- with(scores, xyplot(PC2 ~ PC1, aspect=\"iso\", key = key,\n            col = colr[sex], pch = (0:6)[site]))\nupdate(gph, scales=list(tck=0.5), par.settings=parset,\n       xlab=\"1st Principal Component\", ylab=\"2nd Principal Component\")\n\n\nprint(summary(possum.prc),digits=2)\ncat(\"\\nRotations (otherwise called Loadings)\\n\")\nprint(possum.prc$rotation, digits=2)\n## By default, blanks are shown for loadings &lt; 0.1 in magnitude\n\n\n\nThe stability of the principal components plot\n\nsuppressPackageStartupMessages(library(ggplot2))\ntheme_set(theme_gray(base_size=8))\n## Bootstrap principal components calculations: possum (DAAG)\n## Sample from rows where there are no missing values\nrowsfrom &lt;- (1:nrow(possum))[complete.cases(possum[, 6:14])]\nlogpossum6to14 &lt;- log(possum[rowsfrom, 6:14])\nsexPop &lt;- possum[rowsfrom, c(\"sex\",\"Pop\")]\nn &lt;- length(rowsfrom); ntimes &lt;- 3\nbootscores &lt;- data.frame(scores1=numeric(ntimes*n), scores2=numeric(ntimes*n))\nfor (i in 1:ntimes){\n  samprows &lt;- sample(1:n, n, replace=TRUE)\n  bootscores[n*(i-1)+(1:n), 1:2] &lt;-\n  prcomp(logpossum6to14[samprows, ])$x[, 1:2]\n}\nbootscores[, c(\"sex\",\"Pop\")] &lt;- sexPop[samprows, ]\nbootscores$sampleID &lt;- factor(rep(1:ntimes, rep(n,ntimes)))\ngph &lt;- quickplot(x=scores1, y=scores2, colour=sex, size=I(1.0),\n  asp=1, shape=Pop, facets=.~sampleID, data=bootscores) + \n  scale_shape_discrete(solid=F)\ngph + scale_colour_manual(values=c(\"m\"=\"blue\",\"f\"=\"red\"))  +\n  xlab(\"First Principal Component\") + ylab(\"Second Principal Component\")\n\n\n\n\nSubsection 9.1.3: Multi-dimensional scaling\n\nDistance measures\n\n\nOrdination\n\n## Code that will display individual graphs\nd.possum &lt;- dist(possum[,6:14])  # Euclidean distance matrix\nMASS::sammon(d.possum, k=2, trace=FALSE)$points |&gt; as.data.frame() |&gt;\n  setNames(paste0(\"ord\",1:2)) |&gt; cbind(Pop=DAAG::possum$Pop) -&gt; sammon.possum\nMASS::isoMDS(d.possum, k=2, trace=FALSE) |&gt; as.data.frame() |&gt; \n  setNames(paste0(\"ord\",1:2)) |&gt; cbind(Pop=DAAG::possum$Pop) -&gt; mds.possum\ngph1 &lt;- xyplot(ord2~ord1, groups=Pop, aspect=\"iso\", data=sammon.possum)\ngph2 &lt;- xyplot(ord2~ord1, groups=Pop, aspect=\"iso\", data=mds.possum)\nupdate(c(gph1, gph2, layout=c(2,1)), \n       par.settings=simpleTheme(pch=c(1,3)),\n       between=list(x=0.5), auto.key=list(columns=2),\n       strip=strip.custom(factor.levels=c(\"A: Sammon\",\"B: ISOmds\")))\n\n\n\nBinary data\n\n\n\n\nSection 9.2: Principal component scores in regression\n\n## Principal components: data frame socsupport (DAAG)\nsocsupport &lt;- DAAG::socsupport\nss.pr1 &lt;- prcomp(as.matrix(na.omit(socsupport[, 9:19])), retx=TRUE, scale=TRUE)\n\n\noldpar &lt;- par(fg='gray40',col.axis='gray20',lwd=0.5,col.lab='gray20')\npairs(ss.pr1$x[, 1:3], col='gray40', gap=0.2)\npar(oldpar)\n\n\nsummary(sort(ss.pr1$rotation[,1]))\n## Note the very large maximum value\nwhich.max(ss.pr1$x[,1])\n## Try also boxplot(ss.pr1$x[,1])\n## ss.pr1$x[\"36\",1]  ## Check that this returns 42\n\n\nuse &lt;- complete.cases(socsupport[, 9:19])\nuse[36] &lt;- FALSE\nss.pr &lt;- prcomp(as.matrix(socsupport[use, 9:19]))\n\n\n## Output from summary()\nprint(summary(ss.pr), digits=1)  # Compare contributions\n\n\ncomp &lt;- as.data.frame(ss.pr$x[,1:6])\nss.lm &lt;- lm(socsupport[use, \"BDI\"] ~ ., data=comp)\nsignif(round(coef(summary(ss.lm)),5), digits=3)\n\n\nprint(ss.pr$rotation[, 1], digits=2)\n\n\n## Plot BDI against first principal component score\ngph &lt;- xyplot(BDI ~ ss.pr$x[ ,1], groups=gender, data=socsupport[use,],\npar.settings=simpleTheme(pch=1:2), auto.key=list(columns=2))\nbw9 &lt;- list(pch=c(1,3), list(text=9, points=5))\nupdate(gph, scales=list(tck=0.5), par.settings=bw9,\nxlab =\"1st principal component\")\n\n\n\nSection 9.3: Cluster analysis\n\nSubsection 9.3.1: Hierarchical Clustering\n\nlibrary(mvtnorm)\nmakeClust &lt;- function(n=6, d1=4, d2=4, sigs=c(1, 1, 1, 1), seed=NULL){\n  if(!is.null(seed))set.seed(seed)\n  g1 &lt;- rmvnorm(n, mean = c(-d1,d2), sigma=sigs[1]*diag(2))\n  g2 &lt;- rmvnorm(n, mean = c(d1,d2), sigma=sigs[2]*diag(2))\n  g3 &lt;- rmvnorm(n, mean = c(-d1,-d2), sigma=sigs[3]*diag(2))\n  g4 &lt;- rmvnorm(n, mean = c(d1,-d2), sigma=sigs[4]*diag(2))\n  rbind(g1,g2,g3,g4)\n}\n\n\n## Code for the plots\ndatA &lt;- makeClust(seed=35151)\ndatB &lt;- makeClust(d2=16, seed=35151)\ndatC &lt;- makeClust(d1=2,d2=2, seed=35151)\nplot(datA, xlab=\"X1\", ylab=\"X2\", fg=\"gray\")  \ntitle(main=\"A: 4blobsA\", adj=0, line=0.5, font.main=1)\n## Repeat previous two lines for datB and datC\nplot(datB, xlab=\"X1\", ylab=\"X2\", fg=\"gray\")  \ntitle(main=\"B: 4blobsB\", adj=0, line=0.5, font.main=1)\nplot(datC, xlab=\"X1\", ylab=\"X2\", fg=\"gray\")  \ntitle(main=\"C: 4blobsC\", adj=0, line=0.5, font.main=1)\n\n\n## Possible alternative\nconfig &lt;- c('Equidistant blobs', 'Pulled vertically', 'Closer centers')\ndat123 &lt;- cbind(as.data.frame(rbind(datA, datB, datC)), \n                              gp=factor(rep(1:3, rep(6*4,3)), labels=config))\nxyplot(V2 ~ V1 | gp, data=dat123, scales=list(relation='free'),\n       strip=strip.custom(factor.levels=config), between=list(x=0.5),\n       par.settings=DAAG::DAAGtheme(color=F))\n\n\n## Code for single linkage plots: `?plot.hclust` gives help for the plot method\nclusres_sing &lt;- hclust(dist(datA), method=\"single\")\npar(fig=c(0,0.75,0,1))\nplot(clusres_sing, sub=\"\", xlab=\"\", ylab=\"Distance joined\", adj=0.5, \n     main=\"\", fg=\"gray\")\nmtext('A: Single linkage cluster dendrogram, for 4blobsA layout', side=3, adj=0, \n      font=1, line=1, cex=1.15)\npar(fig=c(0.72,1,0,1), new=TRUE)\nmembs &lt;- cutree(clusres_sing, 4)\ncol4= RColorBrewer::brewer.pal(4,'Set1')\nplot(datA, xlab=\"X1\", ylab=\"X2\", col=col4[membs], fg='gray', pch=membs+1)\nmtext('B: 4blobsA, by color', side=3, adj=1.0, font=1, cex=1.15, line=1)\n## To see plots from 'average' and 'complete' linkage methods,do:\n# plot(hclust(dist(datB), method=\"average\"))\n# plot(hclust(dist(datC), method=\"complete\"))\n\n\n## Dendrograms from data where blobs were pulled vertically\n## Follow each use of `hclust()` with a `plot()` command\nsclusres_sing &lt;- hclust(dist(datB), method=\"single\")\nplot(sclusres_sing, sub=\"\", xlab=\"\", ylab=\"Distance Joined\", main=\"\")\ntitle(main='A: Single linkage, blobs pulled vertically (4blobsB)', \n      adj=0, font.main=1)\nsclusres_sing_s &lt;- hclust(dist(scale(datA)), method=\"single\")\nplot(sclusres_sing_s, sub=\"\", xlab=\"\", ylab=\"Distance Joined\", main=\"\")\ntitle(main='B: Single linkage, (4blobsB, rescaled to variance 1)', \n      adj=0, font.main=1)\n# sclusres_avg_s &lt;- hclust(dist(scale(datB)), method=\"average\")\n# #plot(sclusres_avg_s, sub=\"\", xlab=\"\", ylab=\"\")\n# sclusres_comp_s &lt;- hclust(dist(scale(datB)), method=\"complete\")\n# #plot(sclusres_comp_s, sub=\"\", xlab=\"\", ylab=\"\")\n\n\n## Code. Follow each use of `hclust()` with a `plot()` command\nclusres_sing2 &lt;- hclust(dist(datC), method=\"single\")\nplot(clusres_sing2, sub=\"\", xlab=\"\", ylab=\"\", cex=1.25, cex.main=1.65,\n     main=\"A: Single linkage, closer clusters (4blobsC)\", adj=0, font.main=1)\nclusres_avg2 &lt;- hclust(dist(datC), method=\"average\")\nplot(clusres_avg2, sub=\"\", xlab=\"\", ylab=\"\", cex=1.25, cex.main=1.65,\n     main=\"B: Average linkage, closer clusters (4blobsC)\", adj=0, font.main=1)\nclusres_comp2 &lt;- hclust(dist(datC), method=\"complete\")\nplot(clusres_comp2, sub=\"\", xlab=\"\", ylab=\"\", cex=1.25, cex.main=1.65, \n     main=\"C: Complete linkage, closer clusters (4blobsC)\", adj=0, font.main=1)\n\n\n\nSubsection 9.3.2: \\(k\\)-Means Clustering\n\nset.seed(35151)\nkdat &lt;- makeClust(n=100, d1=5, d2=5, sigs=c(.5, .5, 6, 6))\nplot(kdat, xlab=\"X1\", ylab=\"X2\", fg=\"gray\")\nkmres &lt;- kmeans(kdat, 4, nstart=30)\nplot(kdat, col=rainbow(4)[kmres$cluster], pch=kmres$cluster+1, \n     xlab=\"X1\", ylab=\"X2\", fg=\"gray\")\n\n\nComments on \\(k\\)-means and hierarchical clustering\n\n\n\nSubsection 9.3.3: Mixture model-based clustering\n\n## Code\nplotMix2 &lt;- function(taus=c(.5, .5), means=c(10,15), sds=c(3,1), xlims=c(0,20)){\n  curve(taus[1]*dnorm(x, mean=means[1], sd=sds[1]) + \n          taus[2]*dnorm(x, mean=means[2], sd=sds[2]), \n        from=xlims[1], to=xlims[2], ylab=\"Density\", fg=\"gray\")\n  curve(taus[1]*dnorm(x, mean=means[1], sd=sds[1]), \n        from=xlims[1], to=xlims[2], col=\"red\", lty=2, add=TRUE, fg=\"gray\")\n  curve(taus[2]*dnorm(x, mean=means[2], sd=sds[2]), \n        from=xlims[1], to=xlims[2], col=\"blue\", lty=3, add=TRUE, fg=\"gray\")\n}\nplotMix2(taus=c(.2, .8))\nplotMix2(taus=c(.5, .5))\nplotMix2(taus=c(.9, .1))\n\n\nlibrary(teigen)\npossml &lt;- na.omit(DAAG::possum[,c(3,9:11)])\nset.seed(513451)\ngaus_fit &lt;- teigen(possml[,2:4], models=\"UUUU\", gauss=TRUE, verbose=FALSE, \n                    scale=FALSE)\n\n\n## BIC values are plotted against number of groups\ngaus_fit$allbic\nplot(gaus_fit$allbic, type=\"b\", ylab=\"\", xlab=\"Number of Groups\", fg=\"gray\")\nmtext(side=2, line=3.5, \"BIC\", las=0)\naxis(1, at=1:9, fg=\"gray\")\n\n\ntable(possml$Pop, gaus_fit$classification)\n\n\npar(fig=c(0, 0.5, 0.5, 1))\nplot(gaus_fit, what=\"contour\", xmarg=1, ymarg=2, draw.legend=FALSE, fg=\"gray\")\n## See ?teigen::plot.teigen for details of the plot command used here.\npar(fig=c(0, 0.5, 0, 0.5), new=TRUE)\nplot(gaus_fit, what=\"contour\", xmarg=1, ymarg=3, draw.legend=FALSE, fg=\"gray\")\npar(fig=c(0.5, 1, 0, 0.5), new=TRUE)\nplot(gaus_fit, what=\"contour\", xmarg=2, ymarg=3, draw.legend=FALSE, fg=\"gray\")\npar(fig=c(0,1,0,1))\n\n\nIssues of high parametrization and scaling\n\n\n\nSubsection 9.3.4: Relationship between \\(k\\)-means and mixture models\n\n\n\nSection 9.4: Discriminant analysis\n\nSubsection 9.4.1: Example – plant architecture\n\nleafshape17 &lt;- DAAG::leafshape17\nplot(bladelen ~ bladewid, data=leafshape17, pch=c(1,3)[arch+1])\n## For panel B, specify log=\"xy\" in the call to plot()\n\n\nLogistic regression, versus linear discriminant analysis\n\n\n\nSubsection 9.4.2: Logistic regression\n\n## Fit logistic regression model\nleafshape17 &lt;- DAAG::leafshape17\nleaf17.glm &lt;- glm(arch ~ logwid + loglen, family=binomial(link=logit),\ndata=leafshape17)\nprint(DAAG::sumry(leaf17.glm)$coef, digits=2)\n\n\nPredictive accuracy\n\nset.seed(29)\nleaf17.cv &lt;- DAAG::CVbinary(leaf17.glm)\ntCV &lt;- table(DAAG::leafshape17[[\"arch\"]], round(leaf17.cv$cvhat))\nrownames(tCV) &lt;- colnames(tCV) &lt;- c(\"0=Plagiotropic\",\"1=Orthotropic\")\ncbind(tCV, \"Proportion correct\"=c(tCV[1,1], tCV[2,2])/(tCV[,1]+tCV[,2]))\n\n\nround(unlist(leaf17.cv[c(\"acc.training\",\"acc.cv\")]),3)\n\n\n\n\nSubsection 9.4.3: Linear discriminant analysis\n\nsuppressPackageStartupMessages(library(MASS))\n## Discriminant analysis; data frame leafshape17 (DAAG)\nleaf17.lda &lt;- lda(arch ~ logwid+loglen, data=DAAG::leafshape17)\nprint(leaf17.lda)\n\n\nAssessments of predictive accuracy\n\nset.seed(29)\nleaf17cv.lda &lt;- lda(arch ~ logwid+loglen, data=leafshape17, CV=TRUE)\n## the list element 'class' gives the predicted class\n## The list element 'posterior' holds posterior probabilities\ntab &lt;- table(leafshape17$arch, leaf17cv.lda$class)\nrownames(tab) &lt;- colnames(tab) &lt;- c(\"0=Plagiotropic\",\"1=Orthotropic\")\ncbind(tab, \"Proportion correct\"=c(tCV[1,1], tCV[2,2])/(tCV[,1]+tCV[,2]))\ncbind(tab, c(tab[1,1], class.acc=tab[2,2])/(tab[,1]+tab[,2]))\ncat(\"Overall proportion correct =\", sum(tab[row(tab)==col(tab)])/sum(tab), \"\\n\")\n\n\n\nThe function qda(), and other alternatives to lda()\n\n\n\nSubsection 9.4.4: An example with more than two groups\n\n## Linear discriminant calculations for possum data\npossum &lt;- DAAG::possum\npossum.lda &lt;- lda(site ~ hdlngth + skullw + totlngth + taill + footlgth +\n                  earconch + eye + chest + belly, data=na.omit(possum))\n# na.omit() omits any rows that have one or more missing values\n\n\nplot(possum.lda, dimen=3, col=1:9)\n# Scatterplot matrix - scores on 1st 3 canonical variates\n# See `?plot.lda` for details of the generic lda plot function\n\n\n## Linear discriminant calculations for possum data\nprint(possum.lda, digits=3)\n\n\n\n\nSection 9.5: *High-dimensional data — RNA-Seq gene expression\n\nSetup for installing and using Bioconductor packages\n\n## For latest details, see: https://www.bioconductor.org/install/\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install()\nBiocManager::install('limma','multtest')\n\n\n\n*Brief note on mRNA technical issues\n\n\nSubsection 9.5.1: Data and design matrix setup\n\ncounts &lt;- DAAGbio::plantStressCounts\ncolSums(counts)\n\n\n## Require at least 3 counts per million that are &gt; 1\nkeep &lt;- rowSums(counts)&gt;=3\ncounts &lt;- counts[keep,]\n\n\ntreatment &lt;- factor(rep(c(\"CTL\", \"L\", \"D\"), rep(3,3)))\ndesign &lt;- model.matrix(~0+treatment)\ncolnames(design) &lt;- levels(treatment)\n\n\nA two-dimensional representation\n\nlibrary(limma)\nv &lt;- voom(counts, design, plot=TRUE)\n\n\npar(oma=c(0,0,1,0))\nlibrary(limma)\nv &lt;- voom(counts, design, plot=TRUE)\nfirstchar &lt;- substring(colnames(counts),1,1)\nplotMDS(counts, labels=paste0(firstchar, rep(1:3,3)), cex=0.8)\nbox(col=\"gray\")\nmtext(side=3, line=0.4, adj=0, \"MDS summary plot\")\nmtext(side=3, line=-0.25, adj=0.105, \"A\", outer=TRUE)\nmtext(side=3, line=-0.25, adj=0.605, \"B\", outer=TRUE)\n\n\n\nFitting the model\n\nfit &lt;- lmFit(v, design)\n\n\ncontrs &lt;- c(\"D-CTL\", \"L-CTL\", \"L-D\")\ncontr.matrix &lt;- makeContrasts(contrasts=contrs,\nlevels=levels(treatment))\nfit2 &lt;- contrasts.fit(fit, contr.matrix)\nefit2 &lt;- eBayes(fit2)\n\n\n\n\nSubsection 9.5.2: From \\(p\\)-values to false discovery rate (FDR)\n\n## First contrast only; Drought-CTL\nprint(round(topTable(efit2, coef=1, number=4),15), digits=3)\n\n\nround(sort(p.adjust(p=efit2$p.value[,1], method=\"BH\"))[1:4], 15) # Not run\n\n\nround(topTable(efit2, number=4), 15)\n\n\nhead(decideTests(fit2),5)\n\n\nsummary(decideTests(fit2))\n## Try also\n## summary(decideTests(fit2, p.value=0.001))\n\n\n\n\nSection 9.6: High dimensional data from expression arrays\n\nSubsection 9.6.1: Molecular classification of cancer — an older technology\n\nBreakdown of ALL B-type data, with one observation excluded\n\nlibrary(hddplot)\ndata(golubInfo)\nwith(golubInfo, table(cancer, tissue.mf))\n\n\n## Identify allB samples that are BM:f or BM:m or PB:m\nsubsetB &lt;- with(golubInfo,\ncancer==\"allB\" & tissue.mf%in%c(\"BM:f\",\"BM:m\",\"PB:m\"))\n## Separate off the relevant columns of the matrix Golub\n## NB: variables (rows) by cases (columns)\nGolubB &lt;- with(golubInfo, Golub[, subsetB])\n## Form vector that identifies these as BM:f or BM:m or PB:m\ntissue.mfB &lt;- with(golubInfo, tissue.mf[subsetB, drop=TRUE])\n## Change the level names to leave out the colons\nlevels(tissue.mfB) &lt;- list(\"b_f\"=\"BM:f\", \"b_m\"=\"BM:m\", \"PBm\"=\"PB:m\")\n\n\n\n\nSubsection 9.6.2: Classifications and associated graphs\n\nPreliminary data manipulation\n\n## Display distributions for the first 20 observations\nboxplot(data.frame(GolubB[, 1:20]))  # First 20 columns (observations)\n## Random selection of 20 rows (features)\nboxplot(data.frame(GolubB[sample(1:7129, 20), ]))\n\n\n\nFlawed graphs\n\ncolr &lt;- c(\"red\",\"blue\",\"gray40\", \"magenta\")\ntissue.mf &lt;- golubInfo[, \"tissue.mf\"]\ncancer &lt;- golubInfo[, \"cancer\"]\nG.PBf &lt;- Golub[, tissue.mf==\"PB:f\" & cancer==\"allB\", drop=FALSE]\nset.seed(41)\nrGolubB &lt;- matrix(rnorm(prod(dim(GolubB))), nrow=dim(GolubB)[1])\nrownames(rGolubB) &lt;- rownames(Golub)\nrG.PBf &lt;- matrix(rnorm(prod(dim(G.PBf))), nrow=dim(G.PBf)[1])\nplot2 &lt;- function(x = GolubB, cl=tissue.mfB, x.omit=Golub.PBf, cl.omit=\"PBf\", \n                  ncol = length(cl), nfeatures=12, device = \"\", seed = 37,\n                  pretext=\"\", colr=1:3, levnames = NULL,\n                  ylab=\"2nd discriminant function\"){\n  cl &lt;- factor(cl)\n  if(!is.null(levnames))levels(cl) &lt;- levnames\n  ord15 &lt;- orderFeatures(x, cl=cl)[1:nfeatures]\n  dfB &lt;- t(x[ord15, ])\n  dfB.lda &lt;-  lda(dfB, grouping=cl)\n  scores &lt;- predict(dfB.lda, dimen=2)$x\n  df.PBf &lt;- data.frame(t(x.omit[ord15, drop=FALSE]))\n  colnames(df.PBf) &lt;- colnames(dfB)\n  scores.other &lt;- predict(dfB.lda, newdata=df.PBf)$x\n  scoreplot(list(scores=scores, cl=cl, other=scores.other, cl.other=cl.omit,                       nfeatures=nfeatures), prefix.title=pretext, adj.title=0, \n                 fg=\"gray\", params=list(other=list(pch=4, cex=1.5)),\n            xlab=\"1st discriminant function\", ylab=ylab)\n}\nplot2(x = GolubB, cl = tissue.mfB, x.omit=G.PBf, cl.omit=\"PBf\",\n      nfeatures=15, device = \"\", seed = 37, ylab=\"2nd discriminant function\",\n      colr=colr, pretext=\"A: ALL B-cell:\")\nplot2(x = rGolubB, cl = tissue.mfB, x.omit=rG.PBf, cl.omit=\"Gp 4\", \n     device = \"\", seed = 37, colr=colr, levnames = c(\"Gp 1\", \"Gp 2\", \"Gp 3\"),\n     pretext=\"B: Random data:\", ylab=\"\")\n\n\n## Uses orderFeatures() (hddplot); see below\nord15 &lt;- orderFeatures(GolubB, cl=tissue.mfB)[1:15]\n\n\n## Panel A: Take 1st 15 features & transpose to observations by features\ndfB15 &lt;- data.frame(t(GolubB[ord15, ]))\ndfB15.lda &lt;-  MASS::lda(dfB15, grouping=tissue.mfB)\nscores &lt;- predict(dfB15.lda, dimen=2)$x\n## Scores for the single PB:f observation\nchooseCols &lt;- with(golubInfo, tissue.mf==\"PB:f\"& cancer==\"allB\")\ndf.PBf &lt;- data.frame(t(Golub[ord15, chooseCols, drop=FALSE]))\nscores.PBf &lt;- predict(dfB15.lda, newdata=df.PBf, dimen=2)$x\n## Warning! The plot that now follows may be misleading!\n## Use hddplot::scoreplot()\nscoreplot(list(scores=scores, cl=tissue.mfB, other=scores.PBf, cl.other=\"PB:f\"),\n          fg=\"gray\")\n\n\n## Panel B: Repeat plot, now with random normal data\nsimscores &lt;- simulateScores(nrow=7129, cl=rep(1:3, c(19,10,2)),\ncl.other=4, nfeatures=15, seed=41)\n# Returns list elements: scores, cl, scores.other & cl.other\nscoreplot(simscores)\n\n\n\n\nSubsection 9.6.3: The mean-variance relationship\n\npar(oma=c(0,0,1,0))\ndesignG &lt;- model.matrix(~0+tissue.mfB)\ncolnames(designG) &lt;- levels(tissue.mfB)\nvG &lt;- vooma(GolubB, designG, plot=TRUE)         # Panel A\nplotMDS(vG, pch=unclass(tissue.mfB), cex=0.8)   # Panel B\nleglabs &lt;- c(\"BM:female\",\"BM:male\",\"PB:female\")\nlegend(x=\"bottomright\", bty=\"n\", legend=leglabs, pch=1:3)\nmtext(side=3, line=0.4, adj=0, \"MDS summary plot\")\nmtext(side=3, line=-0.275, adj=0.085, \"A\", outer=TRUE)\nmtext(side=3, line=-0.275, adj=0.585, \"B\", outer=TRUE)\n\n\nCross-validation to determine the optimum number of features\n\n\nCross-validation for a range of choices of number of features\n\n##  Cross-validation to determine the optimum number of features\n## 10-fold (x4). Warning messages are omitted.\n## Accuracy measure will be: tissue.mfB.cv$acc.cv\ntissue.mfB.cv &lt;- cvdisc(GolubB, cl=tissue.mfB, nfeatures=1:23,\nnfold=c(10,4), print.progress=FALSE)\n## Defective measures will be in acc.resub (resubstitution)\n## and acc.sel1 (select features prior to cross-validation)\ntissue.mfB.badcv &lt;- defectiveCVdisc(GolubB, cl=tissue.mfB,\nfoldids=tissue.mfB.cv$folds, nfeatures=1:23)\n##\n## Calculations for random normal data:\nset.seed(43)\nrGolubB &lt;- matrix(rnorm(prod(dim(GolubB))), nrow=nrow(GolubB))\nrtissue.mfB.cv &lt;- cvdisc(rGolubB, cl=tissue.mfB, nfeatures=1:23,\nnfold=c(10,4), print.progress=FALSE)\nrtissue.mfB.badcv &lt;- defectiveCVdisc(rGolubB, cl=tissue.mfB,\nnfeatures=1:23,\nfoldids=rtissue.mfB.cv$folds)\n\n\n\nWhich features?\n\ngenelist &lt;- matrix(tissue.mfB.cv$genelist[1:3, ,], nrow=3)\ntab &lt;- table(genelist, row(genelist))\nord &lt;- order(tab[,1], tab[,2], tab[,3], decreasing=TRUE)\ntab[ord,]\n\n\n\n\nSubsection 9.6.4: Graphs derived from the cross-validation process\n\n## Uses tissue.mfB.acc from above\ntissue.mfB.scores &lt;-\ncvscores(cvlist = tissue.mfB.cv, nfeatures = 3, cl.other = NULL)\nscoreplot(scorelist = tissue.mfB.scores, cl.circle=NULL,\nprefix=\"B-cell subset -\", fg='gray')\n\n\nThe key role of cross-validation\n\n\n\nSubsection 9.6.5: Estimating contrasts, and calculating False Discovery Rates\n\nfitG &lt;- lmFit(vG, designG)\ncontrs &lt;- c(\"b_f-b_m\", \"b_f-PBm\", \"b_m-PBm\")\ncontr.matrix &lt;- makeContrasts(contrasts=contrs,\nlevels=levels(tissue.mfB))\nfit2 &lt;- contrasts.fit(fitG, contr.matrix)\nfit2 &lt;- eBayes(fit2)\n\n\nFrom \\(p\\)-values to false discovery rate (FDR)\n\nprint(topTable(fit2, number=5), digits=2)\n\n\nsummary(decideTests(fit2))\n## Try also\n## summary(decideTests(fit2, p.value=0.001))\n\n\n\nDistributional extremes\n\n\n\n\nSection 9.7: Causal inference from observational data — balance and matching\n\nSubsection 9.7.1: Tools for the task\n\nlibrary(DAAG)\n## Columns 4:7 are factors; columns 9:10 (re75 & re78) are continuous\npropmat &lt;- matrix(0, ncol=6, nrow=8)\ndimnames(propmat) &lt;- list(c(\"psid1\", \"psid2\", \"psid3\", \"cps1\", \"cps2\", \"cps3\",\n\"nsw-ctl\", \"nsw-trt\"), names(nswdemo)[c(4:7, 9:10)])\nfor(k in 1:8){\n  dframe &lt;- switch(k, psid1, psid2, psid3, cps1, cps2, cps3,\n  subset(nswdemo, trt==0), subset(nswdemo, trt==1))\n  propmat[k,] &lt;- c(sapply(dframe[,4:7], function(x){\n                   z &lt;- table(x); z[2]/sum(z)}),\n                   sapply(dframe[,9:10], function(x)sum(x&gt;0)/sum(!is.na(x))))\n}\n\n\nPGtheme &lt;- DAAG::DAAGtheme(color=TRUE)\nlibrary(DAAG)\nif(!require(grid))return(\"Package 'grid' is not installed -- cannot proceed\")\ndsetnames &lt;- c(\"nsw-ctl\", \"nsw-trt\", \"psid1\", \"psid2\", \"psid3\",\n               \"cps1\", \"cps2\", \"cps3\")\ncolrs &lt;- c(\"gray\",\"black\", PGtheme$superpose.line$col[1:3])\nlty &lt;- c(1,2,1,1,1)\nlwd &lt;- c(1,0.75,0.75,0.75,0.75)\ndenplot &lt;-\n  function(sel=c(1:2,6:8), yvar=\"re75\", offset=30, ylim=c(0,1.75),\n    from=NULL, at=c(.5,1,1.5), labels=paste(at), bw=\"nrd0\",\n    ylab=\"Density\", takelog=TRUE, col.axis=\"black\"){\n      nzre &lt;- unlist(lapply(list(subset(nswdemo, trt==0),\n                                 subset(nswdemo, trt==1),\n                                 psid1, psid2, psid3, cps1, cps2, cps3)[sel],\n                            function(x){z &lt;- x[,yvar]; z[z&gt;0]}))\nnum &lt;- unlist(lapply(list(subset(nswdemo, trt==0), subset(nswdemo, trt==1),\n                          psid1, psid2, psid3, cps1, cps2, cps3),\n                     function(x){z &lt;- x[,yvar]; sum(z&gt;0)}))\nxy &lt;- data.frame(nzre=nzre, fac = factor(rep(dsetnames[sel], num[sel]),\n                 levels=dsetnames[sel]))\nif(takelog) {\ny &lt;- log(xy$nzre+offset)\nxlab &lt;- paste(\"log(\", yvar, \"+\", offset, \")\", sep=\"\")} else \n  {\n  y &lt;- xy$nzre\n  xlab &lt;- yvar\n}\ndensityplot(~ y, groups=fac, data=xy, bw=bw, from=from,\n  scales=list(y=list(at=at, labels=labels, col=col.axis), tck=0.25),\n  plot.points=FALSE, col=colrs[1:5], lwd=lwd, lty=lty, \n  key=list(x=0.01, y=0.99, text=list(dsetnames[sel[3:5]]), col=colrs[3:5],\n           cex=0.75, lines=list(lwd=rep(1.5,3)), between=1),\n  par.settings=list(col=colrs, lty=lty, cex=0.75, lwd=lwd, \n                    fontsize=list(text=9, points=5)),\n  fg=\"gray\", ylim=ylim, ylab=ylab, xlab=xlab)\n}\n## Plot base graph; overlay with lattice graphs on same page\npar(fig=c(0,1,0,1), mar=c(0,0,0,0))\nplot(0:1,0:1, axes=FALSE, type=\"n\", bty=\"n\", xlab=\"\", ylab=\"\")\nlegend(x=\"top\",legend=dsetnames[1:2], lty=1:2, lwd=c(1,0.75),\n       col=colrs[1:2], bty=\"n\", ncol=2, yjust=0.75)\nprint(denplot(), position=c(0, 0, 0.32, 0.505), newpage=FALSE)\nprint(denplot(1:5, ylab=\" \", col.axis=\"white\"),\n      position=c(0.21, 0, .53, 0.505), newpage=FALSE)\nprint(denplot(ylab=\" \", yvar=\"re78\", col.axis=\"white\"),\n      position=c(0.47, 0, 0.79, 0.505), newpage=FALSE)\nprint(denplot(1:5, ylab=\" \", yvar=\"re78\", col.axis=\"white\"),\n      position=c(0.68, 0, 1, 0.505), newpage=FALSE)\n## Age\nprint(denplot(yvar=\"age\", takelog=FALSE, ylim=c(0,0.1), from=16,\n      at=c(.02,.04,.06,.08), labels=c(\".02\",\".04\",\".06\",\".08\")),\n      position=c(0, 0.475, 0.32, .98), newpage=FALSE)\nprint(denplot(1:5, yvar=\"age\", takelog=FALSE, ylim=c(0,0.1), from=16,\n      at=c(.02,.04,.06,.08), labels=c(\".02\",\".04\",\".06\",\".08\"),\n      ylab=\" \", col.axis=\"white\"),\n      position=c(0.21, 0.475, .53, .98), newpage=FALSE)\n## educ\nprint(denplot(1:5, yvar=\"educ\", takelog=FALSE, ylim=c(0,0.5), bw=0.5,\n      at=c(.1,.2,.3,.4), ylab=\" \"),\n      position=c(0.47, 0.475, .79, .98), newpage=FALSE)\nprint(denplot(yvar=\"educ\", takelog=FALSE, ylim=c(0,0.75), bw=0.5,\n      at=c(.1,.2,.3,.4), ylab=\" \", col.axis=\"white\"),\n      position=c(0.68, 0.475, 1, .98), newpage=FALSE)\n\n\naddControl &lt;-\nfunction(control, offset=30){\n  nam &lt;- deparse(substitute(control))\n  if(nam==\"nswdemo\")nsw0 &lt;- nswdemo else\n    nsw0 &lt;- rbind(control, subset(DAAG::nswdemo, trt==1))\n  nsw0$z75 &lt;- factor(nsw0$re75==0, labels=c(\"0\",\"&gt;0\"))\n  nsw0$ethnicid &lt;- factor(with(nsw0, ifelse(black==1, \"black\",\n    ifelse(hisp==1, \"hisp\", \"other\"))), levels=c(\"other\",\"black\",\"hisp\"))\nnsw0 &lt;- nsw0[, -match(c(\"black\",\"hisp\"), names(nsw0))]\nnsw0\n}\n\n\n## Create dataset that will be used for later analyses\nnsw &lt;- addControl(psid1)\nnsw &lt;- within(nsw, {re75log &lt;- log(re75+30);\n                    re78log &lt;- log(re78+30);\n                    trt &lt;- factor(trt, labels=c(\"Control\",\"Treat\"))})\n## A treated values only dataset will be required below\ntrtdat &lt;- subset(nsw, trt==\"Treat\")\ntrtdat$pres74 &lt;- factor(!is.na(trtdat$re74), labels=c(\"&lt;NA&gt;\",\"pres\"))\ntable(trtdat$pres74)\n\n\nwith(trtdat, table(pres74,z75))\n\n\n\nSubsection 9.7.2: Regression comparisons\n\nRegression calculations\n\nnsw.gam &lt;- gam(log(re78+30)~ trt + ethnicid + z75 + nodeg + s(age) +\n               s(educ) + log(re75+30), data=nsw)\n\n\n\n\nSubsection 9.7.3: The use of scores to replace covariates\n\n\nSubsection 9.7.4: Two-dimensional representation using randomForest proximities\n\nsuppressPackageStartupMessages(library(randomForest))\nform &lt;- trt ~ age + educ + ethnicid + marr + nodeg + z75 + re75log\nnsw.rf &lt;- randomForest(form, data=nsw, sampsize=c(297,297))\np.rf &lt;- predict(nsw.rf,type=\"prob\")[,2]\nsc.rf &lt;- log((p.rf+0.001)/(1-p.rf+0.001))\n\n\nomitn &lt;- match(c(\"PropScore\",\"weights\",\"subclass\"), names(dat2RF), nomatch=0)\nmatchISO.rf &lt;-matchit(trt ~ age + educ + ethnicid + marr + nodeg + z75 +\n                      re75log, ratio=1, data=dat2RF[,-omitn], distance=isoScores[,1])\n## summary(match.rf,un=F,improvement=F)\n## summary(match.rf, un=F, interactions=T, improvement=F)$sum.matched[,1:4]\n## In the first place, look only at the first 4 columns\n\n\ndat1RF &lt;- match.data(matchISO.rf, distance=\"PropScore\")\ndat1RF.lm &lt;- lm(re78log ~ trt, data = dat1RF, weights = weights)\nlibrary(sandwich)   # Allows use of `vcovCL()` from the `sandwich` package\nlmtest::coeftest(dat1RF.lm, vcov. = vcovCL, cluster = ~subclass)\n## Check for increase in number with non-zero earnings\ndat1RF.glm &lt;- glm(I(re78&gt;0) ~ trt, data = dat1RF, weights = weights, \n                  family=binomial)\nlmtest::coeftest(dat1RF.glm, vcov. = vcovCL, cluster = ~subclass)\n\n\nDerivation and investigation of scores\n\nlibrary(mgcv)\nformG &lt;- trt ~ ethnicid + marr+ z75 + s(age) + s(educ) + s(re75log)\nnsw.gam &lt;- gam(formG, family=binomial(link=\"logit\"), data=nsw)\npred &lt;- predict(nsw.gam, type='response')\ntable(nsw$trt, round(pred))\n## Alternative\nlibrary(splines)  ## Fit normal cubic splines using splines::ns()\nformNS &lt;- trt ~ ethnicid + marr+ z75 + ns(age,2) +\nns(educ) + ns(re75log,3)\nnsw.glm &lt;- glm(formNS, family=binomial(link=\"logit\"), data=nsw)\npred &lt;- predict(nsw.glm, type='response')\ntable(nsw$trt, round(pred))\ncbind(AIC(nsw.glm,nsw.gam), BIC(nsw.glm, nsw.gam))\n\n\n## Include factor by factor and variable interactions with ethnicid\n## and marr (Result not shown)\nformGx &lt;- trt ~ (ethnicid+marr+z75)^2 + s(age, by=ethnicid)+\n                s(educ, by=ethnicid) + s(re75log,by=ethnicid)+\n                s(age, by=marr)+ s(educ, by=marr) + s(re75log,by=marr)\nnswx.gam &lt;- gam(formula = formGx, data = nsw, family=binomial(link = \"logit\"))\npredx &lt;- predict(nswx.gam, type='response')\ntable(nsw$trt, round(predx))\nAIC(nsw.glm,nsw.gam,nswx.gam)\n\n\nlibrary(MatchIt)\n## Use data frame that omits re74. Otherwise matchit() will generate NAs\n## where they occur in re74, even though re74 is not in the model formula.\nnswG &lt;- nsw[, c(\"trt\",\"age\",\"educ\",\"ethnicid\", \"marr\",\"nodeg\",\"z75\",\n                \"re75log\",\"re78log\",\"re78\")]\nformG &lt;- trt ~ ethnicid + marr+ z75 + s(age) + s(educ) + s(re75log)\nmatch.gam &lt;- matchit(formula = formG, data = nswG, method = \"nearest\",\n                     distance = \"gam\", link = \"logit\", reestimate=TRUE)\ndatG &lt;- match.data(match.gam, distance=\"PropScore\")\n## Summary information\nmatch.gam\n## summary(match.gam,un=F,improvement=F)\n## summary(match.gam, un=F, interactions=T, improvement=F)$sum.matched[,1:4]\n## In the first place, look only at the first 4 columns\n\n\nsuppressPackageStartupMessages(library(gridExtra))\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(cobalt))\ngg1&lt;- cobalt::love.plot(match.gam, position=\"bottom\", grid=TRUE,\n                        star.char=\"\",stars='raw') +\n  ggtitle(\"A: Differences from balance\") +\n  theme(plot.title = element_text(hjust=0, vjust=0.5, size=11),\n        plot.margin=unit(c(9,15,0,9), 'pt'))\nsub &lt;- match(with(subset(datG, trt==\"Control\"),subclass),\n             with(subset(datG, trt==\"Treat\"),subclass))\ndatGpaired &lt;- cbind(subset(datG, trt==\"Treat\"),\n                    with(subset(datG, trt==\"Control\")[sub,],\n                    cbind(\"Cre78log\"=re78log,\"CPropScore\"=PropScore)))\ngg2 &lt;- ggplot(datGpaired)+\n  geom_point(aes(PropScore,I(re78log-Cre78log)), size=1)+\n  geom_smooth(aes(PropScore,I(re78log-Cre78log)), method = \"gam\", \n              formula = y ~ s(x, bs = \"cs\")) +\n  xlab(\"Propensity score for treated\")+\n  ylab(\"Treatment vs control differences\") +\n  ggtitle(\"B: Treatment vs control differences\") +\n  theme(plot.title = element_text(hjust=0, vjust=0.5, size=11),\n        plot.margin=unit(c(9,9,0,15), 'pt'))\ngrid.arrange(gg1, gg2, ncol=2) \n\n\nlibrary(sandwich)\ndatG.lm &lt;- lm(re78log ~ trt, data = datG, weights = weights)\n## With 1:1 matching, the weights argument is not really needed\n## Print first two coefficients only. \nlmtest::coeftest(datG.lm, vcov. = vcovCL, cluster = ~subclass)[1:2,]\n## Check number whose income was greater than 0\ndatG.glm &lt;- glm(I(re78&gt;0) ~ trt, data = datG, weights = weights, family=binomial)\nlmtest::coeftest(datG.glm, vcov. = vcovCL, cluster = ~subclass)[1:2,]\n\n\n\nAlternative matching approaches\n\n\n\nSubsection 9.7.5: Coarsened exact matching\n\nform &lt;- trt ~ age + educ + ethnicid + marr + nodeg + z75 + re75log\nmatch5.cem &lt;- matchit(formula=form, data=nswG, method=\"cem\", cutpoints=5)\ndatcem5 &lt;- match.data(match5.cem)\nmatch6.cem &lt;- matchit(formula=form, data=nswG, method=\"cem\", cutpoints=6)\ndatcem6 &lt;- match.data(match6.cem)\n## Show the effect of adding another cutpoint\nmatch5.cem\nmatch6.cem\n\n\nlibrary(sandwich)\ndatcem5.lm &lt;- lm(re78log ~ trt, data = datcem5, weights = weights)\n## The function vcovHC() provides cluster robust standard errors\nlmtest::coeftest(datcem5.lm, vcov. = vcovHC)\n## Estimate treatment effect on number with some earnings:\ndatcem6.glm &lt;- glm(I(re78&gt;0) ~ trt, data = datcem6, weights = weights,\nfamily=binomial)\nlmtest::coeftest(datcem6.glm, vcov. = vcovHC)\n\n\n\n\nSection 9.8: Multiple imputation\n\nsuppressPackageStartupMessages(library(mice))\nBoys &lt;- with(subset(mice::boys, age&gt;=9), \n             data.frame(age=age, loghgt=log(hgt), logbmi=log(bmi), loghc=log(hc)))\n(Pattern &lt;- md.pattern(Boys, plot=F))\n\n\nset.seed(31)       # Set to reproduce result shown\nPatternB &lt;- rbind(Pattern[-c(1,nrow(Pattern)), -ncol(Pattern)],\n             c(0,1,1,1), c(0,1,0,0), c(0,0,1,0))\nboys &lt;- rbind(ic(Boys), \n              ampute(cc(Boys), pattern=PatternB, freq=c(.3,.15,.15,.2,.1,.1), \n                     prop=0.75)$amp)\nmd.pattern(boys, plot=FALSE)\n\n\nset.seed(17)       # Set to reproduce result shown\nout &lt;- capture.output(        # Evaluate; send screen output to text string\n  boys.mids &lt;- mice(boys, method='pmm', m=8)  )\nimpDFs &lt;- complete(boys.mids, action='all')   # Returns a list of m=8 dataframes\n## Average over imputed dataframes (use for exploratory purposes only)\nimpArray &lt;- sapply(impDFs, function(x)as.matrix(x), simplify='array')\nboysAv &lt;- as.data.frame(apply(impArray, 1:2, mean))  \n\n\nfits &lt;- with(boys.mids, lm(logbmi~age+loghgt))\npool.coef &lt;- summary(pool(fits))  # Include in table below\n\n\n## 2) Regression that leaves out rows with NAs\nomitNArows.coef &lt;- coef(summary(lm(logbmi~age+loghgt, data=boys)))\n## 3) Regression fit to average over data frames after imputation\nboysAv.coef &lt;- coef(summary(lm(logbmi~age+loghgt, data=boysAv)))\n## 4) Fit to original data, with 36 rows had missing data\nOrig.coef &lt;- coef(summary(lm(logbmi ~ age+loghgt, data=Boys)))\n\n\nctab &lt;- cbind(summary(pool(fits))[,2:3], omitNArows.coef[,1:2], boysAv.coef[,1:2], \n      Orig.coef[,1:2])\ntab &lt;- setNames(cbind(ctab[,c(1,3,5,7)], ctab[,c(2,4,6,8)]),\n                paste0(rep(c('Est','SE'), c(4,4)), rep(1:4, 2)))\nround(tab,3)\n\n\nTime series cross-sectional data – an example\n\nairquality &lt;- datasets::airquality\nairq &lt;- cbind(airquality[, 1:4], day=1:nrow(airquality))\n  # 'day' (starting May 1) replaces columns 'Month' & 'Day')\n## Replace `Ozone` with `rt4ozone`:\nairq &lt;- cbind(rt4ozone=airq$Ozone^0.25, airq[,-1])\n\n\n## Generate the scatterplot matrix, now with `rt4ozone` replacing `Ozone`\nsmoothPars &lt;- list(col.smooth='red', lty.smooth=2, spread=0)\ncar::spm(airq, cex.labels=1.2, regLine=FALSE, col='blue', \n         oma=c(1.95,3,4, 3), gap=.25, smooth=smoothPars)\n\n\nairq.imp &lt;- mice(airq, m=20, print=FALSE)\n  ## 20 imputations shows up issues of concern very clearly\n\n\n## Code for figure\nout &lt;- micemd::overimpute(airq.imp)\n\n\n\nSome further points\n\n\n\nSection 9.9: Further reading\n\nData with more variables than observations\n\n\nCausal inference\n\n\nMultiple imputation\n\n\n\nSection 9.10: Exercises\n9.3\n\nlibrary(DAAG)\noz::oz(sections=c(3:5, 11:16))\nnames(possumsites)[1:2] &lt;- c(\"long\", \"lat\")\nwith(possumsites, {\npoints(long, lat);\ntext(long, lat, row.names(possumsites), pos=c(2,4,2,2,4,2,2))\n})\n\n9.7\n\ndata(wine, package='gclus')\nmat &lt;- with(wine, \n  round(1-cor(cbind(Alcohol, Malic, Magnesium, Phenols, Flavanoids)),2))\ncolnames(mat) &lt;- rownames(mat) &lt;- 1:5\nprint(mat)\n\n9.9a\n\n`confusion` &lt;-\nfunction(actual, predicted, digits=4){\n  tab &lt;- table(actual, predicted)\n  confuse &lt;- apply(tab, 1, function(x)x/sum(x))\n  print(round(confuse, digits))\n  acc &lt;- sum(tab[row(tab)==col(tab)])/sum(tab)\n  invisible(print(c(\"Overall accuracy\" = round(acc,digits))))\n}\ndata(Vehicle, package=\"mlbench\")\nlhat &lt;- MASS::lda(Class ~ ., data=Vehicle, CV=TRUE)$class\nqhat &lt;- MASS::qda(Class ~ ., data=Vehicle, CV=TRUE)$class\nDAAG::confusion(Vehicle$Class, lhat)\nDAAG::confusion(Vehicle$Class, qhat)\nrandomForest::randomForest(Class ~ ., data=Vehicle, CV=TRUE)\n\n9.9c\n\nVehicle.lda &lt;- MASS::lda(Class ~ ., data=Vehicle)\ntwoD &lt;- predict(Vehicle.lda)$x\nggplot2::quickplot(twoD[,1], twoD[,2], color=Vehicle$Class,\n                   geom=c(\"point\",\"density2d\"))\n\n9.10\n\nlibrary(ape); library(MASS)\nlibrary(DAAGbio)\nprimates.dna &lt;- as.DNAbin(primateDNA)\nprimates.dist &lt;- dist.dna(primates.dna, model=\"K80\")\nprimates.cmd &lt;- cmdscale(primates.dist)\neqscplot(primates.cmd)\nrtleft &lt;- c(4,2,4,2)[unclass(cut(primates.cmd[,1], breaks=4))]\ntext(primates.cmd, labels=row.names(primates.cmd), pos=rtleft)\n\n\nd &lt;- dist(primates.cmd)\nsum((d-primates.dist)^2)/sum(primates.dist^2)\n\n9.11\n\nlibrary(DAAG)\npacific.dist &lt;- dist(x = as.matrix(rockArt[-c(47,54,60,63,92),28:641]), \n                     method = \"binary\")\nsum(pacific.dist==1)/length(pacific.dist)\nplot(density(pacific.dist, to = 1))\n## Check that all columns have at least one distance &lt; 1\nsymmat &lt;- as.matrix(pacific.dist)\ntable(apply(symmat, 2, function(x) sum(x&lt;1)))\npacific.cmd &lt;- cmdscale(pacific.dist)\npacific.sam &lt;- sammon(pacific.dist)\n\n9.15\n\nWine &lt;- setNames(cbind(stack(wine, select=2:14), rep(wine[,-1], 13)),\n                 c(\"value\", \"measure\", \"Class\"))\nbwplot(measure ~ value, data=Wine)\n\n\nwine.pr &lt;- prcomp(wine[,-1], scale=TRUE)\nround(wine.pr$sdev,2)\nt(round(wine.pr$rotation[,1:2],2))\nscores &lt;- as.data.frame(cbind(predict(wine.pr), Class=wine[,1]))\nxyplot(PC2 ~ PC1, groups=Class, data=scores, aspect='iso', \n       par.settings=simpleTheme(pch=16), auto.key=list(columns=3))\n\n\nlibrary(MASS)\nwine.lda &lt;- lda(Class ~ ., data=wine)\nwineCV.lda &lt;- lda(Class ~ ., data=wine, CV=T)\nt(round(wine.lda$scaling,2))\ntab &lt;- table(wine$Class, wineCV.lda$class, \n             dnn=c('Actual', 'Predicted'))   \ntab\nsetNames(round(1-sum(diag(tab))/sum(tab),4), \"CV error rate\")\nscores &lt;- as.data.frame(cbind(predict(wine.lda)$x, Class=wine[,1]))\nxyplot(LD2 ~ LD1, groups=Class, data=scores, aspect='iso', \n       par.settings=simpleTheme(pch=16), auto.key=list(columns=3))\n\n\nwine$Class &lt;- factor(Wine$Class)\nwine.rf &lt;- randomForest(x=wine[,-1], y=wine$Class)\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/ch9.R\")\n}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 9: Multivariate data exploration and discrimination</span>"
    ]
  },
  {
    "objectID": "Appendix.html",
    "href": "Appendix.html",
    "title": "10  Appendix A: The R System – A Brief Overview",
    "section": "",
    "text": "Packages required (plus any dependencies)\nDAAG dplyr tidyr tibble MASS gplots plotrix latticeExtra RColorBrewer\nAdditionally, knitr and Hmisc are required in order to process the qmd source file.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Appendix A: The R System – A Brief Overview</span>"
    ]
  },
  {
    "objectID": "Appendix.html#the-r-system-a-brief-overview",
    "href": "Appendix.html#the-r-system-a-brief-overview",
    "title": "10  Appendix A: The R System – A Brief Overview",
    "section": "The R System – A Brief Overview",
    "text": "The R System – A Brief Overview\n\nSection 10.1: Getting started with R\n\nSubsection 10.1.1: Learn by typing code at the command line\n\n&gt; ## Arithmetic calculations.  See the help page `?Arithmetic` {-}\n&gt; 2*3+10            # The symbol `*` denotes 'multiply'\n\n\n&gt; ## Use the `c()` function to join results into a numeric vector\n&gt; c(sqrt(10), 2*3+10, sqrt(10), 2^3)  # 2^3 is 2 to the power of 3\n&gt; ## R knows about pi\n&gt; 2*pi*6378         # Approximate circumference of earth at equator (km)\n\n\n?help              # Get information on the use of `help()`\n?sqrt              # Or, type help(sqrt)\n?Arithmetic        # See, in similar vein ?Syntax\n?'&lt;'               # `?Comparison` finds the same help page\n\n\n## Two commands on one line; Use ';' as separator\n2*3*4+10; sqrt(10)        ## Try also `cat(2*3*4+10, sqrt(10), sep='n')\n## Convert CO2 carbon emissions from tonnes of carbon to tonnes of CO2 \n3.664*c(.53, 2.56, 9.62)  ## Data are for 1900, 1960 & 2020\n\n\n## Use `cat()` to print several items, with control of formatting\ncat(2*3*4+10, sqrt(10), '\\n')\n\n\nAssignment\n\n## Convert from amounts of carbon to amounts of CO2 (billions of tonnes)\n## and assign result to a named object\nfossilCO2vals &lt;- c(.53, 2.56, 9.62)*3.664  # Amounts in 1900, 1960, and 2020\n  # Equivalently `fossilCO2vals &lt;- c(.53, 2.56, 9.62)*rep(3.664,3)`\n## To assign and print, enclose in round brackets\n(fossilCO2vals &lt;- c(.53, 2.56, 9.62)*3.664)\n\n\n3.664*c(.53,2.56, 9.62) -&gt; fossilCO2vals\n\n\n\nEntry of data at the command line, a graphics formula, and a graph\n\nYear &lt;- c(1900, 1920, 1940, 1960, 1980, 2000, 2020)\nCO2 &lt;- c(.53,.96,1.32,2.56,5.32,6.95,9.62)*3.664\n## Now plot Carbon Dioxide emissions as a function of Year\nplot(CO2 ~ Year, pch=16, fg=\"gray\")\n\n\n\nGrouping vectors togeher into data frames\n\nCO2byYear &lt;- data.frame(year=Year, co2gas=CO2)\nCO2byYear         # Display the contents of the data frame.\nrm(Year, CO2)     # Optionally, remove `Year` and `Carbon` from the workspace\nplot(co2gas ~ year, data=CO2byYear, pch=16)  \n\n\nsqrt(10)           # Number of digits is determined by current seting\noptions(digits=2)  # Change until further notice,\nsqrt(10)\n\n\n\nWide-ranging information access and searches\n\n\n\n\nSection 10.2: R data structures\n\nSubsection 10.2.1: Vectors, dates, and arrays\n\nvehicles &lt;- c(\"Compact\", \"Large\", \"Midsize\", \"Small\", \"Sporty\", \"Van\")\nc(T, F, F, F, T, T, F)   # A logical vector, assuming F=FALSE and T=TRUE\n\n\n## Character vector\nmammals &lt;- c(\"Rat\", \"Pig\", \"Rat\", \"Mouse\", \"Pig\")\n## Logical vector\nrodent &lt;- c(\"TRUE\", \"FALSE\", \"TRUE\", \"FALSE\", \"TRUE\", \"FALSE\")\n## From character vector `mammals`, create factor\nmfac &lt;- factor(mammals)\nlevels(mfac)  \ntable(mfac)\n\n\nDates\n\nday1 &lt;- as.Date(c(\"2022-01-01\", \"2022-02-01\", \"2022-03-01\"))\nas.numeric(day1)   # Days since 1 January 1970\nday1[3] - day1[2]\n\n\n\nThe use of square brackets to extract subsets of vectors\n\n## Specify the indices of the elements that are to be extracted\nx &lt;- c(3, 11, 8, 15, 12,18)\nx[c(1,4:6)]        # Elements in positions 1, 4, 5, and 6\n## Use negative indices to identify elements for omission\nx[-c(2,3)]         # Positive and negative indices cannot be mixed\n## Specify a vector of logical values. \nx &gt; 10             # This generates a vector of logical values\nx[x &gt; 10]\n\n\nbodywt &lt;- c(Cow=465, Goat=28, Donkey=187, Pig=192)\nbodywt[c(\"Goat\", \"Pig\")]\n\n\n\nMatrices and arrays\n\narr123 &lt;- array(1:24, dim=c(2,4,3))\n## This prints as three 2 by 4 matrices.  Print just the first of the three.\narr123[, 2, 1]     # Column 2 and index 1 of 3rd dimension\nattributes(arr123)\n\n\n\n\nSubsection 10.2.2: Factors\n\ngender &lt;- c(rep(\"male\",691), rep(\"female\",692))\ngender &lt;- factor(gender)  # From character vector, create factor\nlevels(gender)            # Notice that `female` comes first\n\n\nGender &lt;- factor(gender, levels=c(\"male\", \"female\"))\n\n\nmf1 &lt;- factor(rep(c('male','female'),c(2,3)), labels=c(\"f\", \"m\"))\n## The following has the same result\nmf2 &lt;- factor(rep(c('male','female'), c(2,3)))\nlevels(mf2) &lt;- c(\"f\",\"m\")   # Assign new levels\nif(all(mf1==mf2))print(mf1)\n\n\nsum(gender==\"male\")\n\n\ntable(chickwts$feed)  # feed is a factor\nsource &lt;- chickwts$feed  \nlevels(source) &lt;- c(\"milk\",\"plant\",\"plant\",\"meat\",\"plant\",\"plant\")\ntable(source)\n\n\nOrdered factors\n\nstress &lt;- rep(c(\"low\",\"medium\",\"high\"), 2)\nord.stress &lt;- ordered(stress, levels=c(\"low\", \"medium\", \"high\"))\nord.stress\nord.stress &gt;= \"medium\"\n\n\n\nMissing values in values of factors\n\n\n\nSubsection 10.2.3: Operations with data frames\n\nCars93sum &lt;- DAAG::Cars93.summary  # Create copy in workspace\nCars93sum\n\n\nCars93sum[4:6, 2:3]   # Extract rows 4 to 6 and columns 2 and 3\nCars93sum[6:4, ]      # Extract rows in the order 6, 5, 4\nCars93sum[, 2:3]      # Extract columns 2 and 3\n## Or, use negative integers to specify rows and/or columns to be omitted\nCars93sum[-(1:3), -c(1,4)]  # In each case, numbers must be all +ve or all -ve\n## Specify row and/or column names\nCars93sum[c(\"Small\",\"Sporty\",\"Van\"), c(\"Max.passengers\",\"No.of.cars\")]\n\n\nData frames vs matrices\n\nnames(Cars93sum)[3] &lt;- \"numCars\"\nnames(Cars9sum) &lt;- c(\"minPass\",\"maxPass\",\"numCars\",\"code\")\n\n\n\nUsing a data frame as a database – with() and within()\n\n## trees (datasets) has data on Black Cherry Trees\nwith(trees, round(c(mean(Girth), median(Girth), sd(Girth)),1))\n\n\nwith(DAAG::pair65,       # stretch of rubber bands\n  {lenchange = heated-ambient\n   c(mean(lenchange), median(lenchange))\n})\n\n\n## Add variables `mph` and `gradient` to `DAAG::nihills`\nnihr &lt;- within(DAAG::nihills, {mph &lt;- dist/time; gradient &lt;- climb/dist})\n\n\n\nExtracting rows from data frames\n\nunlist(Cars93sum[1, ])\n\n\n\n\nSubsection 10.2.4: Data manipulation functions used in earlier chapters\n\n## For columns of `DAAG::jobs`, show the range of values\nsapply(DAAG::jobs, range)\n## Split egg lengths by species, calculate mean, sd, and number for each\nwith(DAAG::cuckoos, sapply(split(length,species), \n                           function(x)c(av=mean(x), sd=sd(x), nobs=length(x))))\n\n\napply(UCBAdmissions, 3, function(x)(x[1,2]/(x[1,2]+x[2,2]))*100) # Females\napply(UCBAdmissions, 3, function(x)(x[1,1]/(x[1,1]+x[2,1]))*100) # Males\n\n\nUCBAdmissions[, , 1]\n\n\nDAAG::cricketer |&gt; dplyr::count(year, left, name=\"Freq\") -&gt; handYear\nnames(handYear)[2] &lt;- \"hand\"\nbyYear &lt;- tidyr::pivot_wider(handYear, names_from='hand', values_from=\"Freq\")\n\n\n\nSubsection 10.2.5: Writing data to a file, and reading data from a file\n\nCO2byYear &lt;- data.frame(year = seq(from=1900, to=2020, by=20),\n                        co2gas = c(1.94, 3.52, 4.84, 9.38, 19.49, 25.46, 35.25))\nwrite.table(CO2byYear, file='gas.txt')    # Write data frame to file\nCO2byYear &lt;- read.table(file=\"gas.txt\")   # Read data back in\nwrite.csv(CO2byYear, file='gas.csv')                  # Write data frame\nCO2byYear &lt;- read.csv(file=\"gas.csv\", row.names=1)    # Read data back in\n\n\nData input from the RStudio menu — data frames vs tibbles\n\n\n\nSubsection 10.2.6: Issues for working with data frames and tibbles\n\nExtraction of columns from data frames and tibbles\n\nsites &lt;- DAAG::possumsites    # sites is then a data frame\nsites[,3]                     # returns a vector\nsites[,3, drop=FALSE]         # returns a 1-column data frame\n\n\ndplyr::as_tibble(sites)[,3]   # returns a 1-column tibble\ndplyr::as_tibble(sites)[[3]]  # returns a vector\nsites[[3]]                    # returns a vector\n\n\n\nConversion between data frames and tibbles\n\nattributes(DAAG::possumsites)[['row.names']]\n\n\npossumSites &lt;- tibble::as_tibble(DAAG::possumsites, rownames=\"Site\")\npossumSites\n\n\n\n\nSubsection 10.2.7: Lists\n\n## Summary statistics for 31 felled black cherry tree\n## Median (middle value), range, number, units\nhtstats &lt;- list(med=76, range=c(low=63,high=87), n=31, units=\"ft\")\nhtstats[1:2]       # Show first two list elements only\n\n\n## The following are alternative ways to extract the second list element\nhtstats[2]          # First list element (Can replace `2` by 'range')\nhtstats[2][1]       # A subset of a list is a list \n\n\nhtstats[[2]]; htstats$range; htstats[[\"range\"]]\n\n\nunlist(htstats[2])  # Contents of second list element, with composite names\nunlist(htstats[2], use.names=F)   # Elements have no names\n\n\ntstats &lt;- with(MASS::shoes, t.test(B, A, paired=TRUE))\nnames(tstats)        ## Names of list elements. See `?t.test` for details.\ntstats[1]            ## Type tstats[1] to see the first list element\n## Compact listing of contents list elements 1 to 5, which are all numeric\nunlist(tstats[1:5])  ## With `unlist(tstats)` all elements become character \n\n\n\n\nSection 10.3: Functions and operators\n\nSubsection 10.3.1: Common useful built-in functions\n\n## Data indices\nlength()       # number of elements in a vector or a list\norder()        # x[order(x)] sorts x (by default, NAs are last)\nwhich()        # which indices of a logical vector are `TRUE`\nwhich.max()    # locates (first) maximum (NB, also: `which.min()`)\n\n\n## Data manipulation\nc()            # join together (`concatenate`) elements or vectors or lists\ndiff()         # vector of first differences\nsort()         # sort elements into order, by default omitting NAs\nrev()          # reverse the order of vector elements\nt()            # transpose matrix or data frame \n               # (a data frame is first coerced to a matrixwith() \nwith()         # do computation using columns of specified data frame\n\n\n## Data summary\nmean()         # mean of the elements of a vector\nmedian()       # median of the elements of a vector\nrange()        # minimum and maximum value elements of vector\nunique()       # form the vector of distinct values\n## List function arguments\nargs()         # information on the arguments to a function\n\n\n## Obtain details\nhead()         # display first few rows (by default 6) of object\nls()           # list names of objects in the workspace\n\n\n## Print multiple objects\ncat()          # prints multiple objects, one after the other\n\n\n## Functions that return TRUE or FALSE?\nall()          # returns TRUE if all values are TRUE\nany()          # returns TRUE if any values are TRUE\nis.factor()    # returns TRUE if the argument is a factor\nis.na()        # returns TRUE if the argument is an NA\n               # NB also is.logical(), etc.\n\n\nseq(from =1, by=2, length.out=3)  # Unabbeviated arguments\nseq(from =1, by=2, length=3)      # Abbreviate `length.out` to `length`\n\n\n\nSubsection 10.3.2: User-written functions\n\ndistance &lt;- c(148,182,173,166,109,141,166)\nmean.and.sd(distance)\n\n\n## Execute the function with the  default argument:\nmean.and.sd()\n\n\n## Thus, to return the mean, SD and name of the input vector\n## replace c(mean=av, SD=sdev) by\nlist(mean=av, SD=sdev, dataset = deparse(substitute(x)))\n\n\n\nSubsection 10.3.3: Generic functions, and the class of an object\n\n\nSubsection 10.3.4: Pipes — a “Do this, then this, . . .” syntax\n\nmean(rnorm(20, sd=2))\n20 |&gt; rnorm(sd=2) |&gt; mean()\n\n\nlogmammals &lt;- MASS::mammals |&gt; log() |&gt; setNames(c(\"logbody\",\"logbrain\"))\n## Alternatively, use the ability to reverse the assignment operator.\nMASS::mammals |&gt; log() |&gt; setNames(c(\"logbody\",\"logbrain\")) -&gt; logmammals \n  ## This last is more in the spirit of pipes.\n\n\nMASS::mammals |&gt;\n  log() |&gt;\n  setNames(c(\"logbody\",\"logbrain\")) |&gt;\n  (\\(d)lm(logbrain ~ logbody, data=d))() |&gt;\n  coef()\n\n\n\nSubsection 10.3.5: Operators\n\n## Multiple of divisor that leaves smallest non-negative remainder\nc(\"Multiple of divisor\" = 24 %/% 9, \"Remainder after division\" = 6)\n\n\n\n\nSection 10.4: Calculations with matrices, arrays, lists, and data frames\n\nCalculations in parallel across all elements of a vector\n\nx &lt;- 1:6\nlog(x)                 # Natural logarithm of 1, 2, ... 6\nlog(x, base=10)        # Common logarithm (base 10)\nlog(64, base=c(2,10))  # Apply different bases to one number\nlog(matrix(1:6, nrow=2), base=2)  # Take logarithms of all matrix elements\n\n\n\nPatterned data\n\nseq(from=5, to=22, by=3)  # The first value is 5.\nrep(c(2,3,5), 4)          #  Repeat the sequence (2, 3, 5) four times over\nrep(c(\"female\", \"male\"), c(2,3))    # Use syntax with a character vector\n\n\n\nSubsection 10.4.1: Missing values\n\nnbranch &lt;- subset(DAAG::rainforest, species==\"Acacia mabellae\")$branch\nnbranch            # Number of small branches (2cm or less)\n\n\nmean(nbranch, na.rm=TRUE)\n\n\nnbranch == NA      # This always equals `NA`\n\n\nis.na(nbranch)    # Use to check for NAs\n\n\nnbranch[is.na(nbranch)] &lt;- -999\n  # `mean(nbranch)` will then be a nonsense value\n\n\nNAs in modeling functions\n\noptions()$na.action # Version 3.2.2, following startup\n\n\n\nCounting and identifying NAs – the use of table()\n\nwith(DAAG::nswdemo, table(trt, re74&gt;0, useNA=\"ifany\"))\n\n\n\nInfinities and NaNs\n\nsummary(DAAG::primates)\n\n\nprimates &lt;- DAAG::primates\n\n\ngplots::plotCI()    # `plotCI() function in package `gplots`\nplotrix::plotCI()   # `plotCI() function in package `plotrix`\n\n\nsessionInfo()[['basePkgs']]\n\n\n## List just the workspace and the first eight packages on the search list:\nsearch()[1:9]\n\n\ndata(package=\"datasets\") \n\n\n\n\n\nSection 10.5: Brief notes on R graphics packages and functions\n\nSubsection 10.5.1: Lattice graphics — a step beyond base graphics\n\ngrog &lt;- DAAG::grog\nchr &lt;- with(grog, match(Country, c('Australia', 'NewZealand')))  \n  # Australia: 1; matches 1st element of c('Australia', 'NewZealand')\n  # NewZealand: 2; matches 2nd element\nplot(Beer ~ Year, data=grog, ylim=c(0, max(Beer)*1.1), pch = chr)\nwith(grog, points(Wine ~ Year, pch=chr, col='red'))\nlegend(\"bottomright\", legend=c(\"Australia\", \"New Zealand\"), pch=1:2)\ntitle(main=\"Beer consumption (l, pure alcohol)\", line=1)\n\n\nlibrary(latticeExtra)    ## Loads both lattice and the add-on latticeExtra\ngph &lt;- xyplot(Beer+Wine ~ Year, groups=Country, data=grog)\nupdate(gph, par.settings=simpleTheme(pch=19), auto.key=list(columns=2))\n\n\n## Or, condition on `Country`\nxyplot(Beer+Wine+Spirit ~ Year | Country, data=grog,\n       par.settings=simpleTheme(pch=19), auto.key=list(columns=3))\n\n\ntinting &lt;- DAAG::tinting\nxyplot(csoa~it | tint*target, groups=agegp, data=tinting, auto.key=list(columns=2))\n\n\ncuckoos &lt;- DAAG::cuckoos\nav &lt;- with(cuckoos, aggregate(length, list(species=species), FUN=mean))\ngph &lt;- dotplot(species ~ length, data=cuckoos, alpha=0.4) +\n  as.layer(dotplot(species ~ x, pch=3, cex=1.4, col=\"black\", data=av))\nupdate(gph, xlab=\"Length of egg (mm)\")\n\n\n## Alternatives, using `layer()` or `as.layer()`\navg &lt;- with(cuckoos, data.frame(nspec=1:nlevels(species), \n                             av=sapply(split(length,species),mean)))\ndotplot(species ~ length, data=cuckoos) + \n  layer(lpoints(nspec~av, pch=3, cex=1.25, col=\"black\"), data=avg)\n\n\ndotplot(species ~ length, data=cuckoos) + \n  as.layer(dotplot(nspec~av, data=avg, pch=3, cex=1.25, col=\"black\"))\n\n\n## Specify panel function\ndotplot(species ~ length, data=cuckoos, \n  panel=function(x,y,...){panel.dotplot(x, y, pch=1, col=\"gray40\")\n    avg &lt;- data.frame(nspec=1:nlevels(y), av=sapply(split(x,y),mean))\n    with(avg, lpoints(nspec~av, pch=3, cex=1.25, col=\"black\")) })\n\n\nCombining separately created graphics objects\n\ncuckoos &lt;- DAAG::cuckoos\n## Panel A: Dotplot without species means added\ngphA &lt;- dotplot(species ~ length, data=cuckoos) \n## Panel B: Box and whisker plot\ngphB &lt;- bwplot(species ~ length, data=cuckoos)\nupdate(c(\"A: Dotplot\"=gphA, \"B: Boxplot\"=gphB), between=list(x=0.4),\n       xlab=\"Length of egg (mm)\", layout=c(2,1))\n  ## `latticeExtra::c()` joins compatible plots together. \n  ## `layout=c(2,1)` : join horizontally; `layout=c(1,2)` : join vertically\n\n\n\n\nSubsection 10.5.2: Dynamic graphics – the rgl package\n\nvignette('plot3D', package='plot3D')\n\n\n\n\nSection 10.6: Plotting characters, symbols, line types and colors\n\nycol &lt;- -2.1 - (0:9) * 2.1\nftype &lt;- c(\"plain\", \"bold\", \"italic\", \"bold italic\", \"symbol\")\nyline &lt;- 4.2\nypmax &lt;- 20\nfarleft &lt;- -7.8\nplot(c(-4, 31), c(4.25, ypmax), type = \"n\", xlab = \"\", ylab = \"\",\naxes = F)\nchh &lt;- par()$cxy[2]\ntext(0:25, rep(ypmax + 0.8 * chh, 26), paste(0:25), srt = 90,\ncex = 0.75, xpd = T)\ntext(-1.5, ypmax + 0.8 * chh, \"pch = \", cex = 0.75, xpd = T)\npoints(0:25, rep(ypmax, 26), pch = 0:25, lwd=0.8)\nletterfont &lt;- function(ypos = ypmax, font = 2) {\npar(font = font)\ntext(-1.35, ypos, \"64-76\", cex = 0.75, adj = 1, xpd = TRUE)\ntext(19 - 1.35, ypos, \"96-108\", cex = 0.75, adj = 1)\npoints(c(0:12), rep(ypos, 13), pch = 64:76)\npoints(19:31, rep(ypos, 13), pch = 96:108)\ntext(farleft, ypos, paste(font), xpd = T)\ntext(farleft, ypos - 0.5, ftype[font], cex = 0.75)\n}\nplotfont &lt;- function(xpos = 0:31, ypos = ypmax, font = 1,\nsel32 = 2:4, showfont = TRUE) {\npar(font = font)\ni &lt;- 0\nfor (j in sel32) {\ni &lt;- i + 1\nmaxval &lt;- j * 32 - 1\nif(j==4)maxval &lt;- maxval-1\ntext(-1.35, ypos - i + 1, paste((j - 1) * 32, \"-\",\nmaxval, sep = \"\"), cex = 0.75, adj = 1, xpd = TRUE)\nif(j!=4)\npoints(xpos, rep(ypos - i + 1, 32), pch = (j - 1) *\n32 + (0:31))\nelse\npoints(xpos[-32], rep(ypos - i + 1, 31), pch = (j - 1) *\n32 + (0:30))\n}\nlines(rep(-1.05, 2), c(ypos - length(sel32) + 1, ypos) +\nc(-0.4, 0.4), xpd = T, col = \"grey40\")\nif (showfont) {\ntext(farleft, ypos, paste(\"font =\", font, \" \"), xpd = T)\ntext(farleft, ypos - 0.5, ftype[font], cex = 0.75,\nxpd = T)\n}\n}\nplotfont(ypos = ypmax - 1.5, font = 1, sel32 = 2:4)\nfor (j in 2:4) letterfont(ypos = ypmax - 2.1 - 1.4 * j, font = j)\nplotfont(ypos = ypmax - 9.1, font = 5, sel32 = 3)\nplotfont(xpos = c(-0.5, 1:31), ypos = ypmax - 10.1, font = 5,\nsel32 = 4, showfont = FALSE)\npar(font = 1)\nltypes &lt;- c(\"blank\", \"solid\", \"dashed\", \"dotted\", \"dotdash\",\n\"longdash\", \"twodash\")\nlcode &lt;- c(\"\", \"\", \"44\", \"13\", \"1343\", \"73\", \"2262\")\nfor (i in 0:6) {\nlines(c(4, 31), c(yline + 4.5 - 0.8 * i, yline + 4.5 -\n0.8 * i), lty = i, lwd = 2, xpd = T)\nif (i == 0)\nnumchar &lt;- paste(\"lty =\", i, \" \")\nelse numchar &lt;- i\ntext(farleft, yline + 4.5 - 0.8 * i, numchar, xpd = TRUE)\ntext(farleft + 3.5, yline + 4.5 - 0.8 * i, ltypes[i +\n1], cex = 0.85, xpd = TRUE)\ntext(farleft + 7.5, yline + 4.5 - 0.8 * i, lcode[i +\n1], cex = 0.85, xpd = TRUE)\n}\n\n\nycol &lt;- -2.1 - (0:9) * 2.1\nftype &lt;- c(\"plain\", \"bold\", \"italic\", \"bold italic\", \"symbol\")\nyline &lt;- 4.2\nypmax &lt;- 20\nfarleft &lt;- -7.8\nplot(c(-4, 31), c(4.25, ypmax), type = \"n\", xlab = \"\", ylab = \"\",\naxes = F)\nchh &lt;- par()$cxy[2]\ntext(0:25, rep(ypmax + 0.8 * chh, 26), paste(0:25), srt = 90,\ncex = 0.75, xpd = T)\ntext(-1.5, ypmax + 0.8 * chh, \"pch = \", cex = 0.75, xpd = T)\npoints(0:25, rep(ypmax, 26), pch = 0:25, lwd=0.8)\nletterfont &lt;- function(ypos = ypmax, font = 2) {\npar(font = font)\ntext(-1.35, ypos, \"64-76\", cex = 0.75, adj = 1, xpd = TRUE)\ntext(19 - 1.35, ypos, \"96-108\", cex = 0.75, adj = 1)\npoints(c(0:12), rep(ypos, 13), pch = 64:76)\npoints(19:31, rep(ypos, 13), pch = 96:108)\ntext(farleft, ypos, paste(font), xpd = T)\ntext(farleft, ypos - 0.5, ftype[font], cex = 0.75)\n}\nplotfont &lt;- function(xpos = 0:31, ypos = ypmax, font = 1,\nsel32 = 2:4, showfont = TRUE) {\npar(font = font)\ni &lt;- 0\nfor (j in sel32) {\ni &lt;- i + 1\nmaxval &lt;- j * 32 - 1\nif(j==4)maxval &lt;- maxval-1\ntext(-1.35, ypos - i + 1, paste((j - 1) * 32, \"-\",\nmaxval, sep = \"\"), cex = 0.75, adj = 1, xpd = TRUE)\nif(j!=4)\npoints(xpos, rep(ypos - i + 1, 32), pch = (j - 1) *\n32 + (0:31))\nelse\npoints(xpos[-32], rep(ypos - i + 1, 31), pch = (j - 1) *\n32 + (0:30))\n}\nlines(rep(-1.05, 2), c(ypos - length(sel32) + 1, ypos) +\nc(-0.4, 0.4), xpd = T, col = \"grey40\")\nif (showfont) {\ntext(farleft, ypos, paste(\"font =\", font, \" \"), xpd = T)\ntext(farleft, ypos - 0.5, ftype[font], cex = 0.75,\nxpd = T)\n}\n}\nplotfont(ypos = ypmax - 1.5, font = 1, sel32 = 2:4)\nfor (j in 2:4) letterfont(ypos = ypmax - 2.1 - 1.4 * j, font = j)\nplotfont(ypos = ypmax - 9.1, font = 5, sel32 = 3)\nplotfont(xpos = c(-0.5, 1:31), ypos = ypmax - 10.1, font = 5,\nsel32 = 4, showfont = FALSE)\npar(font = 1)\nltypes &lt;- c(\"blank\", \"solid\", \"dashed\", \"dotted\", \"dotdash\",\n\"longdash\", \"twodash\")\nlcode &lt;- c(\"\", \"\", \"44\", \"13\", \"1343\", \"73\", \"2262\")\nfor (i in 0:6) {\nlines(c(4, 31), c(yline + 4.5 - 0.8 * i, yline + 4.5 -\n0.8 * i), lty = i, lwd = 2, xpd = T)\nif (i == 0)\nnumchar &lt;- paste(\"lty =\", i, \" \")\nelse numchar &lt;- i\ntext(farleft, yline + 4.5 - 0.8 * i, numchar, xpd = TRUE)\ntext(farleft + 3.5, yline + 4.5 - 0.8 * i, ltypes[i +\n1], cex = 0.85, xpd = TRUE)\ntext(farleft + 7.5, yline + 4.5 - 0.8 * i, lcode[i +\n1], cex = 0.85, xpd = TRUE)\n}\n\n\nFont families\n\n\nColors\n\nlibrary(RColorBrewer)\npalette(brewer.pal(12, \"Set3\"))\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/Appendix.R\")\n}",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Appendix A: The R System – A Brief Overview</span>"
    ]
  }
]